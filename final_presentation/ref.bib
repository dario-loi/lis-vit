
@misc{hu_continuous_2023,
	title = {Continuous {Sign} {Language} {Recognition} with {Correlation} {Network}},
	url = {http://arxiv.org/abs/2303.03202},
	doi = {10.48550/arXiv.2303.03202},
	abstract = {Human body trajectories are a salient cue to identify actions in the video. Such body trajectories are mainly conveyed by hands and face across consecutive frames in sign language. However, current methods in continuous sign language recognition (CSLR) usually process frames independently, thus failing to capture cross-frame trajectories to effectively identify a sign. To handle this limitation, we propose correlation network (CorrNet) to explicitly capture and leverage body trajectories across frames to identify signs. In specific, a correlation module is first proposed to dynamically compute correlation maps between the current frame and adjacent frames to identify trajectories of all spatial patches. An identification module is then presented to dynamically emphasize the body trajectories within these correlation maps. As a result, the generated features are able to gain an overview of local temporal movements to identify a sign. Thanks to its special attention on body trajectories, CorrNet achieves new state-of-the-art accuracy on four large-scale datasets, i.e., PHOENIX14, PHOENIX14-T, CSL-Daily, and CSL. A comprehensive comparison with previous spatial-temporal reasoning methods verifies the effectiveness of CorrNet. Visualizations demonstrate the effects of CorrNet on emphasizing human body trajectories across adjacent frames.},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Hu, Lianyu and Gao, Liqing and Liu, Zekang and Feng, Wei},
	month = mar,
	year = {2023},
	note = {arXiv:2303.03202},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, cnn, cslr},
	file = {Preprint PDF:/home/dario/Zotero/storage/I4CKNFJ5/Hu et al. - 2023 - Continuous Sign Language Recognition with Correlation Network.pdf:application/pdf;Snapshot:/home/dario/Zotero/storage/YJDHIBD2/2303.html:text/html},
}

@inproceedings{forster_rwth-phoenix-weather_2012,
	title = {{RWTH}-{PHOENIX}-{Weather}: {A} {Large} {Vocabulary} {Sign} {Language} {Recognition} and {Translation} {Corpus}},
	shorttitle = {{RWTH}-{PHOENIX}-{Weather}},
	url = {https://www.semanticscholar.org/paper/RWTH-PHOENIX-Weather%3A-A-Large-Vocabulary-Sign-and-Forster-Schmidt/29228179df78b2bc28c0c65cea2f1a43132993c6},
	abstract = {This paper introduces the RWTH-PHOENIX-Weather corpus, a video-based, large vocabulary corpus of German Sign Language suitable for statistical sign language recognition and translation. In contrastto most available sign language data collections, the RWTH-PHOENIX-Weather corpus has not been recorded for linguistic research but for the use in statistical pattern recognition. The corpus contains weather forecasts recorded from German public TV which are manually annotated using glosses distinguishing sign variants, and time boundaries have been marked on the sentence and the gloss level. Further, the spoken German weather forecast has been transcribed in a semi-automatic fashion using a state-of-the-art automatic speech recognition system. Moreover, an additional translation of the glosses into spoken German has been created to capture allowable translation variability. In addition to the corpus, experimental baseline results for hand and head tracking, statistical sign language recognition and translation are presented.},
	urldate = {2024-11-27},
	author = {Forster, Jens and Schmidt, C. and Hoyoux, Thomas and Koller, Oscar and Zelle, Uwe and Piater, J. and Ney, H.},
	month = may,
	year = {2012},
	keywords = {cslr, dataset},
	annote = {[TLDR] The RWTH-PHOENIX-Weather corpus is introduced, a video-based, large vocabulary corpus of German Sign Language suitable for statistical sign language recognition and translation and experimental baseline results for hand and head tracking, statistical signlanguage recognition andtranslation are presented.},
	file = {Full Text PDF:/home/dario/Zotero/storage/LTS2HQJ9/Forster et al. - 2012 - RWTH-PHOENIX-Weather A Large Vocabulary Sign Language Recognition and Translation Corpus.pdf:application/pdf},
}

@misc{ahn_slowfast_2023,
	title = {{SlowFast} {Network} for {Continuous} {Sign} {Language} {Recognition}},
	url = {http://arxiv.org/abs/2309.12304},
	doi = {10.48550/arXiv.2309.12304},
	abstract = {The objective of this work is the effective extraction of spatial and dynamic features for Continuous Sign Language Recognition (CSLR). To accomplish this, we utilise a two-pathway SlowFast network, where each pathway operates at distinct temporal resolutions to separately capture spatial (hand shapes, facial expressions) and dynamic (movements) information. In addition, we introduce two distinct feature fusion methods, carefully designed for the characteristics of CSLR: (1) Bi-directional Feature Fusion (BFF), which facilitates the transfer of dynamic semantics into spatial semantics and vice versa; and (2) Pathway Feature Enhancement (PFE), which enriches dynamic and spatial representations through auxiliary subnetworks, while avoiding the need for extra inference time. As a result, our model further strengthens spatial and dynamic representations in parallel. We demonstrate that the proposed framework outperforms the current state-of-the-art performance on popular CSLR datasets, including PHOENIX14, PHOENIX14-T, and CSL-Daily.},
	language = {en},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Ahn, Junseok and Jang, Youngjoon and Chung, Joon Son},
	month = sep,
	year = {2023},
	note = {arXiv:2309.12304 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/home/dario/Zotero/storage/A9KRWFTI/Ahn et al. - 2023 - SlowFast Network for Continuous Sign Language Recognition.pdf:application/pdf},
}

@article{tubelets,
  author       = {Kai Kang and
                  Hongsheng Li and
                  Junjie Yan and
                  Xingyu Zeng and
                  Bin Yang and
                  Tong Xiao and
                  Cong Zhang and
                  Zhe Wang and
                  Ruohui Wang and
                  Xiaogang Wang and
                  Wanli Ouyang},
  title        = {{T-CNN:} Tubelets with Convolutional Neural Networks for Object Detection
                  from Videos},
  journal      = {CoRR},
  volume       = {abs/1604.02532},
  year         = {2016},
  url          = {http://arxiv.org/abs/1604.02532},
  eprinttype    = {arXiv},
  eprint       = {1604.02532},
  timestamp    = {Fri, 08 Oct 2021 17:28:46 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KangLYZYXZWWWO16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{yang_signformer_2024,
	title = {Signformer is all you need: {Towards} {Edge} {AI} for {Sign} {Language}},
	shorttitle = {Signformer is all you need},
	url = {http://arxiv.org/abs/2411.12901},
	doi = {10.48550/arXiv.2411.12901},
	abstract = {Sign language translation, especially in gloss-free paradigm, is confronting a dilemma of impracticality and unsustainability due to growing resource-intensive methodologies. Contemporary state-of-the-arts (SOTAs) have significantly hinged on pretrained sophiscated backbones such as Large Language Models (LLMs), embedding sources, or extensive datasets, inducing considerable parametric and computational inefficiency for sustainable use in real-world scenario. Despite their success, following this research direction undermines the overarching mission of this domain to create substantial value to bridge hard-hearing and common populations. Committing to the prevailing trend of LLM and Natural Language Processing (NLP) studies, we pursue a profound essential change in architecture to achieve groundup improvements without external aid from pretrained models, prior knowledge transfer, or any NLP strategies considered not-from-scratch.},
	language = {en},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Yang, Eta},
	month = nov,
	year = {2024},
	note = {arXiv:2411.12901 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
	annote = {Comment: Official Code at: https://github.com/EtaEnding/Signformer/tree/main},
	file = {PDF:/home/dario/Zotero/storage/3Y9IWI9V/Yang - 2024 - Signformer is all you need Towards Edge AI for Sign Language.pdf:application/pdf},
}

@misc{gong_llms_2024,
	title = {{LLMs} are {Good} {Sign} {Language} {Translators}},
	url = {http://arxiv.org/abs/2404.00925},
	doi = {10.48550/arXiv.2404.00925},
	abstract = {Sign Language Translation (SLT) is a challenging task that aims to translate sign videos into spoken language. Inspired by the strong translation capabilities of large language models (LLMs) that are trained on extensive multilingual text corpora, we aim to harness off-the-shelf LLMs to handle SLT. In this paper, we regularize the sign videos to embody linguistic characteristics of spoken language, and propose a novel SignLLM framework to transform sign videos into a language-like representation for improved readability by off-the-shelf LLMs. SignLLM comprises two key modules: (1) The Vector-Quantized Visual Sign module converts sign videos into a sequence of discrete characterlevel sign tokens, and (2) the Codebook Reconstruction and Alignment module converts these character-level tokens into word-level sign representations using an optimal transport formulation. A sign-text alignment loss further bridges the gap between sign and text tokens, enhancing semantic compatibility. We achieve state-of-the-art gloss-free results on two widely-used SLT benchmarks.},
	language = {en},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Gong, Jia and Foo, Lin Geng and He, Yixuan and Rahmani, Hossein and Liu, Jun},
	month = apr,
	year = {2024},
	note = {arXiv:2404.00925 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to CVPR 2024},
	annote = {Comment: Accepted to CVPR 2024},
	file = {PDF:/home/dario/Zotero/storage/GHEJLU3H/Gong et al. - 2024 - LLMs are Good Sign Language Translators.pdf:application/pdf;Snapshot:/home/dario/Zotero/storage/HZWURZ9J/2404.html:text/html},
}

@misc{mehta_mobilevit_2022,
	title = {{MobileViT}: {Light}-weight, {General}-purpose, and {Mobile}-friendly {Vision} {Transformer}},
	shorttitle = {{MobileViT}},
	url = {http://arxiv.org/abs/2110.02178},
	doi = {10.48550/arXiv.2110.02178},
	abstract = {Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision transformers (ViTs) have been adopted. Unlike CNNs, ViTs are heavyweight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers. Our results show that MobileViT signiﬁcantly outperforms CNNand ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4\% with about 6 million parameters, which is 3.2\% and 6.2\% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7\% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets.},
	language = {en},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Mehta, Sachin and Rastegari, Mohammad},
	month = mar,
	year = {2022},
	note = {arXiv:2110.02178 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted at ICLR'22},
	file = {PDF:/home/dario/Zotero/storage/G4VZ4JEP/Mehta and Rastegari - 2022 - MobileViT Light-weight, General-purpose, and Mobile-friendly Vision Transformer.pdf:application/pdf},
}

@inproceedings{sandler_mobilenetv2_2018,
	address = {Salt Lake City, UT},
	title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{MobileNetV2}},
	url = {https://ieeexplore.ieee.org/document/8578572/},
	doi = {10.1109/CVPR.2018.00474},
	abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efﬁcient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.},
	language = {en},
	urldate = {2024-12-02},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	month = jun,
	year = {2018},
	pages = {4510--4520},
	file = {PDF:/home/dario/Zotero/storage/Y2Q2C8B5/Sandler et al. - 2018 - MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf:application/pdf},
}

@inproceedings{zuo_local_2022,
	title = {Local {Context}-aware {Self}-attention for {Continuous} {Sign} {Language} {Recognition}},
	url = {https://www.isca-archive.org/interspeech_2022/zuo22_interspeech.html},
	doi = {10.21437/Interspeech.2022-164},
	abstract = {Transformer-based architectures are adopted in many continuous sign language recognition (CSLR) works for sequence modeling due to their strong capability of extracting global contexts. However, since vanilla self-attention (SA), the core module of Transformer, computes a weighted average over all time steps, the local temporal semantics of sign videos may not be fully exploited. In this work, we propose local context-aware selfattention (LCSA) to enhance the vanilla SA to leverage both local and global contexts. We introduce the local contexts at two different levels of model computation: score and query levels. At the score level, we modulate the attention scores explicitly with an additional Gaussian bias. At the query level, local contexts are modeled implicitly using depth-wise temporal convolutional networks (DTCNs). However, the vanilla Gaussian bias has two major shortcomings: first, its window size is fixed and needs to be fine-tuned laboriously; second, the fixed window size is common among all time steps. In this work, a dynamic Gaussian bias is further proposed to address the above issues. Experimental results on two benchmarks, PHOENIX-2014 and CSL, validate the effectiveness and superiority of our method.},
	language = {en},
	urldate = {2024-12-19},
	booktitle = {Interspeech 2022},
	publisher = {ISCA},
	author = {Zuo, Ronglai and Mak, Brian},
	month = sep,
	year = {2022},
	pages = {4810--4814},
	file = {PDF:/home/dario/Zotero/storage/NIRTM64I/Zuo and Mak - 2022 - Local Context-aware Self-attention for Continuous Sign Language Recognition.pdf:application/pdf},
}

@article{graves_connectionist_nodate,
	title = {Connectionist {Temporal} {Classiﬁcation}: {Labelling} {Unsegmented} {Sequence} {Data} with {Recurrent} {Neural} {Networks}},
	abstract = {Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.},
	language = {en},
	author = {Graves, Alex and Fernandez, Santiago and Gomez, Faustino and Schmidhuber, Jurgen},
	file = {PDF:/home/dario/Zotero/storage/W2FNYAYV/Graves et al. - Connectionist Temporal Classiﬁcation Labelling Unsegmented Sequence Data with Recurrent Neural Netw.pdf:application/pdf},
}

@article{graves_connectionist_nodate-1,
	title = {Connectionist {Temporal} {Classiﬁcation}: {Labelling} {Unsegmented} {Sequence} {Data} with {Recurrent} {Neural} {Networks}},
	abstract = {Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.},
	language = {en},
	author = {Graves, Alex and Fernandez, Santiago and Gomez, Faustino and Schmidhuber, Jurgen},
	file = {PDF:/home/dario/Zotero/storage/UKYEAU7Z/Graves et al. - Connectionist Temporal Classiﬁcation Labelling Unsegmented Sequence Data with Recurrent Neural Netw.pdf:application/pdf},
}
