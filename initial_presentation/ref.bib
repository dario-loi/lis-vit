
@misc{hu_continuous_2023,
	title = {Continuous {Sign} {Language} {Recognition} with {Correlation} {Network}},
	url = {http://arxiv.org/abs/2303.03202},
	doi = {10.48550/arXiv.2303.03202},
	abstract = {Human body trajectories are a salient cue to identify actions in the video. Such body trajectories are mainly conveyed by hands and face across consecutive frames in sign language. However, current methods in continuous sign language recognition (CSLR) usually process frames independently, thus failing to capture cross-frame trajectories to effectively identify a sign. To handle this limitation, we propose correlation network (CorrNet) to explicitly capture and leverage body trajectories across frames to identify signs. In specific, a correlation module is first proposed to dynamically compute correlation maps between the current frame and adjacent frames to identify trajectories of all spatial patches. An identification module is then presented to dynamically emphasize the body trajectories within these correlation maps. As a result, the generated features are able to gain an overview of local temporal movements to identify a sign. Thanks to its special attention on body trajectories, CorrNet achieves new state-of-the-art accuracy on four large-scale datasets, i.e., PHOENIX14, PHOENIX14-T, CSL-Daily, and CSL. A comprehensive comparison with previous spatial-temporal reasoning methods verifies the effectiveness of CorrNet. Visualizations demonstrate the effects of CorrNet on emphasizing human body trajectories across adjacent frames.},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Hu, Lianyu and Gao, Liqing and Liu, Zekang and Feng, Wei},
	month = mar,
	year = {2023},
	note = {arXiv:2303.03202},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, cnn, cslr},
	file = {Preprint PDF:/home/dario/Zotero/storage/I4CKNFJ5/Hu et al. - 2023 - Continuous Sign Language Recognition with Correlation Network.pdf:application/pdf;Snapshot:/home/dario/Zotero/storage/YJDHIBD2/2303.html:text/html},
}

@inproceedings{forster_rwth-phoenix-weather_2012,
	title = {{RWTH}-{PHOENIX}-{Weather}: {A} {Large} {Vocabulary} {Sign} {Language} {Recognition} and {Translation} {Corpus}},
	shorttitle = {{RWTH}-{PHOENIX}-{Weather}},
	url = {https://www.semanticscholar.org/paper/RWTH-PHOENIX-Weather%3A-A-Large-Vocabulary-Sign-and-Forster-Schmidt/29228179df78b2bc28c0c65cea2f1a43132993c6},
	abstract = {This paper introduces the RWTH-PHOENIX-Weather corpus, a video-based, large vocabulary corpus of German Sign Language suitable for statistical sign language recognition and translation. In contrastto most available sign language data collections, the RWTH-PHOENIX-Weather corpus has not been recorded for linguistic research but for the use in statistical pattern recognition. The corpus contains weather forecasts recorded from German public TV which are manually annotated using glosses distinguishing sign variants, and time boundaries have been marked on the sentence and the gloss level. Further, the spoken German weather forecast has been transcribed in a semi-automatic fashion using a state-of-the-art automatic speech recognition system. Moreover, an additional translation of the glosses into spoken German has been created to capture allowable translation variability. In addition to the corpus, experimental baseline results for hand and head tracking, statistical sign language recognition and translation are presented.},
	urldate = {2024-11-27},
	author = {Forster, Jens and Schmidt, C. and Hoyoux, Thomas and Koller, Oscar and Zelle, Uwe and Piater, J. and Ney, H.},
	month = may,
	year = {2012},
	keywords = {cslr, dataset},
	annote = {[TLDR] The RWTH-PHOENIX-Weather corpus is introduced, a video-based, large vocabulary corpus of German Sign Language suitable for statistical sign language recognition and translation and experimental baseline results for hand and head tracking, statistical signlanguage recognition andtranslation are presented.},
	file = {Full Text PDF:/home/dario/Zotero/storage/LTS2HQJ9/Forster et al. - 2012 - RWTH-PHOENIX-Weather A Large Vocabulary Sign Language Recognition and Translation Corpus.pdf:application/pdf},
}

@misc{ahn_slowfast_2023,
	title = {{SlowFast} {Network} for {Continuous} {Sign} {Language} {Recognition}},
	url = {http://arxiv.org/abs/2309.12304},
	doi = {10.48550/arXiv.2309.12304},
	abstract = {The objective of this work is the effective extraction of spatial and dynamic features for Continuous Sign Language Recognition (CSLR). To accomplish this, we utilise a two-pathway SlowFast network, where each pathway operates at distinct temporal resolutions to separately capture spatial (hand shapes, facial expressions) and dynamic (movements) information. In addition, we introduce two distinct feature fusion methods, carefully designed for the characteristics of CSLR: (1) Bi-directional Feature Fusion (BFF), which facilitates the transfer of dynamic semantics into spatial semantics and vice versa; and (2) Pathway Feature Enhancement (PFE), which enriches dynamic and spatial representations through auxiliary subnetworks, while avoiding the need for extra inference time. As a result, our model further strengthens spatial and dynamic representations in parallel. We demonstrate that the proposed framework outperforms the current state-of-the-art performance on popular CSLR datasets, including PHOENIX14, PHOENIX14-T, and CSL-Daily.},
	language = {en},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Ahn, Junseok and Jang, Youngjoon and Chung, Joon Son},
	month = sep,
	year = {2023},
	note = {arXiv:2309.12304 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/home/dario/Zotero/storage/A9KRWFTI/Ahn et al. - 2023 - SlowFast Network for Continuous Sign Language Recognition.pdf:application/pdf},
}

@misc{yang_signformer_2024,
	title = {Signformer is all you need: {Towards} {Edge} {AI} for {Sign} {Language}},
	shorttitle = {Signformer is all you need},
	url = {http://arxiv.org/abs/2411.12901},
	doi = {10.48550/arXiv.2411.12901},
	abstract = {Sign language translation, especially in gloss-free paradigm, is confronting a dilemma of impracticality and unsustainability due to growing resource-intensive methodologies. Contemporary state-of-the-arts (SOTAs) have significantly hinged on pretrained sophiscated backbones such as Large Language Models (LLMs), embedding sources, or extensive datasets, inducing considerable parametric and computational inefficiency for sustainable use in real-world scenario. Despite their success, following this research direction undermines the overarching mission of this domain to create substantial value to bridge hard-hearing and common populations. Committing to the prevailing trend of LLM and Natural Language Processing (NLP) studies, we pursue a profound essential change in architecture to achieve groundup improvements without external aid from pretrained models, prior knowledge transfer, or any NLP strategies considered not-from-scratch.},
	language = {en},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Yang, Eta},
	month = nov,
	year = {2024},
	note = {arXiv:2411.12901 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
	annote = {Comment: Official Code at: https://github.com/EtaEnding/Signformer/tree/main},
	file = {PDF:/home/dario/Zotero/storage/3Y9IWI9V/Yang - 2024 - Signformer is all you need Towards Edge AI for Sign Language.pdf:application/pdf},
}

@misc{gong_llms_2024,
	title = {{LLMs} are {Good} {Sign} {Language} {Translators}},
	url = {http://arxiv.org/abs/2404.00925},
	doi = {10.48550/arXiv.2404.00925},
	abstract = {Sign Language Translation (SLT) is a challenging task that aims to translate sign videos into spoken language. Inspired by the strong translation capabilities of large language models (LLMs) that are trained on extensive multilingual text corpora, we aim to harness off-the-shelf LLMs to handle SLT. In this paper, we regularize the sign videos to embody linguistic characteristics of spoken language, and propose a novel SignLLM framework to transform sign videos into a language-like representation for improved readability by off-the-shelf LLMs. SignLLM comprises two key modules: (1) The Vector-Quantized Visual Sign module converts sign videos into a sequence of discrete characterlevel sign tokens, and (2) the Codebook Reconstruction and Alignment module converts these character-level tokens into word-level sign representations using an optimal transport formulation. A sign-text alignment loss further bridges the gap between sign and text tokens, enhancing semantic compatibility. We achieve state-of-the-art gloss-free results on two widely-used SLT benchmarks.},
	language = {en},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Gong, Jia and Foo, Lin Geng and He, Yixuan and Rahmani, Hossein and Liu, Jun},
	month = apr,
	year = {2024},
	note = {arXiv:2404.00925 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to CVPR 2024},
	file = {PDF:/home/dario/Zotero/storage/GHEJLU3H/Gong et al. - 2024 - LLMs are Good Sign Language Translators.pdf:application/pdf},
}

@misc{mehta_mobilevit_2022,
	title = {{MobileViT}: {Light}-weight, {General}-purpose, and {Mobile}-friendly {Vision} {Transformer}},
	shorttitle = {{MobileViT}},
	url = {http://arxiv.org/abs/2110.02178},
	doi = {10.48550/arXiv.2110.02178},
	abstract = {Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision transformers (ViTs) have been adopted. Unlike CNNs, ViTs are heavyweight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers. Our results show that MobileViT signiﬁcantly outperforms CNNand ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4\% with about 6 million parameters, which is 3.2\% and 6.2\% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7\% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets.},
	language = {en},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Mehta, Sachin and Rastegari, Mohammad},
	month = mar,
	year = {2022},
	note = {arXiv:2110.02178 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Accepted at ICLR'22},
	file = {PDF:/home/dario/Zotero/storage/G4VZ4JEP/Mehta and Rastegari - 2022 - MobileViT Light-weight, General-purpose, and Mobile-friendly Vision Transformer.pdf:application/pdf},
}
