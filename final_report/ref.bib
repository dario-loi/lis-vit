
@misc{mehta_mobilevit_2022,
  title      = {{MobileViT}: {Light}-weight, {General}-purpose, and {Mobile}-friendly {Vision} {Transformer}},
  shorttitle = {{MobileViT}},
  url        = {http://arxiv.org/abs/2110.02178},
  doi        = {10.48550/arXiv.2110.02178},
  abstract   = {Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision transformers (ViTs) have been adopted. Unlike CNNs, ViTs are heavyweight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers. Our results show that MobileViT signiﬁcantly outperforms CNNand ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4\% with about 6 million parameters, which is 3.2\% and 6.2\% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7\% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets.},
  language   = {en},
  urldate    = {2024-11-28},
  publisher  = {arXiv},
  author     = {Mehta, Sachin and Rastegari, Mohammad},
  month      = mar,
  year       = {2022},
  note       = {arXiv:2110.02178 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  annote     = {Comment: Accepted at ICLR'22},
  file       = {PDF:/home/dario/Zotero/storage/G4VZ4JEP/Mehta and Rastegari - 2022 - MobileViT Light-weight, General-purpose, and Mobile-friendly Vision Transformer.pdf:application/pdf}
}

@misc{gong_llms_2024,
  title     = {{LLMs} are {Good} {Sign} {Language} {Translators}},
  url       = {http://arxiv.org/abs/2404.00925},
  doi       = {10.48550/arXiv.2404.00925},
  abstract  = {Sign Language Translation (SLT) is a challenging task that aims to translate sign videos into spoken language. Inspired by the strong translation capabilities of large language models (LLMs) that are trained on extensive multilingual text corpora, we aim to harness off-the-shelf LLMs to handle SLT. In this paper, we regularize the sign videos to embody linguistic characteristics of spoken language, and propose a novel SignLLM framework to transform sign videos into a language-like representation for improved readability by off-the-shelf LLMs. SignLLM comprises two key modules: (1) The Vector-Quantized Visual Sign module converts sign videos into a sequence of discrete characterlevel sign tokens, and (2) the Codebook Reconstruction and Alignment module converts these character-level tokens into word-level sign representations using an optimal transport formulation. A sign-text alignment loss further bridges the gap between sign and text tokens, enhancing semantic compatibility. We achieve state-of-the-art gloss-free results on two widely-used SLT benchmarks.},
  language  = {en},
  urldate   = {2024-11-27},
  publisher = {arXiv},
  author    = {Gong, Jia and Foo, Lin Geng and He, Yixuan and Rahmani, Hossein and Liu, Jun},
  month     = apr,
  year      = {2024},
  note      = {arXiv:2404.00925 [cs]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
  annote    = {Comment: Accepted to CVPR 2024},
  annote    = {Comment: Accepted to CVPR 2024},
  file      = {PDF:/home/dario/Zotero/storage/GHEJLU3H/Gong et al. - 2024 - LLMs are Good Sign Language Translators.pdf:application/pdf;Snapshot:/home/dario/Zotero/storage/HZWURZ9J/2404.html:text/html}
}

@misc{yang_signformer_2024,
  title      = {Signformer is all you need: {Towards} {Edge} {AI} for {Sign} {Language}},
  shorttitle = {Signformer is all you need},
  url        = {http://arxiv.org/abs/2411.12901},
  doi        = {10.48550/arXiv.2411.12901},
  abstract   = {Sign language translation, especially in gloss-free paradigm, is confronting a dilemma of impracticality and unsustainability due to growing resource-intensive methodologies. Contemporary state-of-the-arts (SOTAs) have significantly hinged on pretrained sophiscated backbones such as Large Language Models (LLMs), embedding sources, or extensive datasets, inducing considerable parametric and computational inefficiency for sustainable use in real-world scenario. Despite their success, following this research direction undermines the overarching mission of this domain to create substantial value to bridge hard-hearing and common populations. Committing to the prevailing trend of LLM and Natural Language Processing (NLP) studies, we pursue a profound essential change in architecture to achieve groundup improvements without external aid from pretrained models, prior knowledge transfer, or any NLP strategies considered not-from-scratch.},
  language   = {en},
  urldate    = {2024-11-27},
  publisher  = {arXiv},
  author     = {Yang, Eta},
  month      = nov,
  year       = {2024},
  note       = {arXiv:2411.12901 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
  annote     = {Comment: Official Code at: https://github.com/EtaEnding/Signformer/tree/main},
  file       = {PDF:/home/dario/Zotero/storage/3Y9IWI9V/Yang - 2024 - Signformer is all you need Towards Edge AI for Sign Language.pdf:application/pdf}
}

@misc{ahn_slowfast_2023,
  title     = {{SlowFast} {Network} for {Continuous} {Sign} {Language} {Recognition}},
  url       = {http://arxiv.org/abs/2309.12304},
  doi       = {10.48550/arXiv.2309.12304},
  abstract  = {The objective of this work is the effective extraction of spatial and dynamic features for Continuous Sign Language Recognition (CSLR). To accomplish this, we utilise a two-pathway SlowFast network, where each pathway operates at distinct temporal resolutions to separately capture spatial (hand shapes, facial expressions) and dynamic (movements) information. In addition, we introduce two distinct feature fusion methods, carefully designed for the characteristics of CSLR: (1) Bi-directional Feature Fusion (BFF), which facilitates the transfer of dynamic semantics into spatial semantics and vice versa; and (2) Pathway Feature Enhancement (PFE), which enriches dynamic and spatial representations through auxiliary subnetworks, while avoiding the need for extra inference time. As a result, our model further strengthens spatial and dynamic representations in parallel. We demonstrate that the proposed framework outperforms the current state-of-the-art performance on popular CSLR datasets, including PHOENIX14, PHOENIX14-T, and CSL-Daily.},
  language  = {en},
  urldate   = {2024-11-27},
  publisher = {arXiv},
  author    = {Ahn, Junseok and Jang, Youngjoon and Chung, Joon Son},
  month     = sep,
  year      = {2023},
  note      = {arXiv:2309.12304 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition},
  file      = {PDF:/home/dario/Zotero/storage/A9KRWFTI/Ahn et al. - 2023 - SlowFast Network for Continuous Sign Language Recognition.pdf:application/pdf}
}

@inproceedings{forster_rwth-phoenix-weather_2012,
  title      = {{RWTH}-{PHOENIX}-{Weather}: {A} {Large} {Vocabulary} {Sign} {Language} {Recognition} and {Translation} {Corpus}},
  shorttitle = {{RWTH}-{PHOENIX}-{Weather}},
  url        = {https://www.semanticscholar.org/paper/RWTH-PHOENIX-Weather%3A-A-Large-Vocabulary-Sign-and-Forster-Schmidt/29228179df78b2bc28c0c65cea2f1a43132993c6},
  abstract   = {This paper introduces the RWTH-PHOENIX-Weather corpus, a video-based, large vocabulary corpus of German Sign Language suitable for statistical sign language recognition and translation. In contrastto most available sign language data collections, the RWTH-PHOENIX-Weather corpus has not been recorded for linguistic research but for the use in statistical pattern recognition. The corpus contains weather forecasts recorded from German public TV which are manually annotated using glosses distinguishing sign variants, and time boundaries have been marked on the sentence and the gloss level. Further, the spoken German weather forecast has been transcribed in a semi-automatic fashion using a state-of-the-art automatic speech recognition system. Moreover, an additional translation of the glosses into spoken German has been created to capture allowable translation variability. In addition to the corpus, experimental baseline results for hand and head tracking, statistical sign language recognition and translation are presented.},
  urldate    = {2024-11-27},
  author     = {Forster, Jens and Schmidt, C. and Hoyoux, Thomas and Koller, Oscar and Zelle, Uwe and Piater, J. and Ney, H.},
  month      = may,
  year       = {2012},
  keywords   = {cslr, dataset},
  annote     = {[TLDR] The RWTH-PHOENIX-Weather corpus is introduced, a video-based, large vocabulary corpus of German Sign Language suitable for statistical sign language recognition and translation and experimental baseline results for hand and head tracking, statistical signlanguage recognition andtranslation are presented.},
  file       = {Full Text PDF:/home/dario/Zotero/storage/LTS2HQJ9/Forster et al. - 2012 - RWTH-PHOENIX-Weather A Large Vocabulary Sign Language Recognition and Translation Corpus.pdf:application/pdf}
}


@misc{dosovitskiy_image_2021,
  title      = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
  shorttitle = {An {Image} is {Worth} 16x16 {Words}},
  url        = {http://arxiv.org/abs/2010.11929},
  doi        = {10.48550/arXiv.2010.11929},
  abstract   = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  urldate    = {2024-12-22},
  publisher  = {arXiv},
  author     = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  month      = jun,
  year       = {2021},
  note       = {arXiv:2010.11929 [cs]
                version: 2},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
  annote     = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)},
  file       = {Preprint PDF:/home/dario/Zotero/storage/PJ7DLD49/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf:application/pdf;Snapshot:/home/dario/Zotero/storage/SRRYEKGC/2010.html:text/html}
}


@misc{hu_continuous_2023,
  title     = {Continuous {Sign} {Language} {Recognition} with {Correlation} {Network}},
  url       = {http://arxiv.org/abs/2303.03202},
  doi       = {10.48550/arXiv.2303.03202},
  abstract  = {Human body trajectories are a salient cue to identify actions in the video. Such body trajectories are mainly conveyed by hands and face across consecutive frames in sign language. However, current methods in continuous sign language recognition (CSLR) usually process frames independently, thus failing to capture cross-frame trajectories to effectively identify a sign. To handle this limitation, we propose correlation network (CorrNet) to explicitly capture and leverage body trajectories across frames to identify signs. In specific, a correlation module is first proposed to dynamically compute correlation maps between the current frame and adjacent frames to identify trajectories of all spatial patches. An identification module is then presented to dynamically emphasize the body trajectories within these correlation maps. As a result, the generated features are able to gain an overview of local temporal movements to identify a sign. Thanks to its special attention on body trajectories, CorrNet achieves new state-of-the-art accuracy on four large-scale datasets, i.e., PHOENIX14, PHOENIX14-T, CSL-Daily, and CSL. A comprehensive comparison with previous spatial-temporal reasoning methods verifies the effectiveness of CorrNet. Visualizations demonstrate the effects of CorrNet on emphasizing human body trajectories across adjacent frames.},
  urldate   = {2024-11-27},
  publisher = {arXiv},
  author    = {Hu, Lianyu and Gao, Liqing and Liu, Zekang and Feng, Wei},
  month     = mar,
  year      = {2023},
  note      = {arXiv:2303.03202},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition, cslr, cnn},
  file      = {Preprint PDF:/home/dario/Zotero/storage/I4CKNFJ5/Hu et al. - 2023 - Continuous Sign Language Recognition with Correlation Network.pdf:application/pdf;Snapshot:/home/dario/Zotero/storage/YJDHIBD2/2303.html:text/html}
}


@misc{loshchilov_decoupled_2019,
  title     = {Decoupled {Weight} {Decay} {Regularization}},
  url       = {http://arxiv.org/abs/1711.05101},
  doi       = {10.48550/arXiv.1711.05101},
  abstract  = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  urldate   = {2024-12-22},
  publisher = {arXiv},
  author    = {Loshchilov, Ilya and Hutter, Frank},
  month     = jan,
  year      = {2019},
  note      = {arXiv:1711.05101 [cs]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
  annote    = {Comment: Published as a conference paper at ICLR 2019},
  file      = {Preprint PDF:/home/dario/Zotero/storage/9U2LXG6F/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf:application/pdf;Snapshot:/home/dario/Zotero/storage/7ENDEU36/1711.html:text/html}
}


@inproceedings{burgess_bfloat16_2019,
  title     = {Bfloat16 {Processing} for {Neural} {Networks}},
  url       = {https://ieeexplore.ieee.org/document/8877390},
  doi       = {10.1109/ARITH.2019.00022},
  abstract  = {Bfloat16 ("BF16") is a new floating-point format tailored specifically for high-performance processing of Neural Networks and will be supported by major CPU and GPU architectures as well as Neural Network accelerators. This paper proposes a possible implementation of a BF16 multiply-accumulation operation that relaxes several IEEE Floating-Point Standard features to afford low-cost hardware implementations. Specifically, subnorms are flushed to zero; only one non-standard rounding mode (Round-Odd) is supported; NaNs are not propagated; and IEEE exception flags are not provided. The paper shows that this approach achieves the same network-level accuracy as using IEEE single-precision arithmetic ("FP32") for less than half the datapath area cost and with greater throughput.},
  urldate   = {2024-12-22},
  booktitle = {2019 {IEEE} 26th {Symposium} on {Computer} {Arithmetic} ({ARITH})},
  author    = {Burgess, Neil and Milanovic, Jelena and Stephens, Nigel and Monachopoulos, Konstantinos and Mansell, David},
  month     = jun,
  year      = {2019},
  note      = {ISSN: 2576-2265},
  keywords  = {Artificial neural networks, Computer architecture, Digital arithmetic, Error analysis, floating-point, rounding mode, neural networks, Standards, Training},
  pages     = {88--91},
  file      = {IEEE Xplore Abstract Record:/home/dario/Zotero/storage/W8EME6A5/8877390.html:text/html}
}


@software{Falcon_PyTorch_Lightning_2019,
  author  = {Falcon, William and {The PyTorch Lightning team}},
  doi     = {10.5281/zenodo.3828935},
  license = {Apache-2.0},
  month   = mar,
  title   = {{PyTorch Lightning}},
  url     = {https://github.com/Lightning-AI/lightning},
  version = {1.4},
  year    = {2019}
}

@article{Rogozhnikov_Einops_Clear_and_2022,
  author  = {Rogozhnikov, Alex},
  journal = {International Conference on Learning Representations},
  title   = {{Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation}},
  url     = {https://openreview.net/forum?id=oapKSVM2bc},
  year    = {2022}
}

@inproceedings{sandler_mobilenetv2_2018,
  address    = {Salt Lake City, UT},
  title      = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
  isbn       = {978-1-5386-6420-9},
  shorttitle = {{MobileNetV2}},
  url        = {https://ieeexplore.ieee.org/document/8578572/},
  doi        = {10.1109/CVPR.2018.00474},
  abstract   = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efﬁcient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.},
  language   = {en},
  urldate    = {2024-12-02},
  booktitle  = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
  publisher  = {IEEE},
  author     = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  month      = jun,
  year       = {2018},
  pages      = {4510--4520},
  file       = {PDF:/home/dario/Zotero/storage/Y2Q2C8B5/Sandler et al. - 2018 - MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf:application/pdf}
}

@article{graves_connectionist_nodate,
  title    = {Connectionist {Temporal} {Classiﬁcation}: {Labelling} {Unsegmented} {Sequence} {Data} with {Recurrent} {Neural} {Networks}},
  abstract = {Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.},
  language = {en},
  author   = {Graves, Alex and Fernandez, Santiago and Gomez, Faustino and Schmidhuber, Jurgen},
  file     = {PDF:/home/dario/Zotero/storage/UKYEAU7Z/Graves et al. - Connectionist Temporal Classiﬁcation Labelling Unsegmented Sequence Data with Recurrent Neural Netw.pdf:application/pdf}
}

@article{graves_connectionist_nodate-1,
  title    = {Connectionist {Temporal} {Classiﬁcation}: {Labelling} {Unsegmented} {Sequence} {Data} with {Recurrent} {Neural} {Networks}},
  abstract = {Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.},
  language = {en},
  author   = {Graves, Alex and Fernandez, Santiago and Gomez, Faustino and Schmidhuber, Jurgen},
  file     = {PDF:/home/dario/Zotero/storage/W2FNYAYV/Graves et al. - Connectionist Temporal Classiﬁcation Labelling Unsegmented Sequence Data with Recurrent Neural Netw.pdf:application/pdf}
}

@inproceedings{zuo_local_2022,
  title     = {Local {Context}-aware {Self}-attention for {Continuous} {Sign} {Language} {Recognition}},
  url       = {https://www.isca-archive.org/interspeech_2022/zuo22_interspeech.html},
  doi       = {10.21437/Interspeech.2022-164},
  abstract  = {Transformer-based architectures are adopted in many continuous sign language recognition (CSLR) works for sequence modeling due to their strong capability of extracting global contexts. However, since vanilla self-attention (SA), the core module of Transformer, computes a weighted average over all time steps, the local temporal semantics of sign videos may not be fully exploited. In this work, we propose local context-aware selfattention (LCSA) to enhance the vanilla SA to leverage both local and global contexts. We introduce the local contexts at two different levels of model computation: score and query levels. At the score level, we modulate the attention scores explicitly with an additional Gaussian bias. At the query level, local contexts are modeled implicitly using depth-wise temporal convolutional networks (DTCNs). However, the vanilla Gaussian bias has two major shortcomings: first, its window size is fixed and needs to be fine-tuned laboriously; second, the fixed window size is common among all time steps. In this work, a dynamic Gaussian bias is further proposed to address the above issues. Experimental results on two benchmarks, PHOENIX-2014 and CSL, validate the effectiveness and superiority of our method.},
  language  = {en},
  urldate   = {2024-12-19},
  booktitle = {Interspeech 2022},
  publisher = {ISCA},
  author    = {Zuo, Ronglai and Mak, Brian},
  month     = sep,
  year      = {2022},
  pages     = {4810--4814},
  file      = {PDF:/home/dario/Zotero/storage/NIRTM64I/Zuo and Mak - 2022 - Local Context-aware Self-attention for Continuous Sign Language Recognition.pdf:application/pdf}
}
