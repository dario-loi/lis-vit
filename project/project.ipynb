{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIS With MobileViTs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Our MobileViTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import wandb\n",
    "from einops import rearrange\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize this impl:\n",
    "# https://github.com/chinhsuanwu/mobilevit-pytorch/blob/master/mobilevit.py\n",
    "\n",
    "\n",
    "class Conv2DBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        groups=1,\n",
    "        bias=True,\n",
    "        norm=True,\n",
    "        activation=True,\n",
    "    ):\n",
    "        \"\"\"__init__ Constructor for Conv2DBlock\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            Number of input channels\n",
    "        out_channels : int\n",
    "            Number of output channels\n",
    "        kernel_size : int\n",
    "            Size of the kernel\n",
    "        stride : int\n",
    "            Stride of the convolutional layer\n",
    "        padding : int\n",
    "            Padding of the convolutional layer\n",
    "        groups : int\n",
    "            Number of groups\n",
    "        bias : bool\n",
    "            Whether to use bias\n",
    "        \"\"\"\n",
    "\n",
    "        super(Conv2DBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential()\n",
    "\n",
    "        self.block.add_module(\n",
    "            \"conv2d\",\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                groups=groups,\n",
    "                bias=bias,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if norm:\n",
    "            self.block.add_module(\"norm\", nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        if activation:\n",
    "            self.block.add_module(\"activation\", nn.SiLU())\n",
    "\n",
    "        self.block = torch.compile(self.block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class MobileBlockV2(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride, expand_ratio):\n",
    "        \"\"\"__init__ Constructor for MobileBlockV2\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            Number of input channels\n",
    "        out_channels : int\n",
    "            Number of output channels\n",
    "        stride : int\n",
    "            Stride of the convolutional layer\n",
    "        expand_ratio : int\n",
    "            Expansion ratio of the block\n",
    "        \"\"\"\n",
    "\n",
    "        super(MobileBlockV2, self).__init__()\n",
    "\n",
    "        assert stride in [1, 2], \"Stride must be either 1 or 2\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.hidden_dim = int(round(in_channels * expand_ratio))\n",
    "\n",
    "        self.mbv2 = nn.Sequential()\n",
    "        self.uses_inverse_residual = (\n",
    "            self.in_channels == self.out_channels and self.stride == 1\n",
    "        )\n",
    "\n",
    "        if self.expand_ratio == 1:\n",
    "            self.mbv2.add_module(\n",
    "                \"depthwise_3x3\",\n",
    "                Conv2DBlock(  # Depthwise Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=3,\n",
    "                    stride=stride,\n",
    "                    padding=1,\n",
    "                    groups=self.hidden_dim,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                ),\n",
    "            )\n",
    "            self.mbv2.add_module(\n",
    "                \"pointwise-linear_1x1\",\n",
    "                Conv2DBlock(  # Pointwise-Linear Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    groups=self.hidden_dim,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=False,\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            self.mbv2.add_module(\n",
    "                \"pointwise_1x1\",\n",
    "                Conv2DBlock(  # Pointwise-Linear Convolution\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    groups=1,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                ),\n",
    "            )\n",
    "            self.mbv2.add_module(\n",
    "                \"depthwise_3x3\",\n",
    "                Conv2DBlock(  # Depthwise Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=3,\n",
    "                    stride=stride,\n",
    "                    padding=1,\n",
    "                    groups=self.hidden_dim,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                ),\n",
    "            )\n",
    "            self.mbv2.add_module(\n",
    "                \"pointwise-linear_1x1\",\n",
    "                Conv2DBlock(  # Pointwise-Linear Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    groups=1,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=False,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        self.mbv2 = torch.compile(self.mbv2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.uses_inverse_residual:\n",
    "            return x + self.mbv2(x)\n",
    "        else:\n",
    "            return self.mbv2(x)\n",
    "\n",
    "\n",
    "class MobileViTBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, hidden_dim, depth, channels, kernel_size, patch_size, mlp_dim, dropout=0.0\n",
    "    ):\n",
    "\n",
    "        super(MobileViTBlock, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.depth = depth\n",
    "        self.channels = channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.patch_size = patch_size\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.local_conv = torch.compile(\n",
    "            nn.Sequential(\n",
    "                Conv2DBlock(\n",
    "                    in_channels=channels,\n",
    "                    out_channels=channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=1,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                Conv2DBlock(\n",
    "                    in_channels=channels,\n",
    "                    out_channels=hidden_dim,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                    bias=False,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        self.global_transformer = torch.compile(\n",
    "            nn.TransformerEncoder(\n",
    "                nn.TransformerEncoderLayer(\n",
    "                    d_model=hidden_dim,\n",
    "                    nhead=8,\n",
    "                    dim_feedforward=mlp_dim,\n",
    "                    dropout=dropout,\n",
    "                    batch_first=True,\n",
    "                    activation=\"gelu\",\n",
    "                ),\n",
    "                num_layers=depth,\n",
    "                norm=nn.LayerNorm(hidden_dim),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.fusion_conv_preres = torch.compile(\n",
    "            Conv2DBlock(\n",
    "                in_channels=hidden_dim,\n",
    "                out_channels=channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=False,\n",
    "                norm=True,\n",
    "                activation=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.fusion_conv_postres = torch.compile(\n",
    "            Conv2DBlock(\n",
    "                in_channels=2 * channels,\n",
    "                out_channels=channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "                norm=True,\n",
    "                activation=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_res = x.clone()\n",
    "\n",
    "        # local_repr\n",
    "        x = self.local_conv(x)\n",
    "\n",
    "        ph, pw = self.patch_size\n",
    "\n",
    "        # global_repr\n",
    "        _, _, h, w = x.shape\n",
    "        x = rearrange(  # reshape the image into patches for ViT input\n",
    "            x,\n",
    "            \"b d (h ph) (w pw) -> (b h w) (ph pw) d\",\n",
    "            ph=ph,\n",
    "            pw=pw,\n",
    "        )\n",
    "        x = self.global_transformer(x)\n",
    "        x = rearrange(\n",
    "            x,\n",
    "            \"(b h w) (ph pw) d -> b d (h ph) (w pw)\",\n",
    "            h=h // ph,\n",
    "            w=w // pw,\n",
    "            ph=ph,\n",
    "            pw=pw,\n",
    "        )\n",
    "\n",
    "        # fusion\n",
    "        x = self.fusion_conv_preres(x)\n",
    "        x = torch.cat([x, x_res], dim=1)\n",
    "        x = self.fusion_conv_postres(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MobileViT(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dims,\n",
    "        conv_channels,\n",
    "        num_classes,\n",
    "        expand_ratio=4,\n",
    "        patch_size=(2, 2),\n",
    "    ):\n",
    "        super(MobileViT, self).__init__()\n",
    "\n",
    "        self.dims = dims\n",
    "        self.conv_channels = conv_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.patch_size = patch_size\n",
    "        self.kernel_size = 3\n",
    "\n",
    "        L = [2, 4, 3]\n",
    "\n",
    "        self.in_conv = Conv2DBlock(\n",
    "            in_channels=3,\n",
    "            out_channels=conv_channels[0],\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            norm=True,\n",
    "            activation=True,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.mv2_blocks = nn.ModuleList([])\n",
    "\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[0], conv_channels[1], 1, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[1], conv_channels[2], 2, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[2], conv_channels[3], 1, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[2], conv_channels[3], 1, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[3], conv_channels[4], 2, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[5], conv_channels[6], 2, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[7], conv_channels[8], 2, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.mvit_blocks = nn.ModuleList([])\n",
    "\n",
    "        self.mvit_blocks.append(\n",
    "            MobileViTBlock(\n",
    "                dims[0],\n",
    "                L[0],\n",
    "                conv_channels[5],\n",
    "                self.kernel_size,\n",
    "                patch_size,\n",
    "                int(dims[0] * 2),\n",
    "            )\n",
    "        )\n",
    "        self.mvit_blocks.append(\n",
    "            MobileViTBlock(\n",
    "                dims[1],\n",
    "                L[1],\n",
    "                conv_channels[7],\n",
    "                self.kernel_size,\n",
    "                patch_size,\n",
    "                int(dims[1] * 4),\n",
    "            )\n",
    "        )\n",
    "        self.mvit_blocks.append(\n",
    "            MobileViTBlock(\n",
    "                dims[2],\n",
    "                L[2],\n",
    "                conv_channels[9],\n",
    "                self.kernel_size,\n",
    "                patch_size,\n",
    "                int(dims[2] * 4),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.final_pw = Conv2DBlock(\n",
    "            in_channels=conv_channels[-2],\n",
    "            out_channels=conv_channels[-1],\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            groups=1,\n",
    "            bias=False,\n",
    "            norm=True,\n",
    "            activation=False,\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(conv_channels[-1], num_classes, bias=False)\n",
    "\n",
    "        ## Training-related members\n",
    "        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "        self.apply(self.init_weights)  # Initialize weights\n",
    "\n",
    "    def init_weights(self, m):\n",
    "\n",
    "        if type(m) == nn.Conv2d:\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif type(m) == nn.BatchNorm2d:\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif type(m) == nn.Linear:\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.in_conv(x)\n",
    "\n",
    "        for i in range(5):\n",
    "            x = self.mv2_blocks[i](x)\n",
    "\n",
    "        x = self.mvit_blocks[0](x)\n",
    "        x = self.mv2_blocks[5](x)\n",
    "        x = self.mvit_blocks[1](x)\n",
    "        x = self.mv2_blocks[6](x)\n",
    "        x = self.mvit_blocks[2](x)\n",
    "        x = self.final_pw(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def compute_metrics(self, y_hat, y):\n",
    "        _, preds = torch.max(y_hat, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        tp = ((preds == y) & (y == 1)).sum().float()\n",
    "        fp = ((preds != y) & (preds == 1)).sum().float()\n",
    "        fn = ((preds != y) & (y == 1)).sum().float()\n",
    "\n",
    "        precision = tp / (tp + fp + 1e-8)\n",
    "        recall = tp / (tp + fn + 1e-8)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "        return acc, precision, recall, f1\n",
    "\n",
    "    def log_metrics(self, phase, loss, acc, prec, rec, f1):\n",
    "        self.log(f\"{phase}/loss\", loss, prog_bar=True)\n",
    "        self.log(f\"{phase}/accuracy\", acc, prog_bar=True)\n",
    "        self.log(f\"{phase}/precision\", prec, prog_bar=True)\n",
    "        self.log(f\"{phase}/recall\", rec, prog_bar=True)\n",
    "        self.log(f\"{phase}/f1\", f1, prog_bar=True)\n",
    "\n",
    "    def step(self, batch, phase):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc, prec, rec, f1 = self.compute_metrics(y_hat, y)\n",
    "        self.log_metrics(phase, loss, acc, prec, rec, f1)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), lr=2e-4, amsgrad=True, weight_decay=0.01\n",
    "        )\n",
    "        scheduler = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.utilities.model_summary import summarize\n",
    "from lightning.fabric.utilities import measure_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xxs_mvit(classes: int = 10):\n",
    "    return MobileViT(\n",
    "        dims=[64, 80, 96],\n",
    "        conv_channels=[16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 320],\n",
    "        num_classes=classes,\n",
    "        expand_ratio=2,\n",
    "        patch_size=(2, 2),\n",
    "    )\n",
    "\n",
    "\n",
    "def xs_mvit(classes: int = 10):\n",
    "    return MobileViT(\n",
    "        dims=[96, 120, 144],\n",
    "        conv_channels=[16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384],\n",
    "        num_classes=classes,\n",
    "        expand_ratio=4,\n",
    "        patch_size=(4, 4),\n",
    "    )\n",
    "\n",
    "\n",
    "def s_mvit(classes: int = 10):\n",
    "    return MobileViT(\n",
    "        dims=[144, 192, 240],\n",
    "        conv_channels=[16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 640],\n",
    "        num_classes=classes,\n",
    "        expand_ratio=4,\n",
    "        patch_size=(8, 8),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 dataset\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "LFWPeople_trainval = torchvision.datasets.LFWPeople(\n",
    "    root=\"./data\", split=\"train\", download=True, transform=transform\n",
    ")\n",
    "LFWPeople_test = torchvision.datasets.LFWPeople(\n",
    "    root=\"./data\", split=\"test\", download=True, transform=transform\n",
    ")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_size = int(0.8 * len(LFWPeople_trainval))\n",
    "val_size = len(LFWPeople_trainval) - train_size\n",
    "\n",
    "\n",
    "LFWPeople_train, LFWPeople_val = torch.utils.data.random_split(\n",
    "    LFWPeople_trainval,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n",
    "\n",
    "wandb.login(key=secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:x47z66mn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dainty-salad-14</strong> at: <a href='https://wandb.ai/academic_projects/mobilevit/runs/x47z66mn' target=\"_blank\">https://wandb.ai/academic_projects/mobilevit/runs/x47z66mn</a><br/> View project at: <a href='https://wandb.ai/academic_projects/mobilevit' target=\"_blank\">https://wandb.ai/academic_projects/mobilevit</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241203_130139-x47z66mn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:x47z66mn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dario/repos/lis-vit/project/wandb/run-20241203_130427-ws4252k4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/academic_projects/mobilevit/runs/ws4252k4' target=\"_blank\">rural-salad-15</a></strong> to <a href='https://wandb.ai/academic_projects/mobilevit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/academic_projects/mobilevit' target=\"_blank\">https://wandb.ai/academic_projects/mobilevit</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/academic_projects/mobilevit/runs/ws4252k4' target=\"_blank\">https://wandb.ai/academic_projects/mobilevit/runs/ws4252k4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/academic_projects/mobilevit/runs/ws4252k4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7aa2080e8690>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"mobilevit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/dario/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | in_conv     | Conv2DBlock       | 464    | train\n",
      "1 | mv2_blocks  | ModuleList        | 183 K  | train\n",
      "2 | mvit_blocks | ModuleList        | 2.2 M  | train\n",
      "3 | final_pw    | Conv2DBlock       | 37.6 K | train\n",
      "4 | pool        | AdaptiveAvgPool2d | 0      | train\n",
      "5 | classifier  | Linear            | 2.2 M  | train\n",
      "6 | criterion   | CrossEntropyLoss  | 0      | train\n",
      "----------------------------------------------------------\n",
      "4.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 M     Total params\n",
      "18.489    Total estimated model params size (MB)\n",
      "345       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1203 13:05:05.929000 24231 torch/_dynamo/convert_frame.py:844] [21/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W1203 13:05:05.929000 24231 torch/_dynamo/convert_frame.py:844] [21/8]    function: 'inner' (/home/dario/repos/lis-vit/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:38)\n",
      "W1203 13:05:05.929000 24231 torch/_dynamo/convert_frame.py:844] [21/8]    last reason: 21/0: len(L['fn']) == 3                                           \n",
      "W1203 13:05:05.929000 24231 torch/_dynamo/convert_frame.py:844] [21/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1203 13:05:05.929000 24231 torch/_dynamo/convert_frame.py:844] [21/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1905 [00:00<?, ?it/s]                          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1203 13:05:27.161000 24231 torch/_dynamo/convert_frame.py:844] [22/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W1203 13:05:27.161000 24231 torch/_dynamo/convert_frame.py:844] [22/8]    function: 'forward' (/tmp/ipykernel_24231/740721331.py:64)\n",
      "W1203 13:05:27.161000 24231 torch/_dynamo/convert_frame.py:844] [22/8]    last reason: 22/0: GLOBAL_STATE changed: grad_mode \n",
      "W1203 13:05:27.161000 24231 torch/_dynamo/convert_frame.py:844] [22/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1203 13:05:27.161000 24231 torch/_dynamo/convert_frame.py:844] [22/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1905/1905 [05:29<00:00,  5.79it/s, v_num=52k4, train/loss=8.370, train/accuracy=0.000, train/precision=0.000, train/recall=0.000, train/f1=0.000, val/loss=8.380, val/accuracy=0.0373, val/precision=0.000, val/recall=0.000, val/f1=0.000]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Early stopping conditioned on metric `val_loss` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: `train/loss`, `train/accuracy`, `train/precision`, `train/recall`, `train/f1`, `val/loss`, `val/accuracy`, `val/precision`, `val/recall`, `val/f1`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 42\u001b[0m\n\u001b[1;32m     21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     22\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     23\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     ],\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     40\u001b[0m wandb_logger\u001b[38;5;241m.\u001b[39mwatch(model, log_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 42\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:206\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance()\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:378\u001b[0m, in \u001b[0;36m_FitLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    376\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, monitoring_callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    377\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 378\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_epoch_end\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitoring_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mon_epoch_end()\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39m_num_ready_batches_reached():\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;66;03m# if we are restarting and the above condition holds, it's because we are reloading an epoch-end checkpoint.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# since metric-based schedulers require access to metrics and those are not currently saved in the\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# checkpoint, the plateau schedulers shouldn't be updated\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:218\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 218\u001b[0m             \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/callbacks/early_stopping.py:190\u001b[0m, in \u001b[0;36mEarlyStopping.on_train_epoch_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_on_train_epoch_end \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_skip_check(trainer):\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_early_stopping_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/callbacks/early_stopping.py:202\u001b[0m, in \u001b[0;36mEarlyStopping._run_early_stopping_check\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Checks whether the early stopping condition is met and if so tells the trainer to stop the training.\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m logs \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mcallback_metrics\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mfast_dev_run \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_condition_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# disable early_stopping with fast_dev_run\u001b[39;49;00m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogs\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:  \u001b[38;5;66;03m# short circuit if metric not present\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    207\u001b[0m current \u001b[38;5;241m=\u001b[39m logs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonitor]\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/callbacks/early_stopping.py:153\u001b[0m, in \u001b[0;36mEarlyStopping._validate_condition_metric\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m monitor_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrict:\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_msg)\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    155\u001b[0m         rank_zero_warn(error_msg, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mRuntimeWarning\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Early stopping conditioned on metric `val_loss` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: `train/loss`, `train/accuracy`, `train/precision`, `train/recall`, `train/f1`, `val/loss`, `val/accuracy`, `val/precision`, `val/recall`, `val/f1`"
     ]
    }
   ],
   "source": [
    "import lightning.pytorch as lp\n",
    "\n",
    "# setup training and wandb\n",
    "\n",
    "N_LABELS = len(LFWPeople_trainval.class_to_idx)\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "wandb_logger = lp.loggers.WandbLogger()\n",
    "\n",
    "model = xs_mvit(N_LABELS)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    LFWPeople_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=3\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    LFWPeople_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=3\n",
    ")\n",
    "test_loader = DataLoader(LFWPeople_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"cuda\",\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[\n",
    "        lp.callbacks.ModelCheckpoint(\n",
    "            monitor=\"val/loss\",\n",
    "            filename=\"best_model\",\n",
    "            save_top_k=1,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        lp.callbacks.EarlyStopping(\n",
    "            monitor=\"val/ loss\",\n",
    "            patience=3,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "wandb_logger.watch(model, log_graph=False)\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flush cuda cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
