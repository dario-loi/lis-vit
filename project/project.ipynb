{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIS With MobileViTs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Our MobileViTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import wandb\n",
    "from einops import rearrange\n",
    "import torchvision\n",
    "from phoenix_datasets import PhoenixVideoTextDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "from torchmetrics.text import WordErrorRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize this impl:\n",
    "# https://github.com/chinhsuanwu/mobilevit-pytorch/blob/master/mobilevit.py\n",
    "\n",
    "\n",
    "class Conv2DBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        groups=1,\n",
    "        bias=True,\n",
    "        norm=True,\n",
    "        activation=True,\n",
    "    ):\n",
    "        \"\"\"__init__ Constructor for Conv2DBlock\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            Number of input channels\n",
    "        out_channels : int\n",
    "            Number of output channels\n",
    "        kernel_size : int\n",
    "            Size of the kernel\n",
    "        stride : int\n",
    "            Stride of the convolutional layer\n",
    "        padding : int\n",
    "            Padding of the convolutional layer\n",
    "        groups : int\n",
    "            Number of groups\n",
    "        bias : bool\n",
    "            Whether to use bias\n",
    "        \"\"\"\n",
    "\n",
    "        super(Conv2DBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential()\n",
    "\n",
    "        self.block.add_module(\n",
    "            \"conv2d\",\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                groups=groups,\n",
    "                bias=bias,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if norm:\n",
    "            self.block.add_module(\"norm\", nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        if activation:\n",
    "            self.block.add_module(\"activation\", nn.SiLU())  # sigmoid(x) * x // SWISH\n",
    "\n",
    "        self.block = self.block\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class MobileBlockV2(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride, expand_ratio):\n",
    "        \"\"\"__init__ Constructor for MobileBlockV2\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            Number of input channels\n",
    "        out_channels : int\n",
    "            Number of output channels\n",
    "        stride : int\n",
    "            Stride of the convolutional layer\n",
    "        expand_ratio : int\n",
    "            Expansion ratio of the block\n",
    "        \"\"\"\n",
    "\n",
    "        super(MobileBlockV2, self).__init__()\n",
    "\n",
    "        assert stride in [1, 2], \"Stride must be either 1 or 2\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.hidden_dim = int(round(in_channels * expand_ratio))\n",
    "\n",
    "        self.mbv2 = nn.Sequential()\n",
    "        self.uses_inverse_residual = (\n",
    "            self.in_channels == self.out_channels and self.stride == 1\n",
    "        )\n",
    "\n",
    "        if self.expand_ratio == 1:\n",
    "            self.mbv2.add_module(\n",
    "                \"depthwise_3x3\",\n",
    "                Conv2DBlock(  # Depthwise Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=3,\n",
    "                    stride=stride,\n",
    "                    padding=1,\n",
    "                    groups=self.hidden_dim,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                ),\n",
    "            )\n",
    "            self.mbv2.add_module(\n",
    "                \"pointwise-linear_1x1\",\n",
    "                Conv2DBlock(  # Pointwise-Linear Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    groups=self.hidden_dim,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=False,\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            self.mbv2.add_module(\n",
    "                \"pointwise_1x1\",\n",
    "                Conv2DBlock(  # Pointwise-Linear Convolution\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    groups=1,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                ),\n",
    "            )\n",
    "            self.mbv2.add_module(\n",
    "                \"depthwise_3x3\",\n",
    "                Conv2DBlock(  # Depthwise Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=3,\n",
    "                    stride=stride,\n",
    "                    padding=1,\n",
    "                    groups=self.hidden_dim,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                ),\n",
    "            )\n",
    "            self.mbv2.add_module(\n",
    "                \"pointwise-linear_1x1\",\n",
    "                Conv2DBlock(  # Pointwise-Linear Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    groups=1,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=False,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        self.mbv2 = self.mbv2\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.uses_inverse_residual:\n",
    "            return x + self.mbv2(x)\n",
    "        else:\n",
    "            return self.mbv2(x)\n",
    "\n",
    "\n",
    "class MobileViTBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, hidden_dim, depth, channels, kernel_size, patch_size, mlp_dim, dropout=0.0\n",
    "    ):\n",
    "\n",
    "        super(MobileViTBlock, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.depth = depth\n",
    "        self.channels = channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.patch_size = patch_size\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.local_conv = nn.Sequential(\n",
    "            Conv2DBlock(\n",
    "                in_channels=channels,\n",
    "                out_channels=channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                norm=True,\n",
    "                activation=True,\n",
    "                bias=False,\n",
    "            ),\n",
    "            Conv2DBlock(\n",
    "                in_channels=channels,\n",
    "                out_channels=hidden_dim,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                norm=True,\n",
    "                activation=True,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        self.global_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_dim,\n",
    "                nhead=8,\n",
    "                dim_feedforward=mlp_dim,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "                activation=\"gelu\",\n",
    "            ),\n",
    "            num_layers=depth,\n",
    "            norm=nn.LayerNorm(hidden_dim),\n",
    "        )\n",
    "\n",
    "        self.fusion_conv_preres = Conv2DBlock(\n",
    "            in_channels=hidden_dim,\n",
    "            out_channels=channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=False,\n",
    "            norm=True,\n",
    "            activation=True,\n",
    "        )\n",
    "\n",
    "        self.fusion_conv_postres = Conv2DBlock(\n",
    "            in_channels=2 * channels,\n",
    "            out_channels=channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "            norm=True,\n",
    "            activation=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n",
    "        x_res = x.clone()\n",
    "\n",
    "        # local_repr\n",
    "        x = self.local_conv(x)\n",
    "\n",
    "        ph, pw = self.patch_size\n",
    "\n",
    "        # global_repr\n",
    "        _, _, h, w = x.shape\n",
    "        x = rearrange(  # reshape the image into patches for ViT input\n",
    "            x,\n",
    "            \"(b t) d (h ph) (w pw) -> (b h w) (t ph pw) d\",\n",
    "            ph=ph,\n",
    "            pw=pw,\n",
    "            b=B,\n",
    "            t=T,\n",
    "        )\n",
    "        x = self.global_transformer(x)\n",
    "        x = rearrange(\n",
    "            x,\n",
    "            \"(b h w) (t ph pw) d -> (b t) d (h ph) (w pw)\",\n",
    "            h=h // ph,\n",
    "            w=w // pw,\n",
    "            ph=ph,\n",
    "            pw=pw,\n",
    "            b=B,\n",
    "            t=T,\n",
    "        )\n",
    "\n",
    "        # fusion\n",
    "        x = self.fusion_conv_preres(x)\n",
    "        x = torch.cat([x, x_res], dim=1)\n",
    "        x = self.fusion_conv_postres(x)\n",
    "\n",
    "        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MobileViT(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dims,\n",
    "        conv_channels,\n",
    "        num_classes,\n",
    "        vocabulary,\n",
    "        expand_ratio=4,\n",
    "        patch_size=(2, 2),\n",
    "    ):\n",
    "        super(MobileViT, self).__init__()\n",
    "\n",
    "        self.dims = dims\n",
    "        self.conv_channels = conv_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.patch_size = patch_size\n",
    "        self.kernel_size = 3\n",
    "\n",
    "        L = [2, 4, 3]\n",
    "\n",
    "        self.in_conv = Conv2DBlock(\n",
    "            in_channels=3,\n",
    "            out_channels=conv_channels[0],\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            norm=True,\n",
    "            activation=True,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.mv2_blocks = nn.ModuleList([])\n",
    "\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[0], conv_channels[1], 1, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[1], conv_channels[2], 2, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[2], conv_channels[3], 1, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[2], conv_channels[3], 1, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[3], conv_channels[4], 2, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[5], conv_channels[6], 2, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[7], conv_channels[8], 2, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.mvit_blocks = nn.ModuleList([])\n",
    "\n",
    "        self.mvit_blocks.append(\n",
    "            MobileViTBlock(\n",
    "                dims[0],\n",
    "                L[0],\n",
    "                conv_channels[5],\n",
    "                self.kernel_size,\n",
    "                patch_size,\n",
    "                int(dims[0] * 2),\n",
    "            )\n",
    "        )\n",
    "        self.mvit_blocks.append(\n",
    "            MobileViTBlock(\n",
    "                dims[1],\n",
    "                L[1],\n",
    "                conv_channels[7],\n",
    "                self.kernel_size,\n",
    "                patch_size,\n",
    "                int(dims[1] * 4),\n",
    "            )\n",
    "        )\n",
    "        self.mvit_blocks.append(\n",
    "            MobileViTBlock(\n",
    "                dims[2],\n",
    "                L[2],\n",
    "                conv_channels[9],\n",
    "                self.kernel_size,\n",
    "                patch_size,\n",
    "                int(dims[2] * 4),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.final_pw = Conv2DBlock(\n",
    "            in_channels=conv_channels[-2],\n",
    "            out_channels=conv_channels[-1],\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            groups=1,\n",
    "            bias=False,\n",
    "            norm=True,\n",
    "            activation=False,\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(conv_channels[-1], num_classes, bias=False)\n",
    "\n",
    "        ## Training-related members\n",
    "        self.criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "        self.wer = WordErrorRate()\n",
    "\n",
    "        self.vocabulary = vocabulary\n",
    "\n",
    "        self.apply(self.init_weights)  # Initialize weights\n",
    "\n",
    "    def init_weights(self, m):\n",
    "\n",
    "        if type(m) == nn.Conv2d:\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif type(m) == nn.BatchNorm2d:\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif type(m) == nn.Linear:\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n",
    "\n",
    "        x = self.in_conv(x)\n",
    "\n",
    "        for i in range(5):\n",
    "            x = self.mv2_blocks[i](x)\n",
    "\n",
    "        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n",
    "        x = self.mvit_blocks[0](x)\n",
    "\n",
    "        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n",
    "        x = self.mv2_blocks[5](x)\n",
    "\n",
    "        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n",
    "        x = self.mvit_blocks[1](x)\n",
    "\n",
    "        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n",
    "        x = self.mv2_blocks[6](x)\n",
    "\n",
    "        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n",
    "        x = self.mvit_blocks[2](x)\n",
    "\n",
    "        x = rearrange(x, \"b t c h w -> (b t) c h w\", b=B, t=T)\n",
    "        x = self.final_pw(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.classifier(x)\n",
    "        x = rearrange(x, \"(b t) p -> t b p\", b=B, t=T)  # time major due to CTC loss\n",
    "        return x\n",
    "\n",
    "    def step(self, batch, phase):\n",
    "        x, y = batch\n",
    "\n",
    "        B, T, C, H, W = x.shape\n",
    "        B, N = y.shape\n",
    "\n",
    "        # assume padding is done in the dataloader\n",
    "        input_lengths = torch.full((B,), T, dtype=torch.long)\n",
    "        target_lengths = torch.full((B,), N, dtype=torch.long)\n",
    "\n",
    "        logits = self(x)\n",
    "\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "        print(log_probs.shape, y.shape, input_lengths.shape, target_lengths.shape)\n",
    "        print(input_lengths, target_lengths)\n",
    "        loss = self.criterion(log_probs, y, input_lengths, target_lengths)\n",
    "\n",
    "        self.log(f\"{phase}/loss\", loss, prog_bar=True)\n",
    "\n",
    "        if phase != \"train\":\n",
    "            logits = rearrange(logits, \"t b p -> b t p\")  # batch major for WER\n",
    "            word_error_rate = self.calculate_wer(logits, y)\n",
    "            self.log(f\"{phase}/wer\", word_error_rate, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def calculate_wer(self, logits, y):\n",
    "        pred_strings = self.vocabulary.string_from_logits(logits)\n",
    "        target_strings = self.vocabulary.string_from_ground_truth(y)\n",
    "\n",
    "        return self.wer(pred_strings, target_strings)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), lr=2e-4, amsgrad=True, weight_decay=0.01\n",
    "        )\n",
    "        scheduler = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVocab:\n",
    "\n",
    "    def __init__(self, vocab, debug_mode=False):\n",
    "        self.vocab = vocab\n",
    "        self.debug_mode = debug_mode\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.vocab):\n",
    "            if self.debug_mode:\n",
    "                print(\"Index out of bounds, returning blank token\")\n",
    "            idx = 0\n",
    "        return self.vocab[idx]\n",
    "\n",
    "    def __call__(self, labels):\n",
    "        # assume 0 is the blank token\n",
    "\n",
    "        if type(labels) == torch.Tensor:\n",
    "            return [self.vocab[(i).item()] for i in labels]\n",
    "        else:\n",
    "            return [self.vocab[(i)] for i in labels]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def decode_ctc(self, predicted_tokens, blank=0):\n",
    "\n",
    "        decoded = []\n",
    "        for seq in predicted_tokens:  # Iterate over batch\n",
    "            result = []\n",
    "            prev_token = blank\n",
    "            for token in seq:\n",
    "                if token != prev_token and token != blank:\n",
    "                    result.append(token.item() - 1)\n",
    "                prev_token = token\n",
    "            decoded.append(result)\n",
    "        return decoded\n",
    "\n",
    "    def string_from_logits(self, logits):\n",
    "\n",
    "        pred_tkn = torch.argmax(logits, dim=-1)\n",
    "        decoded = self.decode_ctc(pred_tkn)\n",
    "\n",
    "        strings = []\n",
    "        for b in range(len(decoded)):\n",
    "            string = \" \".join(self(decoded[b]))\n",
    "            strings.append(string)\n",
    "\n",
    "        return strings\n",
    "\n",
    "    def string_from_ground_truth(self, labels):\n",
    "        labels = labels - 1\n",
    "        strings = []\n",
    "        for b in range(len(labels)):\n",
    "            string = \" \".join(self(labels[b]))\n",
    "            strings.append(string)\n",
    "\n",
    "        return strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "datapath = \"data/phoenix-2014.v3/phoenix2014-release/phoenix-2014-multisigner\"\n",
    "path = os.path.join(curr_dir, datapath)\n",
    "\n",
    "dtrain = PhoenixVideoTextDataset(\n",
    "    root=path,\n",
    "    split=\"train\",\n",
    "    p_drop=0.2,\n",
    "    random_drop=True,\n",
    ")\n",
    "\n",
    "\n",
    "dval = PhoenixVideoTextDataset(\n",
    "    root=path,\n",
    "    split=\"dev\",\n",
    "    p_drop=0.2,\n",
    "    random_drop=True,\n",
    ")\n",
    "\n",
    "dtest = PhoenixVideoTextDataset(\n",
    "    root=path,\n",
    "    split=\"test\",\n",
    "    p_drop=0.2,\n",
    "    random_drop=True,\n",
    ")\n",
    "\n",
    "\n",
    "def collate_add_blank(base_collate, batch):\n",
    "\n",
    "    def resize_tensor(x):\n",
    "        transform = torchvision.transforms.Resize((256, 192))\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = rearrange(\n",
    "            transform(rearrange(x, \"b t c h w -> (b t) c h w\")),\n",
    "            \"(b t) c h w -> b t c h w\",\n",
    "            b=B,\n",
    "            t=T,\n",
    "        )\n",
    "        return x\n",
    "\n",
    "    batch = base_collate(batch)  # add one to all labels\n",
    "    batch[\"text\"] = [tens + 1 for tens in batch[\"text\"]]\n",
    "    batch = (\n",
    "        torch.stack(batch[\"video\"], 0),\n",
    "        torch.stack(batch[\"text\"], 0),\n",
    "    )\n",
    "\n",
    "    batch = (resize_tensor(batch[0]), batch[1])\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "# extend the collate function so that targets are tensorized (no padding is done, we use batch size 1 and gradient accumulation with the trainer)\n",
    "collation = lambda x: collate_add_blank(dtrain.collate_fn, x)\n",
    "dl_train = DataLoader(\n",
    "    dtrain,\n",
    "    collate_fn=collation,\n",
    "    shuffle=True,\n",
    "    batch_size=1,\n",
    "    num_workers=3,\n",
    "    pin_memory=True,\n",
    ")\n",
    "dl_val = DataLoader(\n",
    "    dval,\n",
    "    collate_fn=collation,\n",
    "    shuffle=False,\n",
    "    batch_size=1,\n",
    "    num_workers=3,\n",
    "    pin_memory=True,\n",
    ")\n",
    "dl_test = DataLoader(\n",
    "    dtest,\n",
    "    collate_fn=collation,\n",
    "    shuffle=False,\n",
    "    batch_size=1,\n",
    "    num_workers=3,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# for some reason the MEIN token is not in the vocabulary, this speaks volumes about the quality of the dataset\n",
    "dtrain.vocab.table[\"MEIN\"] = 1231\n",
    "# weirdly the dataset maps strings to labels, but we really want to map labels to strings\n",
    "inv_table = {v: k for k, v in dtrain.vocab.table.items()}\n",
    "\n",
    "vocab = CustomVocab(inv_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.utilities.model_summary import summarize\n",
    "from lightning.fabric.utilities import measure_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xxs_mvit(vocab):\n",
    "    return MobileViT(\n",
    "        dims=[64, 80, 96],\n",
    "        conv_channels=[16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 320],\n",
    "        num_classes=len(vocab) + 1,  # +1 for the blank token\n",
    "        vocabulary=vocab,\n",
    "        expand_ratio=2,\n",
    "        patch_size=(2, 2),\n",
    "    )\n",
    "\n",
    "\n",
    "def xs_mvit(vocab):\n",
    "    return MobileViT(\n",
    "        dims=[96, 120, 144],\n",
    "        conv_channels=[16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384],\n",
    "        num_classes=len(vocab) + 1,  # +1 for the blank token\n",
    "        vocabulary=vocab,\n",
    "        expand_ratio=4,\n",
    "        patch_size=(4, 4),\n",
    "    )\n",
    "\n",
    "\n",
    "def s_mvit(vocab):\n",
    "    return MobileViT(\n",
    "        dims=[144, 192, 240],\n",
    "        conv_channels=[16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 640],\n",
    "        num_classes=len(vocab) + 1,  # +1 for the blank token\n",
    "        vocabulary=vocab,\n",
    "        expand_ratio=4,\n",
    "        patch_size=(8, 8),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# UNCOMMENT THIS TO LOG TO WANDB\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "#\n",
    "# user_secrets = UserSecretsClient()\n",
    "# secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n",
    "#\n",
    "# wandb.login(key=secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project=\"mobilevit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 417,  715, 1061, 1115,  561,  921,  898, 1029, 1231,  601]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xxs_mvit(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ICH OSTERN WETTER ZUFRIEDEN MITTAG TEMPERATUR SUED WARM MEIN NICHT']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocabulary.string_from_ground_truth(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mloi-1940849\u001b[0m (\u001b[33macademic_projects\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20241212_000321-4vdswqi4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/academic_projects/lightning_logs/runs/4vdswqi4' target=\"_blank\">earnest-dew-7</a></strong> to <a href='https://wandb.ai/academic_projects/lightning_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/academic_projects/lightning_logs' target=\"_blank\">https://wandb.ai/academic_projects/lightning_logs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/academic_projects/lightning_logs/runs/4vdswqi4' target=\"_blank\">https://wandb.ai/academic_projects/lightning_logs/runs/4vdswqi4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | in_conv     | Conv2DBlock       | 464    | train\n",
      "1 | mv2_blocks  | ModuleList        | 45.7 K | train\n",
      "2 | mvit_blocks | ModuleList        | 1.1 M  | train\n",
      "3 | final_pw    | Conv2DBlock       | 26.2 K | train\n",
      "4 | pool        | AdaptiveAvgPool2d | 0      | train\n",
      "5 | classifier  | Linear            | 394 K  | train\n",
      "6 | criterion   | CTCLoss           | 0      | train\n",
      "7 | wer         | WordErrorRate     | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 M     Total params\n",
      "6.241     Total estimated model params size (MB)\n",
      "292       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]torch.Size([172, 1, 1233]) torch.Size([1, 10]) torch.Size([1]) torch.Size([1])\n",
      "tensor([172]) tensor([10])\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:19<00:19,  0.05it/s]torch.Size([144, 1, 1233]) torch.Size([1, 19]) torch.Size([1]) torch.Size([1])\n",
      "tensor([144]) tensor([19])\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Please call `iter(combined_loader)` first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:197\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip:\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:263\u001b[0m, in \u001b[0;36m_FitLoop.setup_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\u001b[38;5;241m.\u001b[39msetup(combined_loader)\n\u001b[0;32m--> 263\u001b[0m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# creates the iterator inside the fetcher\u001b[39;00m\n\u001b[1;32m    264\u001b[0m max_batches \u001b[38;5;241m=\u001b[39m sized_len(combined_loader)\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fetchers.py:104\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_PrefetchDataFetcher\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__iter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;66;03m# ignore pre-fetching, it's not necessary\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fetchers.py:51\u001b[0m, in \u001b[0;36m_DataFetcher.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_DataFetcher\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombined_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/utilities/combined_loader.py:351\u001b[0m, in \u001b[0;36mCombinedLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflattened, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits)\n\u001b[0;32m--> 351\u001b[0m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m iterator\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/utilities/combined_loader.py:92\u001b[0m, in \u001b[0;36m_MaxSizeCycle.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__iter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumed \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterables)\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/utilities/combined_loader.py:43\u001b[0m, in \u001b[0;36m_ModeIterator.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterators \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/utilities/combined_loader.py:43\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterators \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m iterable \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterables]\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1167\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1166\u001b[0m pin_memory_thread\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1167\u001b[0m \u001b[43mpin_memory_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;66;03m# Similar to workers (see comment above), we only register\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;66;03m# pin_memory_thread once it is started.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-linux-x86_64-gnu/lib/python3.11/threading.py:969\u001b[0m, in \u001b[0;36mThread.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 969\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_started\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-linux-x86_64-gnu/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-linux-x86_64-gnu/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 34\u001b[0m\n\u001b[1;32m     12\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     13\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     14\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     accumulate_grad_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m wandb_logger\u001b[38;5;241m.\u001b[39mwatch(model, log_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 34\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:60\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m signal\u001b[38;5;241m.\u001b[39msignal(signal\u001b[38;5;241m.\u001b[39mSIGINT, signal\u001b[38;5;241m.\u001b[39mSIG_IGN)\n\u001b[1;32m     59\u001b[0m _interrupt(trainer, exception)\n\u001b[0;32m---> 60\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_teardown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m launcher \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1008\u001b[0m, in \u001b[0;36mTrainer._teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;66;03m# loop should never be `None` here but it can because we don't know the trainer stage with `ddp_spawn`\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1008\u001b[0m     \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mteardown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mteardown()\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mteardown()\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:411\u001b[0m, in \u001b[0;36m_FitLoop.teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mteardown\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 411\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mteardown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39mteardown()\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fetchers.py:79\u001b[0m, in \u001b[0;36m_DataFetcher.teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mteardown\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combined_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combined_loader\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fetchers.py:141\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fetchers.py:75\u001b[0m, in \u001b[0;36m_DataFetcher.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# teardown calls `reset()`, and if it happens early, `combined_loader` can still be None\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combined_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;241m=\u001b[39m \u001b[43msized_len\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombined_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/fabric/utilities/data.py:51\u001b[0m, in \u001b[0;36msized_len\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Try to get the length of an object, return ``None`` otherwise.\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# try getting the length\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore [arg-type]\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m):\n\u001b[1;32m     53\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/utilities/combined_loader.py:358\u001b[0m, in \u001b[0;36mCombinedLoader.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the number of batches.\"\"\"\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease call `iter(combined_loader)` first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Please call `iter(combined_loader)` first."
     ]
    }
   ],
   "source": [
    "import lightning.pytorch as lp\n",
    "\n",
    "# setup training and wandb\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "wandb_logger = lp.loggers.WandbLogger()\n",
    "\n",
    "model = xxs_mvit(vocab)\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"cpu\",\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[\n",
    "        lp.callbacks.ModelCheckpoint(\n",
    "            monitor=\"val/loss\",\n",
    "            filename=\"best_model\",\n",
    "            save_top_k=1,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        lp.callbacks.EarlyStopping(\n",
    "            monitor=\"val/ loss\",\n",
    "            patience=3,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "    ],\n",
    "    accumulate_grad_batches=16,\n",
    ")\n",
    "\n",
    "wandb_logger.watch(model, log_graph=False)\n",
    "\n",
    "trainer.fit(model, dl_train, dl_val)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flush cuda cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix_datasets import PhoenixVideoTextDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "\n",
    "curr_dir = os.getcwd()\n",
    "datapath = \"data/phoenix-2014.v3/phoenix2014-release/phoenix-2014-multisigner\"\n",
    "path = os.path.join(curr_dir, datapath)\n",
    "\n",
    "dtrain = PhoenixVideoTextDataset(\n",
    "    # your path to this folder, download it from official website first.\n",
    "    root=path,\n",
    "    split=\"train\",\n",
    "    p_drop=0.5,\n",
    "    random_drop=True,\n",
    ")\n",
    "\n",
    "\n",
    "def collate_add_blank(base_collate, batch):\n",
    "\n",
    "    batch = base_collate(batch)  # add one to all labels\n",
    "    batch[\"text\"] = [tens + 1 for tens in batch[\"text\"]]\n",
    "    batch = (\n",
    "        torch.stack(batch[\"video\"], 0),\n",
    "        torch.stack(batch[\"text\"], 0),\n",
    "    )\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "vocab = dtrain.vocab\n",
    "\n",
    "print(\"Vocab\", vocab)\n",
    "\n",
    "collation = lambda x: collate_add_blank(dtrain.collate_fn, x)\n",
    "\n",
    "dl = DataLoader(dtrain, collate_fn=collation, shuffle=True, batch_size=1)\n",
    "\n",
    "for batch in dl:\n",
    "    video, label = batch\n",
    "\n",
    "    print(video.shape, label.shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_inv = {(v + 1): k for k, v in vocab.table.items()}\n",
    "len(vocab_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = CustomVocab(vocab_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(labels_to_text(label[0], vocab_inv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get logits with 1.0 on each label for the first batch\n",
    "logits = torch.zeros(2, len(label[0]), len(vocab_inv) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_inv[399]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set logits to 1.0 for each label\n",
    "logits[:, torch.arange(len(label[0])), label[0]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV.string_from_logits(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(labels_to_text(torch.argmax(logits, dim=-1)[0], vocab_inv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyctcdecode import build_ctcdecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(CV.vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = build_ctcdecoder(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.decode_beams(logits.squeeze(0).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
