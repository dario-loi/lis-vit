{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1775725,"sourceType":"datasetVersion","datasetId":1049383}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LIS With MobileViTs","metadata":{}},{"cell_type":"markdown","source":"## 1. Our MobileViTs","metadata":{}},{"cell_type":"code","source":"!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz\n!gunzip cc.de.300.vec.gz\n!pip install lightning torchmetrics einops av # gensim\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-12-16T15:16:47.701466Z","iopub.execute_input":"2024-12-16T15:16:47.702070Z","iopub.status.idle":"2024-12-16T15:18:11.721693Z","shell.execute_reply.started":"2024-12-16T15:16:47.702020Z","shell.execute_reply":"2024-12-16T15:18:11.712377Z"},"trusted":true},"outputs":[{"name":"stdout","text":"--2024-12-16 15:16:49--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.219.33, 13.227.219.10, 13.227.219.70, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.219.33|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1278030050 (1.2G) [binary/octet-stream]\nSaving to: 'cc.de.300.vec.gz'\n\ncc.de.300.vec.gz    100%[===================>]   1.19G   189MB/s    in 6.5s    \n\n2024-12-16 15:16:56 (187 MB/s) - 'cc.de.300.vec.gz' saved [1278030050/1278030050]\n\nCollecting lightning\n  Downloading lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.6.0)\nCollecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting av\n  Downloading av-14.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\nRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.2)\nRequirement already satisfied: fsspec<2026.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.9.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.11.9)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\nRequirement already satisfied: torch<4.0,>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.4.0+cpu)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.4)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.12.2)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.4.0)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.9.5)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (70.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=2.1.0->lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.7)\nDownloading lightning-2.4.0-py3-none-any.whl (810 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading av-14.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: einops, av, lightning\nSuccessfully installed av-14.0.1 einops-0.8.0 lightning-2.4.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from gensim.models import KeyedVectors\nimport numpy as np\nimport lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.functional as F\nimport wandb\nfrom einops import rearrange\nimport torchvision\nfrom torch.utils.data import DataLoader\nimport os\nimport gc\nimport numpy as np\nfrom torchmetrics.text import WordErrorRate\nfrom tqdm import tqdm\nimport gzip\nimport pickle\nfrom lightning.pytorch.utilities.model_summary import summarize\nfrom lightning.fabric.utilities import measure_flops\nimport pdb","metadata":{"execution":{"iopub.status.busy":"2024-12-16T16:30:27.052137Z","iopub.execute_input":"2024-12-16T16:30:27.052686Z","iopub.status.idle":"2024-12-16T16:30:27.063469Z","shell.execute_reply.started":"2024-12-16T16:30:27.052641Z","shell.execute_reply":"2024-12-16T16:30:27.061734Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":"\n\n# Percorso del file scaricato\nembedding_file = \"/kaggle/working/cc.de.300.vec\"\n\n# Carica gli embedding FastText pre-addestrati\nfasttext_model = KeyedVectors.load_word2vec_format(embedding_file, binary=False)\nspecial_words = ['<start>', '<end>', '<unk>']\nspecial_embeddings=[np.full(300, 1),np.full(300, 2), np.full(300, 3)]\nfasttext_model.add_vectors(special_words,special_embeddings )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T12:39:12.403749Z","iopub.execute_input":"2024-12-16T12:39:12.404388Z","iopub.status.idle":"2024-12-16T12:44:04.591871Z","shell.execute_reply.started":"2024-12-16T12:39:12.404356Z","shell.execute_reply":"2024-12-16T12:44:04.591168Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!ls /kaggle/working","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optimize this impl:\n# https://github.com/chinhsuanwu/mobilevit-pytorch/blob/master/mobilevit.py\nimport pdb\n\n\nclass Conv2DBlock(nn.Module):\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        groups=1,\n        bias=True,\n        norm=True,\n        activation=True,\n        dropout=0.1,\n    ):\n        \"\"\"__init__ Constructor for Conv2DBlock\n\n        Parameters\n        ----------\n        in_channels : int\n            Number of input channels\n        out_channels : int\n            Number of output channels\n        kernel_size : int\n            Size of the kernel\n        stride : int\n            Stride of the convolutional layer\n        padding : int\n            Padding of the convolutional layer\n        groups : int\n            Number of groups\n        bias : bool\n            Whether to use bias\n        dropout : float\n            Dropout rate\n        \"\"\"\n\n        super(Conv2DBlock, self).__init__()\n\n        self.block = nn.Sequential()\n\n        self.block.add_module(\n            \"conv2d\",\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=padding,\n                groups=groups,\n                bias=bias,\n            ),\n        )\n\n        if norm:\n            self.block.add_module(\"norm\", nn.BatchNorm2d(out_channels))\n\n        if activation:\n            self.block.add_module(\"activation\", nn.SiLU())  # sigmoid(x) * x // SWISH\n\n        if dropout:\n            self.block.add_module(\"dropout\", nn.Dropout(dropout))\n\n        self.block = self.block\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass MobileBlockV2(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride, expand_ratio):\n        \"\"\"__init__ Constructor for MobileBlockV2\n\n\n        Parameters\n        ----------\n        in_channels : int\n            Number of input channels\n        out_channels : int\n            Number of output channels\n        stride : int\n            Stride of the convolutional layer\n        expand_ratio : int\n            Expansion ratio of the block\n        \"\"\"\n\n        super(MobileBlockV2, self).__init__()\n\n        assert stride in [1, 2], \"Stride must be either 1 or 2\"\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.stride = stride\n        self.expand_ratio = expand_ratio\n        self.hidden_dim = int(round(in_channels * expand_ratio))\n\n        self.mbv2 = nn.Sequential()\n        self.uses_inverse_residual = (\n            self.in_channels == self.out_channels and self.stride == 1\n        )\n\n        if self.expand_ratio == 1:\n            self.mbv2.add_module(\n                \"depthwise_3x3\",\n                Conv2DBlock(  # Depthwise Convolution\n                    in_channels=self.hidden_dim,\n                    out_channels=self.hidden_dim,\n                    kernel_size=3,\n                    stride=stride,\n                    padding=1,\n                    groups=self.hidden_dim,\n                    bias=False,\n                    norm=True,\n                    activation=True,\n                ),\n            )\n            self.mbv2.add_module(\n                \"pointwise-linear_1x1\",\n                Conv2DBlock(  # Pointwise-Linear Convolution\n                    in_channels=self.hidden_dim,\n                    out_channels=self.hidden_dim,\n                    kernel_size=1,\n                    stride=1,\n                    padding=0,\n                    groups=self.hidden_dim,\n                    bias=False,\n                    norm=True,\n                    activation=False,\n                ),\n            )\n        else:\n            self.mbv2.add_module(\n                \"pointwise_1x1\",\n                Conv2DBlock(  # Pointwise-Linear Convolution\n                    in_channels=in_channels,\n                    out_channels=self.hidden_dim,\n                    kernel_size=1,\n                    stride=1,\n                    padding=0,\n                    groups=1,\n                    bias=False,\n                    norm=True,\n                    activation=True,\n                ),\n            )\n            self.mbv2.add_module(\n                \"depthwise_3x3\",\n                Conv2DBlock(  # Depthwise Convolution\n                    in_channels=self.hidden_dim,\n                    out_channels=self.hidden_dim,\n                    kernel_size=3,\n                    stride=stride,\n                    padding=1,\n                    groups=self.hidden_dim,\n                    bias=False,\n                    norm=True,\n                    activation=True,\n                ),\n            )\n            self.mbv2.add_module(\n                \"pointwise-linear_1x1\",\n                Conv2DBlock(  # Pointwise-Linear Convolution\n                    in_channels=self.hidden_dim,\n                    out_channels=self.out_channels,\n                    kernel_size=1,\n                    stride=1,\n                    padding=0,\n                    groups=1,\n                    bias=False,\n                    norm=True,\n                    activation=False,\n                ),\n            )\n\n        self.mbv2 = self.mbv2\n\n    def forward(self, x):\n        if self.uses_inverse_residual:\n            return x + self.mbv2(x)\n        else:\n            return self.mbv2(x)\n\n\nclass MobileViTBlock(nn.Module):\n\n    def __init__(\n        self, hidden_dim, depth, channels, kernel_size, patch_size, mlp_dim, dropout=0.1\n    ):\n\n        super(MobileViTBlock, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.depth = depth\n        self.channels = channels\n        self.kernel_size = kernel_size\n        self.patch_size = patch_size\n        self.mlp_dim = mlp_dim\n        self.dropout = dropout\n\n        self.local_conv = nn.Sequential(\n            Conv2DBlock(\n                in_channels=channels,\n                out_channels=channels,\n                kernel_size=kernel_size,\n                stride=1,\n                padding=1,\n                norm=True,\n                activation=True,\n                bias=False,\n            ),\n            Conv2DBlock(\n                in_channels=channels,\n                out_channels=hidden_dim,\n                kernel_size=1,\n                stride=1,\n                padding=0,\n                norm=True,\n                activation=True,\n                bias=False,\n            ),\n        )\n\n        self.global_transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(\n                d_model=hidden_dim,\n                nhead=8,\n                dim_feedforward=mlp_dim,\n                dropout=dropout,\n                batch_first=True,\n                activation=\"gelu\",\n            ),\n            num_layers=depth,\n            norm=nn.LayerNorm(hidden_dim),\n        )\n\n        self.fusion_conv_preres = Conv2DBlock(\n            in_channels=hidden_dim,\n            out_channels=channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False,\n            norm=True,\n            activation=True,\n        )\n\n        self.fusion_conv_postres = Conv2DBlock(\n            in_channels=2 * channels,\n            out_channels=channels,\n            kernel_size=kernel_size,\n            stride=1,\n            padding=1,\n            bias=False,\n            norm=True,\n            activation=True,\n        )\n\n    def forward(self, x):\n\n        B, T, C, H, W = x.shape\n        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n        x_res = x.clone()\n\n        # local_repr\n        x = self.local_conv(x)\n\n        ph, pw = self.patch_size\n\n        # global_repr\n        _, _, h, w = x.shape\n        x = rearrange(  # reshape the image into patches for ViT input\n            x,\n            \"(b t) d (h ph) (w pw) -> (b h w) (t ph pw) d\",\n            ph=ph,\n            pw=pw,\n            b=B,\n            t=T,\n        )\n        x = self.global_transformer(x)\n        x = rearrange(\n            x,\n            \"(b h w) (t ph pw) d -> (b t) d (h ph) (w pw)\",\n            h=h // ph,\n            w=w // pw,\n            ph=ph,\n            pw=pw,\n            b=B,\n            t=T,\n        )\n\n        # fusion\n        x = self.fusion_conv_preres(x)\n        x = torch.cat([x, x_res], dim=1)\n        x = self.fusion_conv_postres(x)\n\n        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n\n        return x\n\n\nclass VideoDecoder(nn.Module):\n    def __init__(self, input_dim,hidden_dim, output_dim,vocab, num_layers=1, dropout=0.1, debug=False):\n        super(VideoDecoder, self).__init__()\n        self.debug=debug\n        \n        # RNN o GRU o Transformer Decoder, scegli tu l'architettura\n        self.decoder = nn.TransformerDecoderLayer(\n            d_model=input_dim,\n            nhead=8,  # Numero di teste di attenzione, personalizzabile\n            batch_first=True,\n            dropout=dropout,\n            activation='gelu'  # Funzione di attivazione, puoi cambiarla in base alle esigenze\n        )\n        self.vocab =  vocab\n        self.embed=nn.Linear(300,320)\n        self.output_layer = nn.Linear(input_dim, output_dim)\n        self.mask = torch.tril(torch.ones(64,64))\n\n    def pad_tensor(self, tensor, max_len, embed_dim=300, pad_value=-1):\n        \"\"\"\n        Aggiunge padding lungo la dimensione della sequenza (seqLen) e crea un padding con una dimensione aggiuntiva.\n        \n        Args:\n            tensor (torch.Tensor): Tensore originale di dimensioni (batch x seqLen x embedDim).\n            max_len (int): Lunghezza massima desiderata per seqLen.\n            embed_dim (int): Dimensione degli embedding (es. 300).\n            pad_value (int): Valore da usare per il padding (default: -1).\n        \n        Returns:\n            torch.Tensor: Tensore di dimensioni (batch x max_len x embedDim) con il padding aggiunto.\n        \"\"\"\n        seq_len = tensor.size(0)\n        # assert current_embed_dim == embed_dim, \"La dimensione degli embedding non corrisponde a embed_dim.\"\n    \n        \n        # Calcola la lunghezza del padding necessario\n        padding_length = max_len - seq_len\n        # Crea il padding con le dimensioni corrette\n        padding = torch.full(\n            (batch_size, padding_length, embed_dim), pad_value, dtype=tensor.dtype, device=tensor.device\n        )\n        # Concatena il tensore originale con il padding lungo la dimensione seqLen (dim=1)\n        \n        \n        return padding\n\n\n        \n    def forward(self, tgt,padding_mask, memory):\n        if self.debug:\n            pdb.set_trace()\n        tgt = self.embed(tgt)\n        # encoder_output avrà dimensione (batch_size, seq_length, input_dim)\n        output = self.decoder(tgt=tgt.to(dtype=torch.float32), memory=memory, tgt_mask=self.mask, \\\n                              tgt_key_padding_mask=padding_mask)\n        output = self.output_layer(output)\n        \n        return output\n\n\nclass MobileViT(pl.LightningModule):\n\n    def __init__(\n        self,\n        dims,\n        conv_channels,\n        num_classes,\n        vocabulary,\n        expand_ratio=4,\n        patch_size=(2, 2),\n        debug= False\n    ):\n        super(MobileViT, self).__init__()\n\n        self.dims = dims\n        self.conv_channels = conv_channels\n        self.num_classes = num_classes\n        self.expand_ratio = expand_ratio\n        self.patch_size = patch_size\n        self.kernel_size = 3\n        self.debug = debug\n\n        L = [2, 4, 3]\n\n        self.in_conv = Conv2DBlock(\n            in_channels=3,\n            out_channels=conv_channels[0],\n            kernel_size=self.kernel_size,\n            stride=2,\n            padding=1,\n            norm=True,\n            activation=True,\n            bias=False,\n        )\n\n        self.mv2_blocks = nn.ModuleList([])\n\n        self.mv2_blocks.append(\n            MobileBlockV2(\n                conv_channels[0], conv_channels[1], 1, expand_ratio=expand_ratio\n            )\n        )\n        self.mv2_blocks.append(\n            MobileBlockV2(\n                conv_channels[1], conv_channels[2], 2, expand_ratio=expand_ratio\n            )\n        )\n        self.mv2_blocks.append(\n            MobileBlockV2(\n                conv_channels[2], conv_channels[3], 1, expand_ratio=expand_ratio\n            )\n        )\n        self.mv2_blocks.append(\n            MobileBlockV2(\n                conv_channels[2], conv_channels[3], 1, expand_ratio=expand_ratio\n            )\n        )\n        self.mv2_blocks.append(\n            MobileBlockV2(\n                conv_channels[3], conv_channels[4], 2, expand_ratio=expand_ratio\n            )\n        )\n        self.mv2_blocks.append(\n            MobileBlockV2(\n                conv_channels[5], conv_channels[6], 2, expand_ratio=expand_ratio\n            )\n        )\n        self.mv2_blocks.append(\n            MobileBlockV2(\n                conv_channels[7], conv_channels[8], 2, expand_ratio=expand_ratio\n            )\n        )\n\n        self.mvit_blocks = nn.ModuleList([])\n\n        self.mvit_blocks.append(\n            MobileViTBlock(\n                dims[0],\n                L[0],\n                conv_channels[5],\n                self.kernel_size,\n                patch_size,\n                int(dims[0] * 2),\n            )\n        )\n        self.mvit_blocks.append(\n            MobileViTBlock(\n                dims[1],\n                L[1],\n                conv_channels[7],\n                self.kernel_size,\n                patch_size,\n                int(dims[1] * 4),\n            )\n        )\n        self.mvit_blocks.append(\n            MobileViTBlock(\n                dims[2],\n                L[2],\n                conv_channels[9],\n                self.kernel_size,\n                patch_size,\n                int(dims[2] * 4),\n            )\n        )\n\n        self.final_pw = Conv2DBlock(\n            in_channels=conv_channels[-2],\n            out_channels=conv_channels[-1],\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            groups=1,\n            bias=False,\n            norm=True,\n            activation=False,\n        )\n\n        self.decoder = VideoDecoder(input_dim=320, hidden_dim=conv_channels[-1], output_dim=len(vocabulary), vocab = vocabulary, debug=debug)\n\n        \n\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.classifier = nn.Linear(conv_channels[-1], num_classes, bias=False)\n\n        ## Training-related members\n        self.criterion = nn.CrossEntropyLoss(ignore_index=-1)\n        self.wer = WordErrorRate()\n\n        self.vocabulary = vocabulary\n\n        self.apply(self.init_weights)  # Initialize weights\n\n    def init_weights(self, m):\n\n        if type(m) == nn.Conv2d:\n            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif type(m) == nn.BatchNorm2d:\n            nn.init.constant_(m.weight, 1)\n            nn.init.constant_(m.bias, 0)\n        elif type(m) == nn.Linear:\n            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def add_padding(self, sequences, max_len):\n        seq_len = sequences.size()[0]\n        if seq_len < max_len:\n            padding = max_len - seq_len\n            # Pad along the first dimension (batch dimension)\n            padded_sequences = torch.nn.functional.pad(\n                sequences, \n                (0, padding),  # Padding lungo la seconda dimensione (seq_len)\n                value=-1\n            )\n        \n        else:\n            padded_sequences = sequences\n        \n    \n        return padded_sequences\n\n    \n\n    def forward(self, x, embeddings, padding_mask):\n        if self.debug:\n            pdb.set_trace()\n        \n    \n\n        B, T, C, H, W = x.shape\n        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n\n        x = self.in_conv(x)\n\n        for i in range(5):\n            x = self.mv2_blocks[i](x)\n\n        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n        x = self.mvit_blocks[0](x)\n\n        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n        x = self.mv2_blocks[5](x)\n\n        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n        x = self.mvit_blocks[1](x)\n\n        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n        x = self.mv2_blocks[6](x)\n\n        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n        x = self.mvit_blocks[2](x)\n\n        x = rearrange(x, \"b t c h w -> (b t) c h w\", b=B, t=T)\n        \n        x = self.final_pw(x)\n        x = self.pool(x)\n        x = x.flatten(1).unsqueeze(0)\n        x = self.decoder(embeddings,padding_mask,x )\n        \n        return x\n\n    def step(self, batch, phase):\n        if self.debug:\n            pdb.set_trace()\n        x, y,embeddings, padding_mask = batch\n\n        B, T, C, H, W = x.shape\n        B, N = y.shape\n\n        # assume padding is done in the dataloader\n        input_lengths = torch.full((B,), T, dtype=torch.long)\n        target_lengths = torch.full((B,), N, dtype=torch.long)\n\n        logits = self(x, embeddings, padding_mask)\n        if self.debug:\n            pdb.set_trace()\n\n        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n        # sum one to y as 0 is the blank token in our representation\n\n        # out_sentence = torch.argmax(log_probs, dim=-1)\n        # Rimodellare logits per adattarli a nn.CrossEntropyLoss\n        logits_reshaped = log_probs.view(-1, log_probs.size(-1))  # [batch_size * seq_len, num_classes]\n        \n        # Rimodellare target per adattarli a nn.CrossEntropyLoss\n        target_reshaped = y.view(-1)  # [batch_size * seq_len]\n        \n        loss = self.criterion(logits_reshaped, target_reshaped)\n\n        self.log(f\"{phase}/loss\", loss, prog_bar=True)\n\n        if phase != \"train\":\n            word_error_rate = self.calculate_wer(log_probs, y)\n            self.log(f\"{phase}/wer\", word_error_rate, prog_bar=True)\n\n        return loss\n\n    def calculate_wer(self, log_probs, y):\n        if self.debug:\n            pdb.set_trace()\n        pred_strings = self.string_from_logits(log_probs)\n        target_strings = self.string_from_ground_truth(y)\n\n        pred_strings = [\" \".join(pred) for pred in pred_strings]\n        target_strings = [\" \".join(target) for target in target_strings]\n        if self.debug:\n            pdb.set_trace()\n\n        return self.wer(pred_strings, target_strings)\n\n    def string_from_logits(self, log_probs):\n        \n        decoded = self.ctc_decode(log_probs)\n\n        return self.string_from_ground_truth(decoded)\n\n    def ctc_decode(self, logits, blank=0):\n        logits = torch.argmax(logits, dim=-1)  # Take the most probable class\n        decoded = []\n        if self.debug:\n            pdb.set_trace()\n        for seq in logits.T:  # Iterate over batch\n            result = []\n            prev_token = blank\n            for token in seq:\n                if token != prev_token and token != blank:\n                    result.append(token.item())\n                prev_token = token\n            decoded.append(result)\n        if self.debug:\n            pdb.set_trace()\n        return decoded\n\n    def string_from_ground_truth(self, y):\n        target_strings = []\n        if self.debug:\n            pdb.set_trace()\n        for target in y:\n            if isinstance(target, torch.Tensor):\n                target = target.tolist()\n            \n            target_strings.append(self.vocabulary.decode_from_ids(target))\n        return target_strings\n\n    def training_step(self, batch, batch_idx):\n        return self.step(batch, \"train\")\n\n    def validation_step(self, batch, batch_idx):\n        return self.step(batch, \"val\")\n\n    def test_step(self, batch, batch_idx):\n        return self.step(batch, \"test\")\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(\n            self.parameters(), lr=2e-4, amsgrad=True, weight_decay=1e-4\n        )\n        scheduler = {\n            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n                optimizer, T_0=10, T_mult=2, eta_min=1e-6\n            ),\n            \"interval\": \"step\",\n            \"frequency\": 32,\n        }\n\n        return [optimizer], [scheduler]","metadata":{"execution":{"iopub.status.busy":"2024-12-16T18:42:06.906360Z","iopub.execute_input":"2024-12-16T18:42:06.907103Z","iopub.status.idle":"2024-12-16T18:42:06.989572Z","shell.execute_reply.started":"2024-12-16T18:42:06.907052Z","shell.execute_reply":"2024-12-16T18:42:06.988054Z"},"trusted":true},"outputs":[],"execution_count":137},{"cell_type":"markdown","source":"# 2. Our Datamodules","metadata":{}},{"cell_type":"code","source":"from dataclasses import dataclass\nimport random\n\n\n@dataclass\nclass PhoenixFiles:\n    train: str\n    dev: str\n    test: str\n\n\n@dataclass\nclass PhoenixDataHyperparameters:\n    num_workers: int\n    batch_size: int\n    sample_random: bool\n    sample_uniform: bool\n    sample_percentage: float\n    transforms: torchvision.transforms.Compose\n\n\nclass PhoenixVocabulary:\n\n    def __init__(self, filenames: PhoenixFiles):\n        self.filenames = filenames\n        \n        embedding_file = \"/kaggle/working/cc.de.300.vec\"\n\n        # Carica gli embedding FastText pre-addestrati\n        print(\"loading embeddings\")\n        self.fasttext_model = KeyedVectors.load_word2vec_format(embedding_file, binary=False)\n        special_words = ['<start>', '<end>', '<unk>']\n        special_embeddings=[np.full(300, 1),np.full(300, 2), np.full(300, 3)]\n        self.fasttext_model.add_vectors(special_words,special_embeddings )\n        self.vocab = self.build_vocab()\n        self.vocab_inversed = {v[0]: k for k, v in self.vocab.items()}\n\n    def get_embedding(self, word):\n        # Crea un embedding speciale per OOV utilizzando l'embedding di <unk>\n        \n        return self.fasttext_model[word] if word in self.fasttext_model else torch.tensor(self.fasttext_model['<unk>'])\n    \n\n    def build_vocab(self):\n        vocab = {}\n\n        files = [self.filenames.train, self.filenames.dev, self.filenames.test]\n\n        for file in files:\n            with gzip.open(file, \"rb\") as f:\n                annotations = pickle.load(f)\n\n            for ann in tqdm(\n                random.sample(annotations, len(annotations)),\n                desc=f\"Extracting tokens from {os.path.basename(file)}\",\n            ):\n\n                # random.sample shuffles the strings, this improves token distribution (hopefully improves training)\n                for word in random.sample(\n                    ann[\"gloss\"].split(), len(ann[\"gloss\"].split())\n                ):\n                    if word not in vocab:\n                        vocab[word] = (len(vocab), self.get_embedding(word))\n\n        vocab[\"<start>\"] = (len(vocab),self.get_embedding(\"<start>\"))\n        vocab[\"<end>\"] = (len(vocab),self.get_embedding(\"<end>\"))\n\n        return vocab\n\n    def __len__(self):\n        return len(self.vocab)\n\n    def __getitem__(self, idx):\n        return self.vocab[idx]\n\n    def encode_as_ids(self, sentence):\n        return [self.vocab[word] for word in sentence]\n\n    def decode_from_ids(self, ids):\n        return [self.vocab_inversed[tkn] if tkn != -1 for tkn in ids]\n\n\nclass Phoenix14TDataset(torch.utils.data.Dataset):\n\n    def __init__(self, path: str, vocabulary: PhoenixVocabulary, hyperparameters):\n        self.metadata_path = path\n        self.base_dir = os.path.dirname(path)\n        self.data = None\n        self.vocab: PhoenixVocabulary = vocabulary\n        self.data_hparams = hyperparameters\n        self.transform = hyperparameters.transforms\n\n        assert (\n            self.data_hparams.sample_random != self.data_hparams.sample_uniform\n        ), \"Both random and uniform sampling cannot be enabled at the same time\"\n\n    def setup(self, stage):\n        with gzip.open(self.metadata_path, \"rb\") as f:\n            self.data = pickle.load(f)\n\n\n        self.signers = []   \n        self.video_names = [] \n        self.annotations = []  \n        self.text = []  \n        self.targets = []\n        self.embeddings = []\n        for d in self.data:\n            self.signers.append(d[\"signer\"])\n            self.video_names.append(d[\"name\"])\n            self.annotations.append(d[\"gloss\"])\n            self.text.append(d[\"text\"])\n        max_len = 0  \n        for ann in tqdm(\n                self.annotations,\n                desc=(\n                    f\"Encoding annotations for {stage}\"\n                    if stage\n                    else \"Encoding annotations\"\n                ),\n            ):\n            tmp = self.vocab.encode_as_ids([\"<start>\"]+[token for token in ann.split()]+[\"<end>\"])\n            max_len = max(max_len, len(tmp))\n            self.targets.append([v[0] for v in tmp])\n            self.embeddings.append([v[1] for v in tmp])\n        print(max_len)\n        \n\n\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n\n        video_path = os.path.join(\n            self.base_dir, \"videos_phoenix\", \"videos\", self.video_names[idx] + \".mp4\"\n        )\n\n        video = torchvision.io.read_video(\n            video_path,\n            pts_unit=\"sec\",\n            output_format=\"TCHW\",\n        )[0]\n        \n\n        if self.data_hparams.sample_random:\n            indices = sorted(random.sample(\n                range(len(video)), len(video) * self.data_hparams.sample_percentage\n            ))\n        elif self.data_hparams.sample_uniform:\n            indices = np.linspace(\n                0,\n                len(video) - 1,\n                int(len(video) * self.data_hparams.sample_percentage),\n            ).tolist()\n        else:\n            indices = list(range(len(video)))\n            \n\n        T = len(indices)\n        video_tensor = video.unsqueeze(0)\n        video_tensor = rearrange(\n                self.transform(\n                    rearrange(video_tensor[:, indices], \"b t c h w -> (b t) c h w\")\n                ), \"(b t) c h w -> b t c h w\", c=3, t=T\n        ).squeeze(0)\n       # pdb.set_trace()\n        # print(idx)\n        tgt = torch.tensor(self.targets[idx], dtype=torch.long)\n        tgt_embedding = torch.tensor(self.embeddings[idx], dtype=torch.float)\n        #print(type(tgt))\n\n        return video_tensor, torch.cat([tgt[1:],torch.full((max(0, 64 - tgt.size(0) + 1),), -1)]), \\\n            torch.cat([tgt_embedding[:-1,:],torch.full((max(0, 64 - tgt.size(0) + 1),300), 0)]), \\\n            torch.cat([torch.ones(tgt.size(0)-1), torch.zeros(max(0, 64 - tgt.size(0) + 1))])\n\n    def get_metadata(self, idx):\n        return {\n            \"signer\": self.signers[idx],\n            \"video_name\": self.video_names[idx],\n            \"annotation\": self.annotations[idx],\n            \"text\": self.text[idx],\n        }\n\n\nclass Phoenix14TDatamodule(pl.LightningDataModule):\n\n    def __init__(\n        self,\n        files: PhoenixFiles,\n        vocabulary: PhoenixVocabulary,\n        num_workers: int,\n        data_hyperparameters: PhoenixDataHyperparameters,\n    ):\n        super(Phoenix14TDatamodule, self).__init__()\n        self.metadata_paths = files\n        self.vocabulary = vocabulary\n        self.workers = num_workers\n        self.train = None\n        self.dev = None\n        self.test = None\n        self.data_hparams = data_hyperparameters\n\n        assert (\n            self.data_hparams.sample_random != self.data_hparams.sample_uniform\n        ), \"Both random and uniform sampling cannot be enabled at the same time\"\n\n    def setup(self, stage):\n        self.train = Phoenix14TDataset(\n            self.metadata_paths.train, self.vocabulary, self.data_hparams\n        )\n        self.train.setup(\"train\")\n\n        self.dev = Phoenix14TDataset(\n            self.metadata_paths.dev, self.vocabulary, self.data_hparams\n        )\n        self.dev.setup(\"dev\")\n\n        self.test = Phoenix14TDataset(\n            self.metadata_paths.test, self.vocabulary, self.data_hparams\n        )\n        self.test.setup(\"test\")\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train,\n            shuffle=True,\n            batch_size=1,\n            pin_memory=True,\n            num_workers=self.workers,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.dev,\n            shuffle=False,\n            batch_size=1,\n            pin_memory=True,\n            num_workers=self.workers,\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            self.test,\n            shuffle=False,\n            batch_size=1,\n            pin_memory=True,\n            num_workers=self.workers,\n        )\n        \n\n\ndef animate_torch_tensor(video):\n    \n    # Adapted from https://stackoverflow.com/a/57275596\n    \n    %matplotlib inline\n    from matplotlib import pyplot as plt\n    from matplotlib import animation\n    from IPython.display import HTML\n\n    \n    video_np = video.numpy().transpose(0, 2, 3, 1)\n\n    fig = plt.figure()\n    im = plt.imshow(video_np[0,:,:,:])\n\n    plt.close() # this is required to not display the generated image\n\n    def init():\n        im.set_data(video_np[0,:,:,:])\n\n    def animate(i):\n        im.set_data(video_np[i,:,:,:])\n        return im\n\n    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video_np.shape[0],\n                                interval=1000//6)\n    return HTML(anim.to_html5_video())","metadata":{"execution":{"iopub.status.busy":"2024-12-16T18:44:45.051269Z","iopub.execute_input":"2024-12-16T18:44:45.051994Z","iopub.status.idle":"2024-12-16T18:44:45.122425Z","shell.execute_reply.started":"2024-12-16T18:44:45.051933Z","shell.execute_reply":"2024-12-16T18:44:45.120079Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[140], line 80\u001b[0;36m\u001b[0m\n\u001b[0;31m    return [self.vocab_inversed[tkn] if tkn != -1 for tkn in ids]\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected 'else' after 'if' expression\n"],"ename":"SyntaxError","evalue":"expected 'else' after 'if' expression (2431605598.py, line 80)","output_type":"error"}],"execution_count":140},{"cell_type":"code","source":"def xxs_mvit(vocab):\n    return MobileViT(\n        dims=[64, 80, 96],\n        conv_channels=[16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 320],\n        num_classes=len(vocab),\n        vocabulary=vocab,\n        expand_ratio=2,\n        patch_size=(2, 2),\n        debug=True\n    )\n\n\ndef xs_mvit(vocab):\n    return MobileViT(\n        dims=[96, 120, 144],\n        conv_channels=[16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384],\n        num_classes=len(vocab),\n        vocabulary=vocab,\n        expand_ratio=4,\n        patch_size=(4, 4),\n    )\n\n\ndef s_mvit(vocab):\n    return MobileViT(\n        dims=[144, 192, 240],\n        conv_channels=[16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 640],\n        num_classes=len(vocab),\n        vocabulary=vocab,\n        expand_ratio=4,\n        patch_size=(8, 8),\n    )","metadata":{"execution":{"iopub.status.busy":"2024-12-16T16:32:11.094171Z","iopub.execute_input":"2024-12-16T16:32:11.096759Z","iopub.status.idle":"2024-12-16T16:32:11.111656Z","shell.execute_reply.started":"2024-12-16T16:32:11.096367Z","shell.execute_reply":"2024-12-16T16:32:11.108035Z"},"trusted":true},"outputs":[],"execution_count":35},{"cell_type":"code","source":"import wandb\n\n# UNCOMMENT THIS TO LOG TO WANDB\n# from kaggle_secrets import UserSecretsClient\n\n# user_secrets = UserSecretsClient()\n# secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n\n\nwandb.login(key=\"aaf831dabc88d936d4e6b439b798bb4cb42814ea\")","metadata":{"execution":{"iopub.status.busy":"2024-12-16T12:44:53.884420Z","iopub.execute_input":"2024-12-16T12:44:53.885064Z","iopub.status.idle":"2024-12-16T12:44:55.945840Z","shell.execute_reply.started":"2024-12-16T12:44:53.885026Z","shell.execute_reply":"2024-12-16T12:44:55.945084Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"files = PhoenixFiles(\n    train=\"/kaggle/input/phoenix14t.pami0.train.annotations_only.gzip\",\n    dev=\"/kaggle/input/phoenix14t.pami0.dev.annotations_only.gzip\",\n    test=\"/kaggle/input/phoenix14t.pami0.test.annotations_only.gzip\",\n)\n\ntransforms = torchvision.transforms.Compose(\n    [\n        torchvision.transforms.Resize(\n            (192, 256), interpolation=torchvision.transforms.InterpolationMode.BICUBIC\n        ),  # from uint8 to float32\n        lambda x: x / 255.0,\n    ]\n)\n\ndata_hparams = PhoenixDataHyperparameters(\n    num_workers=3,\n    batch_size=1,\n    sample_random=False,\n    sample_uniform=True,\n    sample_percentage=1 / 3,\n    transforms=transforms,\n)\n\nvocab = PhoenixVocabulary(files)\n\ndm = Phoenix14TDatamodule(files, vocab, 1, data_hparams)","metadata":{"execution":{"iopub.status.busy":"2024-12-16T16:32:34.410629Z","iopub.execute_input":"2024-12-16T16:32:34.411239Z","iopub.status.idle":"2024-12-16T16:40:51.108384Z","shell.execute_reply.started":"2024-12-16T16:32:34.411183Z","shell.execute_reply":"2024-12-16T16:40:51.106218Z"},"trusted":true},"outputs":[{"name":"stdout","text":"loading embeddings\n","output_type":"stream"},{"name":"stderr","text":"\nExtracting tokens from phoenix14t.pami0.train.annotations_only.gzip: 100%|██████████| 7096/7096 [00:00<00:00, 80952.80it/s]\n\nExtracting tokens from phoenix14t.pami0.dev.annotations_only.gzip: 100%|██████████| 519/519 [00:00<00:00, 62664.63it/s]\n\nExtracting tokens from phoenix14t.pami0.test.annotations_only.gzip: 100%|██████████| 642/642 [00:00<00:00, 69279.18it/s]\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"dm = Phoenix14TDatamodule(files, vocab, 3, data_hparams)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T18:08:43.610272Z","iopub.execute_input":"2024-12-16T18:08:43.610950Z","iopub.status.idle":"2024-12-16T18:08:43.618856Z","shell.execute_reply.started":"2024-12-16T18:08:43.610848Z","shell.execute_reply":"2024-12-16T18:08:43.617092Z"}},"outputs":[],"execution_count":117},{"cell_type":"code","source":"dm.setup(\"fit\")","metadata":{"execution":{"iopub.status.busy":"2024-12-16T18:08:46.701068Z","iopub.execute_input":"2024-12-16T18:08:46.701737Z","iopub.status.idle":"2024-12-16T18:08:46.884816Z","shell.execute_reply.started":"2024-12-16T18:08:46.701688Z","shell.execute_reply":"2024-12-16T18:08:46.883263Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\nEncoding annotations for train: 100%|██████████| 7096/7096 [00:00<00:00, 106649.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"32\n","output_type":"stream"},{"name":"stderr","text":"\nEncoding annotations for dev: 100%|██████████| 519/519 [00:00<00:00, 98732.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"20\n","output_type":"stream"},{"name":"stderr","text":"\nEncoding annotations for test: 100%|██████████| 642/642 [00:00<00:00, 104019.13it/s]","output_type":"stream"},{"name":"stdout","text":"20\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":118},{"cell_type":"code","source":"dm.train.targets[0][0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T16:43:52.931208Z","iopub.execute_input":"2024-12-16T16:43:52.932415Z","iopub.status.idle":"2024-12-16T16:43:52.954396Z","shell.execute_reply.started":"2024-12-16T16:43:52.932340Z","shell.execute_reply":"2024-12-16T16:43:52.951060Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"(1115,\n array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32))"},"metadata":{}}],"execution_count":40},{"cell_type":"markdown","source":"## Check out a video","metadata":{}},{"cell_type":"code","source":"animate_torch_tensor(dm.train[0][0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# flush cuda cache\ntorch.cuda.empty_cache()\n\nimport lightning.pytorch as lp\n\n# wandb.init(project=\"mobilevit\")\n\n# setup training and wandb\n\nBATCH_SIZE = 1  # we don't do padding, so batch size must be 1\n\n# wandb_logger = lp.loggers.WandbLogger()\n\nmodel = xxs_mvit(vocab)\n\n\ntrainer = pl.Trainer(\n    max_epochs=100,\n    accelerator=\"cpu\",#cuda\n    #logger=wandb_logger,\n    callbacks=[\n        lp.callbacks.ModelCheckpoint(\n            monitor=\"val/loss\",\n            filename=\"best_model\",\n            save_top_k=1,\n            mode=\"min\",\n        ),\n        lp.callbacks.EarlyStopping(\n            monitor=\"val/loss\",\n            patience=3,\n            mode=\"min\",\n        ),\n        lp.callbacks.LearningRateMonitor(logging_interval=\"step\"),\n    ],\n    limit_train_batches=0.25,\n    accumulate_grad_batches=32,\n)\n\n# wandb_logger.watch(model, log_graph=False)\n\ntrainer.fit(model, datamodule=dm)\n\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-12-16T18:42:43.255294Z","iopub.execute_input":"2024-12-16T18:42:43.255943Z","iopub.status.idle":"2024-12-16T18:44:01.817009Z","shell.execute_reply.started":"2024-12-16T18:42:43.255873Z","shell.execute_reply":"2024-12-16T18:44:01.814669Z"},"trusted":true},"outputs":[{"name":"stderr","text":"INFO: GPU available: False, used: False\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\n\nEncoding annotations for train: 100%|██████████| 7096/7096 [00:00<00:00, 114867.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"32\n","output_type":"stream"},{"name":"stderr","text":"\nEncoding annotations for dev: 100%|██████████| 519/519 [00:00<00:00, 71120.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"20\n","output_type":"stream"},{"name":"stderr","text":"\nEncoding annotations for test: 100%|██████████| 642/642 [00:00<00:00, 73588.30it/s]\nINFO: \n  | Name        | Type              | Params | Mode \n----------------------------------------------------------\n0 | in_conv     | Conv2DBlock       | 464    | train\n1 | mv2_blocks  | ModuleList        | 45.7 K | train\n2 | mvit_blocks | ModuleList        | 1.1 M  | train\n3 | final_pw    | Conv2DBlock       | 26.2 K | train\n4 | decoder     | VideoDecoder      | 2.6 M  | train\n5 | pool        | AdaptiveAvgPool2d | 0      | train\n6 | classifier  | Linear            | 357 K  | train\n7 | criterion   | CrossEntropyLoss  | 0      | train\n8 | wer         | WordErrorRate     | 0      | train\n----------------------------------------------------------\n4.1 M     Trainable params\n0         Non-trainable params\n4.1 M     Total params\n16.459    Total estimated model params size (MB)\n344       Modules in train mode\n0         Modules in eval mode\n","output_type":"stream"},{"name":"stdout","text":"20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72deade57d92486f9626964ca090df88"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_24/4138576939.py:171: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  tgt_embedding = torch.tensor(self.embeddings[idx], dtype=torch.float)\n","output_type":"stream"},{"name":"stdout","text":"> \u001b[0;32m/tmp/ipykernel_24/3572483869.py\u001b[0m(574)\u001b[0;36mstep\u001b[0;34m()\u001b[0m\n\u001b[0;32m    572 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 574 \u001b[0;31m        \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576 \u001b[0;31m        \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_24/4138576939.py:171: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  tgt_embedding = torch.tensor(self.embeddings[idx], dtype=torch.float)\n/tmp/ipykernel_24/4138576939.py:171: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  tgt_embedding = torch.tensor(self.embeddings[idx], dtype=torch.float)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  c\n"},{"name":"stdout","text":"> \u001b[0;32m/tmp/ipykernel_24/3572483869.py\u001b[0m(539)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n\u001b[0;32m    537 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 539 \u001b[0;31m        \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    540 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b t c h w -> (b t) c h w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    541 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  c\n"},{"name":"stdout","text":"> \u001b[0;32m/tmp/ipykernel_24/3572483869.py\u001b[0m(358)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n\u001b[0;32m    356 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 358 \u001b[0;31m        \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359 \u001b[0;31m        \u001b[0;31m# encoder_output avrà dimensione (batch_size, seq_length, input_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360 \u001b[0;31m        output = self.decoder(tgt=tgt.to(dtype=torch.float32), memory=memory, tgt_mask=self.mask, \\\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  c\n"},{"name":"stdout","text":"> \u001b[0;32m/tmp/ipykernel_24/3572483869.py\u001b[0m(587)\u001b[0;36mstep\u001b[0;34m()\u001b[0m\n\u001b[0;32m    585 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 587 \u001b[0;31m        \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588 \u001b[0;31m        \u001b[0;31m# sum one to y as 0 is the blank token in our representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  c\n"},{"name":"stdout","text":"> \u001b[0;32m/tmp/ipykernel_24/3572483869.py\u001b[0m(610)\u001b[0;36mcalculate_wer\u001b[0;34m()\u001b[0m\n\u001b[0;32m    608 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 610 \u001b[0;31m        \u001b[0mpred_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_from_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611 \u001b[0;31m        \u001b[0mtarget_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_from_ground_truth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  c\n"},{"name":"stdout","text":"> \u001b[0;32m/tmp/ipykernel_24/3572483869.py\u001b[0m(631)\u001b[0;36mctc_decode\u001b[0;34m()\u001b[0m\n\u001b[0;32m    629 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    630 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 631 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Iterate over batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632 \u001b[0;31m            \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633 \u001b[0;31m            \u001b[0mprev_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  c\n"},{"name":"stdout","text":"> \u001b[0;32m/tmp/ipykernel_24/3572483869.py\u001b[0m(641)\u001b[0;36mctc_decode\u001b[0;34m()\u001b[0m\n\u001b[0;32m    639 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 641 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    643 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mstring_from_ground_truth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  c\n"},{"name":"stdout","text":"> \u001b[0;32m/tmp/ipykernel_24/3572483869.py\u001b[0m(647)\u001b[0;36mstring_from_ground_truth\u001b[0;34m()\u001b[0m\n\u001b[0;32m    645 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 647 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649 \u001b[0;31m                \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  n\n"},{"name":"stdout","text":"> \u001b[0;32m/tmp/ipykernel_24/3572483869.py\u001b[0m(648)\u001b[0;36mstring_from_ground_truth\u001b[0;34m()\u001b[0m\n\u001b[0;32m    646 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 648 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649 \u001b[0;31m                \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  target\n"},{"name":"stdout","text":"[653]\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  c\n"},{"name":"stdout","text":"> \u001b[0;32m/tmp/ipykernel_24/3572483869.py\u001b[0m(647)\u001b[0;36mstring_from_ground_truth\u001b[0;34m()\u001b[0m\n\u001b[0;32m    645 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 647 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649 \u001b[0;31m                \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  n\n"},{"name":"stdout","text":"> \u001b[0;32m/tmp/ipykernel_24/3572483869.py\u001b[0m(648)\u001b[0;36mstring_from_ground_truth\u001b[0;34m()\u001b[0m\n\u001b[0;32m    646 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 648 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649 \u001b[0;31m                \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  target\n"},{"name":"stdout","text":"tensor([ 180,   21,   45, 1116,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n          -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n          -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n          -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n          -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n          -1,   -1,   -1,   -1])\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  n\n"},{"name":"stdout","text":"> \u001b[0;32m/tmp/ipykernel_24/3572483869.py\u001b[0m(649)\u001b[0;36mstring_from_ground_truth\u001b[0;34m()\u001b[0m\n\u001b[0;32m    647 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 649 \u001b[0;31m                \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651 \u001b[0;31m                \u001b[0mtarget_strings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_from_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  target_strings\n"},{"name":"stdout","text":"[]\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  q\n"}],"execution_count":139},{"cell_type":"code","source":"xxs_kwargs = {\n    \"dims\": [64, 80, 96],\n    \"conv_channels\": [16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 320],\n    \"num_classes\": len(vocab),\n    \"vocabulary\": vocab,\n    \"expand_ratio\": 2,\n    \"patch_size\": (2, 2),\n}\nget_vector\nmodel = xxs_mvit(vocab)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n    model = model.cuda()\n    x, y = dm.train[53]\n    x = x.cuda().unsqueeze(0)\n\n    logits = model(x)\n    decoded = model.ctc_decode(logits)\n    print(torch.tensor(decoded).min())\n\n    print(\"Predicted:\", vocab.decode_from_ids(decoded[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(\" \".join(vocab.decode_from_ids(y.tolist())))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}