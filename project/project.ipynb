{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIS With MobileViTs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Our MobileViTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import wandb\n",
    "from einops import rearrange\n",
    "import torchvision\n",
    "from phoenix_datasets import PhoenixVideoTextDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "from torchmetrics.text import WordErrorRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize this impl:\n",
    "# https://github.com/chinhsuanwu/mobilevit-pytorch/blob/master/mobilevit.py\n",
    "\n",
    "\n",
    "class Conv2DBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        groups=1,\n",
    "        bias=True,\n",
    "        norm=True,\n",
    "        activation=True,\n",
    "    ):\n",
    "        \"\"\"__init__ Constructor for Conv2DBlock\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            Number of input channels\n",
    "        out_channels : int\n",
    "            Number of output channels\n",
    "        kernel_size : int\n",
    "            Size of the kernel\n",
    "        stride : int\n",
    "            Stride of the convolutional layer\n",
    "        padding : int\n",
    "            Padding of the convolutional layer\n",
    "        groups : int\n",
    "            Number of groups\n",
    "        bias : bool\n",
    "            Whether to use bias\n",
    "        \"\"\"\n",
    "\n",
    "        super(Conv2DBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential()\n",
    "\n",
    "        self.block.add_module(\n",
    "            \"conv2d\",\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                groups=groups,\n",
    "                bias=bias,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if norm:\n",
    "            self.block.add_module(\"norm\", nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        if activation:\n",
    "            self.block.add_module(\"activation\", nn.SiLU())  # sigmoid(x) * x // SWISH\n",
    "\n",
    "        self.block = self.block\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class MobileBlockV2(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride, expand_ratio):\n",
    "        \"\"\"__init__ Constructor for MobileBlockV2\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            Number of input channels\n",
    "        out_channels : int\n",
    "            Number of output channels\n",
    "        stride : int\n",
    "            Stride of the convolutional layer\n",
    "        expand_ratio : int\n",
    "            Expansion ratio of the block\n",
    "        \"\"\"\n",
    "\n",
    "        super(MobileBlockV2, self).__init__()\n",
    "\n",
    "        assert stride in [1, 2], \"Stride must be either 1 or 2\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.hidden_dim = int(round(in_channels * expand_ratio))\n",
    "\n",
    "        self.mbv2 = nn.Sequential()\n",
    "        self.uses_inverse_residual = (\n",
    "            self.in_channels == self.out_channels and self.stride == 1\n",
    "        )\n",
    "\n",
    "        if self.expand_ratio == 1:\n",
    "            self.mbv2.add_module(\n",
    "                \"depthwise_3x3\",\n",
    "                Conv2DBlock(  # Depthwise Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=3,\n",
    "                    stride=stride,\n",
    "                    padding=1,\n",
    "                    groups=self.hidden_dim,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                ),\n",
    "            )\n",
    "            self.mbv2.add_module(\n",
    "                \"pointwise-linear_1x1\",\n",
    "                Conv2DBlock(  # Pointwise-Linear Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    groups=self.hidden_dim,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=False,\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            self.mbv2.add_module(\n",
    "                \"pointwise_1x1\",\n",
    "                Conv2DBlock(  # Pointwise-Linear Convolution\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    groups=1,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                ),\n",
    "            )\n",
    "            self.mbv2.add_module(\n",
    "                \"depthwise_3x3\",\n",
    "                Conv2DBlock(  # Depthwise Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=3,\n",
    "                    stride=stride,\n",
    "                    padding=1,\n",
    "                    groups=self.hidden_dim,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                ),\n",
    "            )\n",
    "            self.mbv2.add_module(\n",
    "                \"pointwise-linear_1x1\",\n",
    "                Conv2DBlock(  # Pointwise-Linear Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    groups=1,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=False,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        self.mbv2 = self.mbv2\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.uses_inverse_residual:\n",
    "            return x + self.mbv2(x)\n",
    "        else:\n",
    "            return self.mbv2(x)\n",
    "\n",
    "\n",
    "class MobileViTBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, hidden_dim, depth, channels, kernel_size, patch_size, mlp_dim, dropout=0.0\n",
    "    ):\n",
    "\n",
    "        super(MobileViTBlock, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.depth = depth\n",
    "        self.channels = channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.patch_size = patch_size\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.local_conv = nn.Sequential(\n",
    "            Conv2DBlock(\n",
    "                in_channels=channels,\n",
    "                out_channels=channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                norm=True,\n",
    "                activation=True,\n",
    "                bias=False,\n",
    "            ),\n",
    "            Conv2DBlock(\n",
    "                in_channels=channels,\n",
    "                out_channels=hidden_dim,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                norm=True,\n",
    "                activation=True,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        self.global_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_dim,\n",
    "                nhead=8,\n",
    "                dim_feedforward=mlp_dim,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "                activation=\"gelu\",\n",
    "            ),\n",
    "            num_layers=depth,\n",
    "            norm=nn.LayerNorm(hidden_dim),\n",
    "        )\n",
    "\n",
    "        self.fusion_conv_preres = Conv2DBlock(\n",
    "            in_channels=hidden_dim,\n",
    "            out_channels=channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=False,\n",
    "            norm=True,\n",
    "            activation=True,\n",
    "        )\n",
    "\n",
    "        self.fusion_conv_postres = Conv2DBlock(\n",
    "            in_channels=2 * channels,\n",
    "            out_channels=channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "            norm=True,\n",
    "            activation=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n",
    "        x_res = x.clone()\n",
    "\n",
    "        # local_repr\n",
    "        x = self.local_conv(x)\n",
    "\n",
    "        ph, pw = self.patch_size\n",
    "\n",
    "        # global_repr\n",
    "        _, _, h, w = x.shape\n",
    "        x = rearrange(  # reshape the image into patches for ViT input\n",
    "            x,\n",
    "            \"(b t) d (h ph) (w pw) -> (b h w) (t ph pw) d\",\n",
    "            ph=ph,\n",
    "            pw=pw,\n",
    "            b=B,\n",
    "            t=T,\n",
    "        )\n",
    "        x = self.global_transformer(x)\n",
    "        x = rearrange(\n",
    "            x,\n",
    "            \"(b h w) (t ph pw) d -> (b t) d (h ph) (w pw)\",\n",
    "            h=h // ph,\n",
    "            w=w // pw,\n",
    "            ph=ph,\n",
    "            pw=pw,\n",
    "            b=B,\n",
    "            t=T,\n",
    "        )\n",
    "\n",
    "        # fusion\n",
    "        x = self.fusion_conv_preres(x)\n",
    "        x = torch.cat([x, x_res], dim=1)\n",
    "        x = self.fusion_conv_postres(x)\n",
    "\n",
    "        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MobileViT(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dims,\n",
    "        conv_channels,\n",
    "        num_classes,\n",
    "        vocabulary,\n",
    "        expand_ratio=4,\n",
    "        patch_size=(2, 2),\n",
    "    ):\n",
    "        super(MobileViT, self).__init__()\n",
    "\n",
    "        self.dims = dims\n",
    "        self.conv_channels = conv_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.patch_size = patch_size\n",
    "        self.kernel_size = 3\n",
    "\n",
    "        L = [2, 4, 3]\n",
    "\n",
    "        self.in_conv = Conv2DBlock(\n",
    "            in_channels=3,\n",
    "            out_channels=conv_channels[0],\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            norm=True,\n",
    "            activation=True,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.mv2_blocks = nn.ModuleList([])\n",
    "\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[0], conv_channels[1], 1, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[1], conv_channels[2], 2, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[2], conv_channels[3], 1, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[2], conv_channels[3], 1, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[3], conv_channels[4], 2, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[5], conv_channels[6], 2, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[7], conv_channels[8], 2, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.mvit_blocks = nn.ModuleList([])\n",
    "\n",
    "        self.mvit_blocks.append(\n",
    "            MobileViTBlock(\n",
    "                dims[0],\n",
    "                L[0],\n",
    "                conv_channels[5],\n",
    "                self.kernel_size,\n",
    "                patch_size,\n",
    "                int(dims[0] * 2),\n",
    "            )\n",
    "        )\n",
    "        self.mvit_blocks.append(\n",
    "            MobileViTBlock(\n",
    "                dims[1],\n",
    "                L[1],\n",
    "                conv_channels[7],\n",
    "                self.kernel_size,\n",
    "                patch_size,\n",
    "                int(dims[1] * 4),\n",
    "            )\n",
    "        )\n",
    "        self.mvit_blocks.append(\n",
    "            MobileViTBlock(\n",
    "                dims[2],\n",
    "                L[2],\n",
    "                conv_channels[9],\n",
    "                self.kernel_size,\n",
    "                patch_size,\n",
    "                int(dims[2] * 4),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.final_pw = Conv2DBlock(\n",
    "            in_channels=conv_channels[-2],\n",
    "            out_channels=conv_channels[-1],\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            groups=1,\n",
    "            bias=False,\n",
    "            norm=True,\n",
    "            activation=False,\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(conv_channels[-1], num_classes, bias=False)\n",
    "\n",
    "        ## Training-related members\n",
    "        self.criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "        self.wer = WordErrorRate()\n",
    "\n",
    "        self.vocabulary = vocabulary\n",
    "\n",
    "        self.apply(self.init_weights)  # Initialize weights\n",
    "\n",
    "    def init_weights(self, m):\n",
    "\n",
    "        if type(m) == nn.Conv2d:\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif type(m) == nn.BatchNorm2d:\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif type(m) == nn.Linear:\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n",
    "\n",
    "        x = self.in_conv(x)\n",
    "\n",
    "        for i in range(5):\n",
    "            x = self.mv2_blocks[i](x)\n",
    "\n",
    "        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n",
    "        x = self.mvit_blocks[0](x)\n",
    "\n",
    "        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n",
    "        x = self.mv2_blocks[5](x)\n",
    "\n",
    "        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n",
    "        x = self.mvit_blocks[1](x)\n",
    "\n",
    "        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n",
    "        x = self.mv2_blocks[6](x)\n",
    "\n",
    "        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n",
    "        x = self.mvit_blocks[2](x)\n",
    "\n",
    "        x = rearrange(x, \"b t c h w -> (b t) c h w\", b=B, t=T)\n",
    "        x = self.final_pw(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.classifier(x)\n",
    "        x = rearrange(x, \"(b t) p -> b t p\", b=B, t=T)\n",
    "        return x\n",
    "\n",
    "    def log_metrics(self, phase, loss, acc, prec, rec, f1):\n",
    "        self.log(f\"{phase}/loss\", loss, prog_bar=True)\n",
    "        self.log(f\"{phase}/accuracy\", acc, prog_bar=True)\n",
    "        self.log(f\"{phase}/precision\", prec, prog_bar=True)\n",
    "        self.log(f\"{phase}/recall\", rec, prog_bar=True)\n",
    "        self.log(f\"{phase}/f1\", f1, prog_bar=True)\n",
    "\n",
    "    def step(self, batch, phase):\n",
    "        x, y = batch\n",
    "\n",
    "        B, T, C, H, W = x.shape\n",
    "        B, N = y.shape\n",
    "\n",
    "        # assume padding is done in the dataloader\n",
    "        input_lengths = torch.full((B,), T, dtype=torch.long)\n",
    "        target_lengths = torch.full((B,), N, dtype=torch.long)\n",
    "\n",
    "        logits = self(x)\n",
    "\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "        loss = self.criterion(log_probs, y, input_lengths, target_lengths)\n",
    "\n",
    "        self.log(f\"{phase}/loss\", loss, prog_bar=True)\n",
    "\n",
    "        if phase != \"train\":\n",
    "            word_error_rate = self.calculate_wer(logits, y)\n",
    "            self.log(f\"{phase}/wer\", word_error_rate, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def calculate_wer(self, logits, y):\n",
    "        pred_strings = self.vocabulary.string_from_logits(logits)\n",
    "        target_strings = self.vocabulary.string_from_label(y)\n",
    "\n",
    "        return self.wer(pred_strings, target_strings)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), lr=2e-4, amsgrad=True, weight_decay=0.01\n",
    "        )\n",
    "        scheduler = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVocab:\n",
    "\n",
    "    def __init__(self, vocab, debug_mode=False):\n",
    "        self.vocab = vocab\n",
    "        self.debug_mode = debug_mode\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.vocab):\n",
    "            if self.debug_mode:\n",
    "                print(\"Index out of bounds, returning blank token\")\n",
    "            idx = 0\n",
    "        return self.vocab[idx]\n",
    "\n",
    "    def __call__(self, labels):\n",
    "        # assume 0 is the blank token\n",
    "        return [self.vocab[(i - 1)] for i in labels]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def decode_ctc(self, predictions, blank=0):\n",
    "        pred_tokens = torch.argmax(predictions, dim=-1)  # Take the most probable class\n",
    "        decoded = []\n",
    "        for seq in pred_tokens:  # Iterate over batch\n",
    "            result = []\n",
    "            prev_token = blank\n",
    "            for token in seq:\n",
    "                if token != prev_token and token != blank:\n",
    "                    result.append(token.item())\n",
    "                prev_token = token\n",
    "            decoded.append(result)\n",
    "        return decoded\n",
    "\n",
    "    def string_from_logits(self, logits):\n",
    "\n",
    "        decoded = self.decode_ctc(logits)\n",
    "\n",
    "        strings = []\n",
    "        for b in range(len(decoded)):\n",
    "            string = \" \".join(self(decoded[b]))\n",
    "            strings.append(string)\n",
    "\n",
    "        return strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "datapath = \"data/phoenix-2014.v3/phoenix2014-release/phoenix-2014-multisigner\"\n",
    "path = os.path.join(curr_dir, datapath)\n",
    "\n",
    "dtrain = PhoenixVideoTextDataset(\n",
    "    root=path,\n",
    "    split=\"train\",\n",
    "    p_drop=0.2,\n",
    "    random_drop=True,\n",
    ")\n",
    "\n",
    "\n",
    "dval = PhoenixVideoTextDataset(\n",
    "    root=path,\n",
    "    split=\"dev\",\n",
    "    p_drop=0.2,\n",
    "    random_drop=True,\n",
    ")\n",
    "\n",
    "dtest = PhoenixVideoTextDataset(\n",
    "    root=path,\n",
    "    split=\"test\",\n",
    "    p_drop=0.2,\n",
    "    random_drop=True,\n",
    ")\n",
    "\n",
    "\n",
    "def collate_add_blank(base_collate, batch):\n",
    "\n",
    "    def resize_tensor(x):\n",
    "        transform = torchvision.transforms.Resize((256, 192))\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = rearrange(\n",
    "            transform(rearrange(x, \"b t c h w -> (b t) c h w\")),\n",
    "            \"(b t) c h w -> b t c h w\",\n",
    "            b=B,\n",
    "            t=T,\n",
    "        )\n",
    "        return x\n",
    "\n",
    "    batch = base_collate(batch)  # add one to all labels\n",
    "    batch[\"text\"] = [tens + 1 for tens in batch[\"text\"]]\n",
    "    batch = (\n",
    "        torch.stack(batch[\"video\"], 0),\n",
    "        torch.stack(batch[\"text\"], 0),\n",
    "    )\n",
    "\n",
    "    batch = (resize_tensor(batch[0]), batch[1])\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "# extend the collate function so that targets are tensorized (no padding is done, we use batch size 1 and gradient accumulation with the trainer)\n",
    "collation = lambda x: collate_add_blank(dtrain.collate_fn, x)\n",
    "dl_train = DataLoader(\n",
    "    dtrain,\n",
    "    collate_fn=collation,\n",
    "    shuffle=True,\n",
    "    batch_size=1,\n",
    "    num_workers=3,\n",
    "    pin_memory=True,\n",
    ")\n",
    "dl_val = DataLoader(\n",
    "    dval,\n",
    "    collate_fn=collation,\n",
    "    shuffle=False,\n",
    "    batch_size=1,\n",
    "    num_workers=3,\n",
    "    pin_memory=True,\n",
    ")\n",
    "dl_test = DataLoader(\n",
    "    dtest,\n",
    "    collate_fn=collation,\n",
    "    shuffle=False,\n",
    "    batch_size=1,\n",
    "    num_workers=3,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# weirdly the dataset maps strings to labels, but we really want to map labels to strings\n",
    "inv_table = {v: k for k, v in dtrain.vocab.table.items()}\n",
    "\n",
    "vocab = CustomVocab(inv_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.utilities.model_summary import summarize\n",
    "from lightning.fabric.utilities import measure_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xxs_mvit(vocab):\n",
    "    return MobileViT(\n",
    "        dims=[64, 80, 96],\n",
    "        conv_channels=[16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 320],\n",
    "        num_classes=len(vocab) + 1,  # +1 for the blank token\n",
    "        vocabulary=vocab,\n",
    "        expand_ratio=2,\n",
    "        patch_size=(2, 2),\n",
    "    )\n",
    "\n",
    "\n",
    "def xs_mvit(vocab):\n",
    "    return MobileViT(\n",
    "        dims=[96, 120, 144],\n",
    "        conv_channels=[16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384],\n",
    "        num_classes=len(vocab) + 1,  # +1 for the blank token\n",
    "        vocabulary=vocab,\n",
    "        expand_ratio=4,\n",
    "        patch_size=(4, 4),\n",
    "    )\n",
    "\n",
    "\n",
    "def s_mvit(vocab):\n",
    "    return MobileViT(\n",
    "        dims=[144, 192, 240],\n",
    "        conv_channels=[16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 640],\n",
    "        num_classes=len(vocab) + 1,  # +1 for the blank token\n",
    "        vocabulary=vocab,\n",
    "        expand_ratio=4,\n",
    "        patch_size=(8, 8),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# UNCOMMENT THIS TO LOG TO WANDB\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "#\n",
    "# user_secrets = UserSecretsClient()\n",
    "# secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n",
    "#\n",
    "# wandb.login(key=secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:o9clmbxy) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">different-bee-24</strong> at: <a href='https://wandb.ai/academic_projects/mobilevit/runs/o9clmbxy' target=\"_blank\">https://wandb.ai/academic_projects/mobilevit/runs/o9clmbxy</a><br/> View project at: <a href='https://wandb.ai/academic_projects/mobilevit' target=\"_blank\">https://wandb.ai/academic_projects/mobilevit</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241211_212958-o9clmbxy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:o9clmbxy). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dario/repos/lis-vit/project/wandb/run-20241211_213615-8a0xc6s8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/academic_projects/mobilevit/runs/8a0xc6s8' target=\"_blank\">neat-glitter-25</a></strong> to <a href='https://wandb.ai/academic_projects/mobilevit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/academic_projects/mobilevit' target=\"_blank\">https://wandb.ai/academic_projects/mobilevit</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/academic_projects/mobilevit/runs/8a0xc6s8' target=\"_blank\">https://wandb.ai/academic_projects/mobilevit/runs/8a0xc6s8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/academic_projects/mobilevit/runs/8a0xc6s8?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ff72c132ad0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"mobilevit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/dario/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | in_conv     | Conv2DBlock       | 464    | train\n",
      "1 | mv2_blocks  | ModuleList        | 45.7 K | train\n",
      "2 | mvit_blocks | ModuleList        | 1.1 M  | train\n",
      "3 | final_pw    | Conv2DBlock       | 26.2 K | train\n",
      "4 | pool        | AdaptiveAvgPool2d | 0      | train\n",
      "5 | classifier  | Linear            | 394 K  | train\n",
      "6 | criterion   | CTCLoss           | 0      | train\n",
      "7 | wer         | WordErrorRate     | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 M     Total params\n",
      "6.240     Total estimated model params size (MB)\n",
      "292       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import lightning.pytorch as lp\n",
    "\n",
    "# setup training and wandb\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "wandb_logger = lp.loggers.WandbLogger()\n",
    "\n",
    "model = xxs_mvit(vocab)\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"cpu\",\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[\n",
    "        lp.callbacks.ModelCheckpoint(\n",
    "            monitor=\"val/loss\",\n",
    "            filename=\"best_model\",\n",
    "            save_top_k=1,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        lp.callbacks.EarlyStopping(\n",
    "            monitor=\"val/ loss\",\n",
    "            patience=3,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "    ],\n",
    "    accumulate_grad_batches=16,\n",
    ")\n",
    "\n",
    "wandb_logger.watch(model, log_graph=False)\n",
    "\n",
    "trainer.fit(model, dl_train, dl_val)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flush cuda cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab {'A': 0, 'AACHEN': 1, 'AB': 2, 'AB-JETZT': 3, 'AB-PLUSPLUS': 4, 'ABEND': 5, 'ABER': 6, 'ABFALLEN': 7, 'ABSCHIED': 8, 'ABSCHNITT': 9, 'ABSINKEN': 10, 'ABWECHSELN': 11, 'ACH': 12, 'ACHT': 13, 'ACHTE': 14, 'ACHTHUNDERT': 15, 'ACHTUNG': 16, 'ACHTZEHN': 17, 'ACHTZIG': 18, 'AEHNLCH': 19, 'AEHNLICH': 20, 'AENDERN': 21, 'AFRIKA': 22, 'AKTIV': 23, 'AKTUELL': 24, 'ALLE': 25, 'ALLGAEU': 26, 'ALLGEMEIN': 27, 'ALPEN': 28, 'ALPENRAND': 29, 'ALPENTAL': 30, 'ALS': 31, 'ALSO': 32, 'ALT': 33, 'AM': 34, 'AM-KUESTE': 35, 'AM-MEER': 36, 'AM-RAND': 37, 'AM-TAG': 38, 'AMERIKA': 39, 'AN': 40, 'ANDERE': 41, 'ANDERE-MOEGLICHKEIT': 42, 'ANDERS': 43, 'ANFANG': 44, 'ANGEMESSEN': 45, 'ANGENEHM': 46, 'ANGST': 47, 'ANHALT': 48, 'ANKOMMEN': 49, 'ANSAMMELN': 50, 'ANSCHAUEN': 51, 'APRIL': 52, 'ARM': 53, 'ATLANTIK': 54, 'AUCH': 55, 'AUCH-NICHT': 56, 'AUF': 57, 'AUF-JEDEN-FALL': 58, 'AUFBLUEHEN': 59, 'AUFEINANDERTREFFEN': 60, 'AUFFUELLEN': 61, 'AUFHEITERN': 62, 'AUFHOEREN': 63, 'AUFKLAREN': 64, 'AUFKOMMEN': 65, 'AUFLOCKERUNG': 66, 'AUFLOCKERUNG-PLUSPLUS': 67, 'AUFLOESEN': 68, 'AUFLOESEN-PLUSPLUS': 69, 'AUFPASSEN': 70, 'AUFTAUCHEN': 71, 'AUFZIEHEN': 72, 'AUFZIEHEN-PLUSPLUS': 73, 'AUGUST': 74, 'AUS': 75, 'AUSEINANDER': 76, 'AUSHALTEN': 77, 'AUSNAHME': 78, 'AUSRICHTEN': 79, 'AUSSEHEN': 80, 'AUSSERGEWOEHNLICH': 81, 'AUSWAEHLEN-PLUSPLUS': 82, 'AUTO': 83, 'AUTOMATISCH': 84, 'B': 85, 'BAD': 86, 'BADEN': 87, 'BADEN-WUERTTEMBERG': 88, 'BALD': 89, 'BAUER': 90, 'BAUM': 91, 'BAYERN': 92, 'BEDECKT': 93, 'BEDEUTEN': 94, 'BEDINGUNGEN': 95, 'BEGINN': 96, 'BEGRUESSEN': 97, 'BEI-UNS': 98, 'BEISEITE': 99, 'BEISPIEL': 100, 'BEKANNT': 101, 'BEKANNTGEBEN': 102, 'BEKOMMEN': 103, 'BELAESTIGUNG': 104, 'BELGIEN': 105, 'BEOBACHTEN': 106, 'BEREICH': 107, 'BERG': 108, 'BERGAB': 109, 'BERGAUF': 110, 'BERLIN': 111, 'BERUF': 112, 'BERUHIGEN': 113, 'BESONDERS': 114, 'BESPRECHEN': 115, 'BESSER': 116, 'BESSER-PLUSPLUS': 117, 'BESTIMMT': 118, 'BETROFFEN': 119, 'BETT': 120, 'BEWEGEN': 121, 'BEWOELKT': 122, 'BEWOELKT-PLUSPLUS': 123, 'BIS': 124, 'BIS-JETZT': 125, 'BIS-MITTE': 126, 'BIS-MORGEN': 127, 'BISHER': 128, 'BISSCHEN': 129, 'BISSCHEN-PLUSPLUS': 130, 'BITTE': 131, 'BLATT': 132, 'BLAU': 133, 'BLEIBEN': 134, 'BLEIBEN-GLEICH': 135, 'BLITZ': 136, 'BLOCKIEREN': 137, 'BLUETE': 138, 'BLUMEN-PLUSPLUS': 139, 'BODEN': 140, 'BODENSEE': 141, 'BOEE': 142, 'BRAND': 143, 'BRANDENBURG': 144, 'BRAUCHEN': 145, 'BRAUN': 146, 'BREMEN': 147, 'BRINGEN': 148, 'BRITANNIEN': 149, 'BROCKEN': 150, 'BRUCKBERG': 151, 'BUNT': 152, 'BURG': 153, 'C': 154, 'CHANCE': 155, 'CHANCE-PLUSPLUS': 156, 'CHAOS': 157, 'D': 158, 'DA': 159, 'DABEI': 160, 'DABEI-PLUSPLUS': 161, 'DAENEMARK': 162, 'DAFUER': 163, 'DAMEN': 164, 'DANACH': 165, 'DANEBEN': 166, 'DANN': 167, 'DARAUF': 168, 'DARUM': 169, 'DARUNTER': 170, 'DAS-IST-ES': 171, 'DAS-WAR-ES': 172, 'DASSELBE': 173, 'DAUER': 174, 'DAUERND': 175, 'DAZU': 176, 'DAZWISCHEN': 177, 'DEMNAECHST': 178, 'DENKEN': 179, 'DESHALB': 180, 'DESWEGEN': 181, 'DEUTSCH': 182, 'DEUTSCHLAND': 183, 'DEUTSCHLANDRAUM': 184, 'DEZEMBER': 185, 'DICHT': 186, 'DICK': 187, 'DIENST': 188, 'DIENSTAG': 189, 'DIESE': 190, 'DIESE-WOCHE': 191, 'DIESMAL': 192, 'DOCH': 193, 'DOCH-SONST-NOCH': 194, 'DONNER': 195, 'DONNERSTAG': 196, 'DRAUSSEN': 197, 'DREHEN': 198, 'DREI': 199, 'DREI-MONATE': 200, 'DREIDREISSIG': 201, 'DREIHUNDERT': 202, 'DREIMAL': 203, 'DREISSIG': 204, 'DREIZEHN': 205, 'DRESDEN': 206, 'DRITTE': 207, 'DRUCK': 208, 'DRUCKFLAECHE': 209, 'DU': 210, 'DUENN': 211, 'DUESSELDORF': 212, 'DUMM': 213, 'DUNST': 214, 'DURCH': 215, 'DURCHEINANDER': 216, 'DURCHGEHEND': 217, 'DURCHSCHNITT': 218, 'E': 219, 'EBEN': 220, 'ECHT': 221, 'EIFEL': 222, 'EIGENTLICH': 223, 'EIN': 224, 'EIN-BISSCHEN': 225, 'EIN-JAHR': 226, 'EIN-PAAR': 227, 'EIN-WOCHE': 228, 'EINE': 229, 'EINFACH': 230, 'EINFLUSS': 231, 'EINHUNDERT': 232, 'EINIGE': 233, 'EINIGERMASSEN': 234, 'EINLUSS': 235, 'EINS': 236, 'EINSCHRAENKEN': 237, 'EINZELN-PLUSPLUS': 238, 'EIS': 239, 'EISEN': 240, 'ELF': 241, 'ELFTE': 242, 'EMPFINDLICH': 243, 'ENDE': 244, 'ENDLICH': 245, 'ENGLAND': 246, 'ENORM': 247, 'ENTFERNT': 248, 'ENTHALTEN': 249, 'ENTSCHULDIGUNG': 250, 'ENTSPANNT': 251, 'ENTWICKELN': 252, 'ERDRUTSCH': 253, 'ERFAHREN': 254, 'ERFAHRUNG': 255, 'ERFURT': 256, 'ERHOEHEN': 257, 'ERLEICHERT': 258, 'ERSCHROCKEN': 259, 'ERST': 260, 'ERSTE': 261, 'ERSTMAL': 262, 'ERWARTEN': 263, 'ERZ': 264, 'ES-BEDEUTET': 265, 'ES-GIBT': 266, 'ETWAS': 267, 'EUCH': 268, 'EUROPA': 269, 'EWIG': 270, 'EXTREM': 271, 'F': 272, 'FACH': 273, 'FAHREN': 274, 'FALLEN': 275, 'FAST': 276, 'FEBRUAR': 277, 'FEHLT': 278, 'FEIER': 279, 'FELD': 280, 'FERTIG': 281, 'FEST': 282, 'FEUCHT': 283, 'FINNLAND': 284, 'FLACH': 285, 'FLAECHENDECKEND': 286, 'FLIESSEN': 287, 'FLOCKEN': 288, 'FLUSS': 289, 'FLUT': 290, 'FOEHN': 291, 'FOLGE': 292, 'FRAGEZEICHEN': 293, 'FRANKFURT': 294, 'FRANKREICH': 295, 'FREI': 296, 'FREITAG': 297, 'FREIZEIT': 298, 'FREUEN': 299, 'FREUNDLICH': 300, 'FRISCH': 301, 'FRONT': 302, 'FROST': 303, 'FROST-PLUSPLUS': 304, 'FRUEH': 305, 'FRUEHLING': 306, 'FUEHLEN': 307, 'FUEHLEN-WIE': 308, 'FUENF': 309, 'FUENF-TAGE': 310, 'FUENF-UHR': 311, 'FUENFHUNDERT': 312, 'FUENFTE': 313, 'FUENFZEHN': 314, 'FUENFZIG': 315, 'FUER': 316, 'FUER-ALLE': 317, 'FUER-UNS': 318, 'G': 319, 'GANZTAGS': 320, 'GEBEN': 321, 'GEBIRGE': 322, 'GEFAHR': 323, 'GEFRIEREN': 324, 'GEHEN': 325, 'GEHOERT': 326, 'GEHT-SO': 327, 'GELB': 328, 'GEMISCHT': 329, 'GEMUETLICH': 330, 'GENAU': 331, 'GENIESSEN': 332, 'GENUG': 333, 'GERADE': 334, 'GESAMT': 335, 'GESCHWINDIGKEIT': 336, 'GESTERN': 337, 'GETRENNT': 338, 'GEWESEN': 339, 'GEWITTER': 340, 'GEWITTER-PLUSPLUS': 341, 'GEWOHNT': 342, 'GIPFEL': 343, 'GLATT': 344, 'GLATTEIS': 345, 'GLAUBEN': 346, 'GLEICH': 347, 'GLEICH-BLEIBEN': 348, 'GLEICH-PLUSPLUS': 349, 'GLEICH-WIE': 350, 'GLUECK': 351, 'GOLD': 352, 'GOTT': 353, 'GRAD': 354, 'GRAD-PLUSPLUS': 355, 'GRAU': 356, 'GRAUPEL': 357, 'GRAUPEL-PLUSPLUS': 358, 'GRENZE': 359, 'GRIECHENLAND': 360, 'GROB': 361, 'GROSS': 362, 'GROSSBRITANNIEN': 363, 'GRUEN': 364, 'GRUND': 365, 'GUT': 366, 'GUT-ABEND': 367, 'H': 368, 'HAAR': 369, 'HABEN': 370, 'HABEN2': 371, 'HAELFTE': 372, 'HAFTEN': 373, 'HAGEL': 374, 'HAGEL-PLUSPLUS': 375, 'HALB': 376, 'HALLO': 377, 'HALTEN': 378, 'HAMBURG': 379, 'HANNOVER': 380, 'HARMLOS': 381, 'HART': 382, 'HAUPT': 383, 'HAUPTSAECHLICH': 384, 'HAVEN': 385, 'HEFTIG': 386, 'HEILIG': 387, 'HEILIGE': 388, 'HEISS': 389, 'HELL': 390, 'HERAB': 391, 'HERBST': 392, 'HERREN': 393, 'HERVORRAGEND': 394, 'HERZ': 395, 'HESSEN': 396, 'HEUTE': 397, 'HEUTE-ABEND': 398, 'HEUTE-MITTAG': 399, 'HEUTE-NACHT': 400, 'HIER': 401, 'HIMMEL': 402, 'HINDERNIS': 403, 'HOCH': 404, 'HOCHWASSER': 405, 'HOEHE': 406, 'HOEHER': 407, 'HOEREN': 408, 'HOFFEN': 409, 'HOLEN': 410, 'HOLLAND': 411, 'HOLSTEIN': 412, 'HUND': 413, 'HUNDERT': 414, 'HUT': 415, 'I': 416, 'ICH': 417, 'IHR': 418, 'IM': 419, 'IM-LAUFE': 420, 'IM-MOMENT': 421, 'IM-VERLAUF': 422, 'IMMER': 423, 'IN': 424, 'IN-BESTIMMT-ZEIT': 425, 'IN-DIESE-WOCHE': 426, 'IN-KOMMEND': 427, 'IN-KOMMEND-TAG': 428, 'IN-KOMMEND-ZEIT': 429, 'IN-KUERZE': 430, 'IN-PAAR-TAG': 431, 'IN-PAAR-TAGE-SPAETER': 432, 'INFORMIEREN': 433, 'INNERHALB': 434, 'INSEL': 435, 'INSEL-PLUSPLUS': 436, 'INSGESAMT': 437, 'INTERESSANT': 438, 'INTERNET': 439, 'IRGENDWANN': 440, 'IRLAND': 441, 'ISLAND': 442, 'ITALIEN': 443, 'IX': 444, 'J': 445, 'JA': 446, 'JAHR': 447, 'JANUAR': 448, 'JEDEN-TAG': 449, 'JETZT': 450, 'JULI': 451, 'JUNI': 452, 'K': 453, 'KALT': 454, 'KALT-PLUSPLUS': 455, 'KALTFRONT': 456, 'KANADA': 457, 'KANAL': 458, 'KANN': 459, 'KAPUTTGEGANGEN': 460, 'KARFREITAG': 461, 'KAUM': 462, 'KAUM-PLUSPLUS': 463, 'KEIN': 464, 'KIEL': 465, 'KILOMETER': 466, 'KLAPPEN': 467, 'KLAR': 468, 'KLAR-PLUSPLUS': 469, 'KLEIN': 470, 'KLEINIGKEIT': 471, 'KNAPP': 472, 'KOBLENZ': 473, 'KOELN': 474, 'KOENNEN': 475, 'KOENNEN-PLUSPLUS': 476, 'KOERPER': 477, 'KOERPER-PLUSPLUS': 478, 'KOMMA': 479, 'KOMMEN': 480, 'KOMMEN-PLUSPLUS': 481, 'KOMPLETT': 482, 'KONSTANT': 483, 'KORB': 484, 'KRAEFTIG': 485, 'KRATZEN': 486, 'KRISE': 487, 'KROATIEN': 488, 'KUCHEN': 489, 'KUEHL': 490, 'KUEHL-PLUSPLUS': 491, 'KUEHLER': 492, 'KUESTE': 493, 'KURVE': 494, 'KURZ': 495, 'L': 496, 'LAERM-PLUSPLUS': 497, 'LAGE': 498, 'LAHM': 499, 'LAND': 500, 'LANDSCHAFT': 501, 'LANG': 502, 'LANGSAM': 503, 'LAUFEN': 504, 'LAUSITZ': 505, 'LEBEN': 506, 'LEICHT': 507, 'LEIDER': 508, 'LEIPZIG': 509, 'LESEN': 510, 'LETZTE': 511, 'LETZTE-WOCHE': 512, 'LEUTE': 513, 'LICHT': 514, 'LIEB': 515, 'LIEGEN': 516, 'LITER': 517, 'LOCH': 518, 'LOCH-PLUSPLUS': 519, 'LOCKER': 520, 'LOS': 521, 'LUECKE': 522, 'LUFT': 523, 'M': 524, 'MACHEN': 525, 'MAERZ': 526, 'MAESSIG': 527, 'MAESSIG-PLUSPLUS': 528, 'MAI': 529, 'MAINZ': 530, 'MAL': 531, 'MAL-AB-ZU': 532, 'MAL-PLUSPLUS': 533, 'MANCHMAL': 534, 'MANCHMAL-PLUSPLUS': 535, 'MARKT': 536, 'MASCHINE': 537, 'MATSCH': 538, 'MAXIMAL': 539, 'MECKLENBURG': 540, 'MECKLENBURG-VORPOMMERN': 541, 'MEER': 542, 'MEHR': 543, 'MEHR-PLUSPLUS': 544, 'MEHR-WENIG': 545, 'MEHRMALS': 546, 'MEINEN': 547, 'MEISTENS': 548, 'MERKEN': 549, 'MERKWUERDIG': 550, 'MESSEN': 551, 'METER': 552, 'MILD': 553, 'MINDESTENS': 554, 'MINUS': 555, 'MISCHUNG': 556, 'MIT': 557, 'MITBEKOMMEN': 558, 'MITEILEN': 559, 'MITNEHMEN': 560, 'MITTAG': 561, 'MITTE': 562, 'MITTEGEBIRGE': 563, 'MITTEILEN': 564, 'MITTWOCH': 565, 'MITZIEHEN': 566, 'MM': 567, 'MOEGEN': 568, 'MOEGLICH': 569, 'MOMENT': 570, 'MOND': 571, 'MONTAG': 572, 'MORGEN': 573, 'MORGEN-FRUEH': 574, 'MORGENS': 575, 'MOSKAU': 576, 'MUENCHEN': 577, 'MUENSTER': 578, 'MUESSEN': 579, 'MUND': 580, 'N': 581, 'NACH': 582, 'NACH-HAUSE': 583, 'NACHMITTAG': 584, 'NACHT': 585, 'NAECHSTE': 586, 'NAECHSTE-WOCHE': 587, 'NAEHE': 588, 'NAH': 589, 'NASS': 590, 'NATUR': 591, 'NEBEL': 592, 'NEIN': 593, 'NEU': 594, 'NEUN': 595, 'NEUNHUNDERT': 596, 'NEUNTE': 597, 'NEUNZEHN': 598, 'NEUNZEHNTE': 599, 'NEUNZIG': 600, 'NICHT': 601, 'NICHT-ACHTZEHN': 602, 'NICHT-BEDEUTEN': 603, 'NICHT-DEUTSCH': 604, 'NICHT-DIESE-WOCHE': 605, 'NICHT-EINFLUSS': 606, 'NICHT-FROST': 607, 'NICHT-FUEHLEN': 608, 'NICHT-FUENF': 609, 'NICHT-GEMUETLICH': 610, 'NICHT-GENUG': 611, 'NICHT-GEWITTER': 612, 'NICHT-GLEICH': 613, 'NICHT-GRAD': 614, 'NICHT-HABEN': 615, 'NICHT-HART': 616, 'NICHT-HEISS': 617, 'NICHT-HOEHE': 618, 'NICHT-IMMER': 619, 'NICHT-KALT': 620, 'NICHT-KAUM': 621, 'NICHT-KEIN': 622, 'NICHT-KLAR': 623, 'NICHT-KOENNEN': 624, 'NICHT-KOMMEN': 625, 'NICHT-MEHR': 626, 'NICHT-MEISTENS': 627, 'NICHT-NACHT': 628, 'NICHT-NAJA': 629, 'NICHT-NEBEL': 630, 'NICHT-NEIN': 631, 'NICHT-NICHT-MEHR': 632, 'NICHT-NICHT-NUR': 633, 'NICHT-NICHTS': 634, 'NICHT-NOCH-NICHT': 635, 'NICHT-NORD': 636, 'NICHT-NULL': 637, 'NICHT-REGEN': 638, 'NICHT-REGEN-PLUSPLUS': 639, 'NICHT-REGION': 640, 'NICHT-RICHTIG': 641, 'NICHT-SCHEINEN': 642, 'NICHT-SCHLECHT': 643, 'NICHT-SCHLIMM': 644, 'NICHT-SCHNEE': 645, 'NICHT-SCHOEN': 646, 'NICHT-SEHEN': 647, 'NICHT-SELTEN': 648, 'NICHT-SONNE': 649, 'NICHT-SPUEREN': 650, 'NICHT-STARK': 651, 'NICHT-TEIL': 652, 'NICHT-THEMA': 653, 'NICHT-TROCKEN': 654, 'NICHT-VIEL': 655, 'NICHT-VON': 656, 'NICHT-WARM': 657, 'NICHT-WARTEN': 658, 'NICHT-ZU-WARM': 659, 'NICHT-cl-KOMMEN': 660, 'NICHTALP-AUCH': 661, 'NICHTALP-BRAUCHEN': 662, 'NICHTALP-GIBT': 663, 'NICHTALP-KEIN': 664, 'NICHTALP-KOENNEN': 665, 'NICHTALP-MUSS': 666, 'NICHTALP-NICHT': 667, 'NICHTALP-PASSEN': 668, 'NICHTALP-STIMMT': 669, 'NICHTS': 670, 'NIEDER': 671, 'NIEDERSACHSEN': 672, 'NIEDERUNG': 673, 'NIEDRIG': 674, 'NIESELREGEN': 675, 'NN': 676, 'NOCH': 677, 'NOCH-MEHR': 678, 'NOCH-NICHT': 679, 'NOCH-PLUSPLUS': 680, 'NOCHEINMAL': 681, 'NORD': 682, 'NORDOST': 683, 'NORDOSTRAUM': 684, 'NORDPOL': 685, 'NORDRAUM': 686, 'NORDRHEIN': 687, 'NORDRHEIN-WESTFALEN': 688, 'NORDSEE': 689, 'NORDWEST': 690, 'NORDWESTRAUM': 691, 'NORMAL': 692, 'NORWEGEN': 693, 'NOVEMBER': 694, 'NRW': 695, 'NULL': 696, 'NUMMER': 697, 'NUR': 698, 'O': 699, 'OB': 700, 'OBEN': 701, 'OBER': 702, 'OBWOHL': 703, 'ODER': 704, 'OESTERREICH': 705, 'OFT': 706, 'OFT-PLUSPLUS': 707, 'OHNE': 708, 'OKTOBER': 709, 'ORANGE': 710, 'ORKAN': 711, 'ORT': 712, 'ORT-PLUSPLUS': 713, 'OST': 714, 'OSTERN': 715, 'OSTRAUM': 716, 'P': 717, 'PAAR': 718, 'PAAR-TAG': 719, 'PASSEN': 720, 'PASSIEREN': 721, 'PAUSE': 722, 'PFALZ': 723, 'PFEIL': 724, 'PFINGSTEN': 725, 'PFLANZE': 726, 'PLOETZLICH': 727, 'PLUS': 728, 'POLEN': 729, 'POMMERN': 730, 'PORTUGAL': 731, 'POSITIV': 732, 'PRO': 733, 'PROBLEM': 734, 'PROZENT': 735, 'PUENKTLICH': 736, 'PULLOVER': 737, 'PUNKT': 738, 'QUADRAT': 739, 'QUADRATMETER': 740, 'QUELL': 741, 'R': 742, 'RAND': 743, 'RAUM': 744, 'RAUSFALLEN': 745, 'RECHNEN': 746, 'REDUZIEREN': 747, 'REGEN': 748, 'REGEN-AUF-ALPEN': 749, 'REGEN-PLUSPLUS': 750, 'REGION': 751, 'REGION-PLUSPLUS': 752, 'REIF': 753, 'REIN': 754, 'REKORD': 755, 'REST': 756, 'RHEIN': 757, 'RHEINDLAND': 758, 'RHEINLAND': 759, 'RHEINLAND-PFALZ': 760, 'RICHTIG': 761, 'RICHTUNG': 762, 'RISIKO': 763, 'RODELN': 764, 'ROSTOCK': 765, 'ROT': 766, 'RUECKEN': 767, 'RUEGEN': 768, 'RUHIG': 769, 'RUHRGEBIET': 770, 'RUMAENIEN': 771, 'RUND-UM-DIE-UHR': 772, 'RUSSLAND': 773, 'S': 774, 'S0NNE': 775, 'SAARLAND': 776, 'SACHSEN': 777, 'SAGE': 778, 'SAGEN': 779, 'SAMSTAG': 780, 'SAND': 781, 'SAUER': 782, 'SCH': 783, 'SCHADEN': 784, 'SCHAETZEN': 785, 'SCHAFFEN': 786, 'SCHAU-MAL': 787, 'SCHAUEN': 788, 'SCHAUER': 789, 'SCHAUER-PLUSPLUS': 790, 'SCHEINEN': 791, 'SCHEINEN-PLUSPLUS': 792, 'SCHIRM': 793, 'SCHLAF': 794, 'SCHLAGSAHNE': 795, 'SCHLECHT': 796, 'SCHLECHTER': 797, 'SCHLESWIG': 798, 'SCHLIMM': 799, 'SCHLIMMER': 800, 'SCHLUSS': 801, 'SCHMELZEN': 802, 'SCHNEE': 803, 'SCHNEE-PLUSPLUS': 804, 'SCHNEIEN': 805, 'SCHNEIEN-PLUSPLUS': 806, 'SCHNELL': 807, 'SCHOEN': 808, 'SCHON': 809, 'SCHON-WIEDER': 810, 'SCHOTTLAND': 811, 'SCHRANK': 812, 'SCHUETZEN': 813, 'SCHULD': 814, 'SCHWACH': 815, 'SCHWACH-PLUSPLUS': 816, 'SCHWARZ': 817, 'SCHWEDEN': 818, 'SCHWER': 819, 'SCHWIERIG': 820, 'SCHWIERIG-PLUSPLUS': 821, 'SCHWITZEN': 822, 'SCHWUEL': 823, 'SECHS': 824, 'SECHSHUNDERT': 825, 'SECHSTE': 826, 'SECHSZEHN': 827, 'SEE': 828, 'SEHEN': 829, 'SEHR': 830, 'SEI-DANK': 831, 'SEIT': 832, 'SEITE': 833, 'SELBE': 834, 'SELTEN': 835, 'SEPTEMBER': 836, 'SICHER': 837, 'SIE': 838, 'SIEBEN': 839, 'SIEBEN-WOCHE': 840, 'SIEBENHUNDERT': 841, 'SIEBTE': 842, 'SIEBZEHN': 843, 'SIEBZIG': 844, 'SINKEN': 845, 'SITUATION': 846, 'SITZ': 847, 'SKANDINAVIEN': 848, 'SKI': 849, 'SO': 850, 'SO-BLEIBEN': 851, 'SOLL': 852, 'SOMMER': 853, 'SONNE': 854, 'SONNE-PLUSPLUS': 855, 'SONNENUNTERGANG': 856, 'SONNTAG': 857, 'SONST': 858, 'SOWIESO': 859, 'SPAET': 860, 'SPAETER': 861, 'SPAETESTEN': 862, 'SPANIEN': 863, 'SPAZIEREN': 864, 'SPEZIELL': 865, 'SPITZE': 866, 'SPORT': 867, 'SPRIESSEN': 868, 'SPUEREN': 869, 'STABIL': 870, 'STADT': 871, 'STAMM': 872, 'STARK': 873, 'START': 874, 'STAU': 875, 'STEHEN': 876, 'STEIGEN': 877, 'STEIGEN-OBEN': 878, 'STEIGEN-RUNTER': 879, 'STEIGEN-RUNTER-PLUSPLUS': 880, 'STEIN': 881, 'STELLENWEISE': 882, 'STERN': 883, 'STERN-PLUSPLUS': 884, 'STIMMT': 885, 'STOCKEN': 886, 'STOERUNG': 887, 'STRAHLEN': 888, 'STRASSE': 889, 'STRENG': 890, 'STROEMEN': 891, 'STROM': 892, 'STUNDE': 893, 'STURM': 894, 'STURM-PLUSPLUS': 895, 'STUTTGART': 896, 'SUCHEN': 897, 'SUED': 898, 'SUED-PLUSPLUS': 899, 'SUEDOST': 900, 'SUEDOSTRAUM': 901, 'SUEDRAUM': 902, 'SUEDWEST': 903, 'SUEDWESTRAUM': 904, 'SUPER': 905, 'SYLT': 906, 'T': 907, 'T-SHIRT': 908, 'TAG': 909, 'TAGE': 910, 'TAGSUEBER': 911, 'TAL': 912, 'TANKEN': 913, 'TATSAECHLICH': 914, 'TAU': 915, 'TAUEN': 916, 'TAUGEN': 917, 'TAUSEND': 918, 'TEIL': 919, 'TEILWEISE': 920, 'TEMPERATUR': 921, 'TEMPERATUR-PLUSPLUS': 922, 'TEXT': 923, 'THEMA': 924, 'THUERINGEN': 925, 'TIEF': 926, 'TJA': 927, 'TOLL': 928, 'TRAUM': 929, 'TRENNEN': 930, 'TROCKEN': 931, 'TROPFEN': 932, 'TROPFEN-PLUSPLUS': 933, 'TROPISCH': 934, 'TROTZDEM': 935, 'TRUEB': 936, 'TRUEB-PLUSPLUS': 937, 'TSCHECHIEN': 938, 'TSCHUESS': 939, 'TUERKEI': 940, 'TUERKEI-PLUSPLUS': 941, 'TUN': 942, 'TYPISCH': 943, 'U': 944, 'UEBER': 945, 'UEBER-UNTER': 946, 'UEBERALL': 947, 'UEBERFLUTUNG': 948, 'UEBERMORGEN': 949, 'UEBERSCHWEMMUNG': 950, 'UEBERWIEGEND': 951, 'UHR': 952, 'UM': 953, 'UM-PLUSPLUS': 954, 'UMKEHREN': 955, 'UMSTAENDLICH': 956, 'UMSTELLEN': 957, 'UMWANDELN': 958, 'UND': 959, 'UND-DANN': 960, 'UND-SO-WEITER': 961, 'UNGARN': 962, 'UNGEFAEHR': 963, 'UNGEMUETLICH': 964, 'UNSER': 965, 'UNSICHER': 966, 'UNTEN': 967, 'UNTER': 968, 'UNTERNEHMEN': 969, 'UNTERSCHIED': 970, 'UNTERSCHIED-PLUSPLUS': 971, 'UNWAHRSCHEINLICH': 972, 'UNWETTER': 973, 'URLAUB': 974, 'V': 975, 'VERAENDERN': 976, 'VERANTWORTLICH': 977, 'VERBINDEN': 978, 'VERBREITEN': 979, 'VERDICHTEN': 980, 'VEREINZELT': 981, 'VERGLEICH': 982, 'VERKEHR': 983, 'VERLAUFEN': 984, 'VERMEIDEN': 985, 'VERRINGERN': 986, 'VERSCHIEBEN': 987, 'VERSCHIEDEN': 988, 'VERSCHWINDEN': 989, 'VERSCHWINDEN-PLUSPLUS': 990, 'VERSPAETET': 991, 'VERSUCHEN': 992, 'VERTEILEN': 993, 'VERTREIBEN': 994, 'VERWOEHNT': 995, 'VIDEO': 996, 'VIEL': 997, 'VIELLEICHT': 998, 'VIER': 999, 'VIERHUNDERT': 1000, 'VIERTE': 1001, 'VIERZEHN': 1002, 'VIERZIG': 1003, 'VOGEL': 1004, 'VOLL': 1005, 'VON': 1006, 'VOR': 1007, 'VOR-ALLEM': 1008, 'VOR-LETZTEN-TAGEN': 1009, 'VORAUS': 1010, 'VORAUSSAGE': 1011, 'VORBEI': 1012, 'VORBEREITEN': 1013, 'VORDERSCHEIBE': 1014, 'VORHER': 1015, 'VORMITTAG': 1016, 'VORPOMMERN': 1017, 'VORSICHT': 1018, 'VORSTELLEN': 1019, 'VORTEIL': 1020, 'VORUEBERGEHEND': 1021, 'W': 1022, 'WACHSEN': 1023, 'WAHR': 1024, 'WAHRSCHEINLICH': 1025, 'WALD': 1026, 'WANN': 1027, 'WAR': 1028, 'WARM': 1029, 'WARNUNG': 1030, 'WARSCHEINLICH': 1031, 'WARTEN': 1032, 'WARUM': 1033, 'WAS': 1034, 'WASCH': 1035, 'WASSER': 1036, 'WASSER-STEIGEN': 1037, 'WECHSEL': 1038, 'WECHSELHAFT': 1039, 'WECHSELHAFT-PLUSPLUS': 1040, 'WEG': 1041, 'WEHEN': 1042, 'WEHEN-PLUSPLUS': 1043, 'WEIBER': 1044, 'WEIHNACHT': 1045, 'WEIHNACHTEN': 1046, 'WEIL': 1047, 'WEIN': 1048, 'WEIT': 1049, 'WEIT-SEHEN': 1050, 'WEITER': 1051, 'WENIG': 1052, 'WENIGER': 1053, 'WENN': 1054, 'WER': 1055, 'WERDEN': 1056, 'WERT': 1057, 'WESER': 1058, 'WEST': 1059, 'WESTRAUM': 1060, 'WETTER': 1061, 'WICHTIG': 1062, 'WIE': 1063, 'WIE-AUSSEHEN': 1064, 'WIE-GEBLIEBEN': 1065, 'WIE-IMMER': 1066, 'WIE-IMMER-PLUSPLUS': 1067, 'WIE-LANG': 1068, 'WIEDER': 1069, 'WIEDER-ZURUECK': 1070, 'WIESE': 1071, 'WIEVIEL': 1072, 'WIND': 1073, 'WIND-PLUSPLUS': 1074, 'WINTER': 1075, 'WIR': 1076, 'WIRBEL': 1077, 'WIRBELSTURM': 1078, 'WIRKEN': 1079, 'WIRKLICH': 1080, 'WIRTSCHAFT': 1081, 'WISSEN': 1082, 'WO': 1083, 'WOCHE': 1084, 'WOCHENENDE': 1085, 'WOHER': 1086, 'WOHNEN': 1087, 'WOLKE': 1088, 'WOLKE-PLUSPLUS': 1089, 'WUENSCHEN': 1090, 'WUERTTEMBERG': 1091, 'WUERZ': 1092, 'WUNDERBAR': 1093, 'WUNDERSCHOEN': 1094, 'X': 1095, 'Y': 1096, 'Z': 1097, 'ZAHL': 1098, 'ZEHN': 1099, 'ZEHNTE': 1100, 'ZEIGEN': 1101, 'ZEIGEN-BILDSCHIRM': 1102, 'ZEIT': 1103, 'ZEITSKALA': 1104, 'ZENTIMETER': 1105, 'ZENTRUM': 1106, 'ZIEHEN': 1107, 'ZONE': 1108, 'ZOOM': 1109, 'ZU': 1110, 'ZU-ENDE': 1111, 'ZU-HAUSE': 1112, 'ZU-TUN': 1113, 'ZUERST': 1114, 'ZUFRIEDEN': 1115, 'ZUG': 1116, 'ZUM-BEISPIEL': 1117, 'ZUM-GLUECK': 1118, 'ZURUECK': 1119, 'ZUSAMMEN': 1120, 'ZUSAMMENHANG': 1121, 'ZUSAMMENSTOSS': 1122, 'ZUSAMMENTREFFEN': 1123, 'ZUSCHAUER': 1124, 'ZWANZIG': 1125, 'ZWEI': 1126, 'ZWEI-TAG': 1127, 'ZWEIDREISSIG': 1128, 'ZWEIFEL': 1129, 'ZWEITE': 1130, 'ZWEIZWANZIG': 1131, 'ZWISCHEN': 1132, 'ZWISCHEN-NULL': 1133, 'ZWOELF': 1134, 'ZWOELFTE': 1135, '__EMOTION__': 1136, '__LEFTHAND__': 1137, '__OFF__': 1138, '__ON__': 1139, '__PU__': 1140, 'cl-KOMMEN': 1141, 'cl-KOMMEN-PLUSPLUS': 1142, 'cl-NULL': 1143, 'lh-HOCH': 1144, 'loc-ALPEN': 1145, 'loc-AUFLOESEN': 1146, 'loc-AUFLOESEN-PLUSPLUS': 1147, 'loc-AUFTEILEN': 1148, 'loc-AUFZIEHEN': 1149, 'loc-AUFZIEHEN-PLUSPLUS': 1150, 'loc-AUSWAEHLEN': 1151, 'loc-BEREICH': 1152, 'loc-BERG': 1153, 'loc-DRUCKFLAECHE': 1154, 'loc-FLAECHENDECKEND': 1155, 'loc-FLUSS': 1156, 'loc-GLITZERN': 1157, 'loc-GRAD': 1158, 'loc-GRENZE': 1159, 'loc-HABEN': 1160, 'loc-HABEN2-PLUSPLUS': 1161, 'loc-INSEL-PLUSPLUS': 1162, 'loc-IRGENDWO': 1163, 'loc-IX': 1164, 'loc-KARTE': 1165, 'loc-KOMMEN': 1166, 'loc-KUESTE': 1167, 'loc-LAND': 1168, 'loc-LAND-PLUSPLUS': 1169, 'loc-LOCH-PLUSPLUS': 1170, 'loc-MEER': 1171, 'loc-MINUS': 1172, 'loc-MINUS-PLUSPLUS': 1173, 'loc-MITTE': 1174, 'loc-NEBEL': 1175, 'loc-NEBEN': 1176, 'loc-NEUN': 1177, 'loc-NORD': 1178, 'loc-NORDOST': 1179, 'loc-NORDWEST': 1180, 'loc-NULL': 1181, 'loc-OBEN': 1182, 'loc-ORT': 1183, 'loc-ORT-PLUSPLUS': 1184, 'loc-OST': 1185, 'loc-OSTBAYERN': 1186, 'loc-POSITION': 1187, 'loc-POSITION-PLUSPLUS': 1188, 'loc-RAUM': 1189, 'loc-REGEN': 1190, 'loc-REGEN-PLUSPLUS': 1191, 'loc-REGION': 1192, 'loc-REGION-PLUSPLUS': 1193, 'loc-SCHWACH-PLUSPLUS': 1194, 'loc-SECHS': 1195, 'loc-SEE': 1196, 'loc-SIEBEN': 1197, 'loc-SIEBENZEHN': 1198, 'loc-SONNE-SCHEINEN': 1199, 'loc-STELLENWEISE-PLUSPLUS': 1200, 'loc-STREIFEN': 1201, 'loc-STROEMEN': 1202, 'loc-STURM': 1203, 'loc-SUED': 1204, 'loc-SUEDOST': 1205, 'loc-SUEDRAUM': 1206, 'loc-SUEDWEST': 1207, 'loc-TAL': 1208, 'loc-TIEF': 1209, 'loc-TROCKEN': 1210, 'loc-UEBERALL': 1211, 'loc-UNTEN': 1212, 'loc-VERSCHIEBEN': 1213, 'loc-VIER': 1214, 'loc-WEHEN': 1215, 'loc-WEST': 1216, 'loc-WIND': 1217, 'loc-WOLKE': 1218, 'loc-ZEIGEN': 1219, 'loc-ZONE': 1220, 'loc-ZWEI': 1221, 'loc-ZWISCHEN': 1222, 'loc-ZWOELF': 1223, 'poss-BEI-UNS': 1224, 'poss-EUCH': 1225, 'poss-MEIN': 1226, 'poss-SEIN': 1227, 'qu-DU': 1228, 'qu-MOEGEN': 1229, 'qu-STEIGEN': 1230, 'unk': 1231}\n",
      "torch.Size([1, 74, 3, 260, 210]) torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "from phoenix_datasets import PhoenixVideoTextDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "\n",
    "curr_dir = os.getcwd()\n",
    "datapath = \"data/phoenix-2014.v3/phoenix2014-release/phoenix-2014-multisigner\"\n",
    "path = os.path.join(curr_dir, datapath)\n",
    "\n",
    "dtrain = PhoenixVideoTextDataset(\n",
    "    # your path to this folder, download it from official website first.\n",
    "    root=path,\n",
    "    split=\"train\",\n",
    "    p_drop=0.5,\n",
    "    random_drop=True,\n",
    ")\n",
    "\n",
    "\n",
    "def collate_add_blank(base_collate, batch):\n",
    "\n",
    "    batch = base_collate(batch)  # add one to all labels\n",
    "    batch[\"text\"] = [tens + 1 for tens in batch[\"text\"]]\n",
    "    batch = (\n",
    "        torch.stack(batch[\"video\"], 0),\n",
    "        torch.stack(batch[\"text\"], 0),\n",
    "    )\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "vocab = dtrain.vocab\n",
    "\n",
    "print(\"Vocab\", vocab)\n",
    "\n",
    "collation = lambda x: collate_add_blank(dtrain.collate_fn, x)\n",
    "\n",
    "dl = DataLoader(dtrain, collate_fn=collation, shuffle=True, batch_size=1)\n",
    "\n",
    "for batch in dl:\n",
    "    video, label = batch\n",
    "\n",
    "    print(video.shape, label.shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1231"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_inv = {(v + 1): k for k, v in vocab.table.items()}\n",
    "len(vocab_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = CustomVocab(vocab_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__OFF__ U UEBERFLUTUNG DANEBEN __EMOTION__ __EMOTION__ REGEN-PLUSPLUS MEINEN GEWESEN GEWESEN'"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(labels_to_text(label[0], vocab_inv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1140,  946,  950,  168, 1138, 1138,  752,  549,  341,  341])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get logits with 1.0 on each label for the first batch\n",
    "logits = torch.zeros(2, len(label[0]), len(vocab_inv) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HEUTE-ABEND'"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_inv[399]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set logits to 1.0 for each label\n",
    "logits[:, torch.arange(len(label[0])), label[0]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__OFF__ U UEBERFLUTUNG DANEBEN __EMOTION__ REGEN-PLUSPLUS MEINEN GEWESEN',\n",
       " '__OFF__ U UEBERFLUTUNG DANEBEN __EMOTION__ REGEN-PLUSPLUS MEINEN GEWESEN']"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV.string_from_logits(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1140,  946,  950,  168, 1138, 1138,  752,  549,  341,  341]])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__OFF__ U UEBERFLUTUNG DANEBEN __EMOTION__ __EMOTION__ REGEN-PLUSPLUS MEINEN GEWESEN GEWESEN'"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(labels_to_text(torch.argmax(logits, dim=-1)[0], vocab_inv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyctcdecode import build_ctcdecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(CV.vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n"
     ]
    }
   ],
   "source": [
    "decoder = build_ctcdecoder(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1232"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__PU__UEBER-UNTERUEBERSCHWEMMUNGDARAUF__OFF__REGION-PLUSPLUSMERKENGEWITTER-PLUSPLUS',\n",
       "  None,\n",
       "  [('__PU__UEBER-UNTERUEBERSCHWEMMUNGDARAUF__OFF__REGION-PLUSPLUSMERKENGEWITTER-PLUSPLUS',\n",
       "    (0, 10))],\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0))]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.decode_beams(logits.squeeze(0).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
