{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1775725,"sourceType":"datasetVersion","datasetId":1049383}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LIS With MobileViTs","metadata":{}},{"cell_type":"markdown","source":"## 1. Our MobileViTs","metadata":{}},{"cell_type":"code","source":"!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz\n!gunzip cc.de.300.vec.gz\n!pip install lightning torchmetrics einops av gensim\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-12-15T22:08:40.303467Z","iopub.execute_input":"2024-12-15T22:08:40.304065Z","iopub.status.idle":"2024-12-15T22:10:16.169948Z","shell.execute_reply.started":"2024-12-15T22:08:40.304020Z","shell.execute_reply":"2024-12-15T22:10:16.168333Z"},"trusted":true},"outputs":[{"name":"stdout","text":"--2024-12-15 22:08:41--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.96, 3.163.189.108, 3.163.189.51, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.96|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1278030050 (1.2G) [binary/octet-stream]\nSaving to: 'cc.de.300.vec.gz'\n\ncc.de.300.vec.gz    100%[===================>]   1.19G  47.7MB/s    in 26s     \n\n2024-12-15 22:09:07 (47.8 MB/s) - 'cc.de.300.vec.gz' saved [1278030050/1278030050]\n\nCollecting lightning\n  Downloading lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.6.0)\nCollecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting av\n  Downloading av-14.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\nRequirement already satisfied: gensim in /opt/conda/lib/python3.10/site-packages (4.3.3)\nRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.2)\nRequirement already satisfied: fsspec<2026.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.9.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.11.9)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\nRequirement already satisfied: torch<4.0,>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.4.0+cpu)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.4)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.12.2)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.4.0)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.4)\nCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n  Downloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (7.0.4)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.9.5)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (70.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.2)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=2.1.0->lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.7)\nDownloading lightning-2.4.0-py3-none-any.whl (810 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading av-14.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scipy, einops, av, lightning\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.14.1\n    Uninstalling scipy-1.14.1:\n      Successfully uninstalled scipy-1.14.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.3 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\ntsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed av-14.0.1 einops-0.8.0 lightning-2.4.0 scipy-1.13.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from gensim.models import KeyedVectors\nimport numpy as np\n\n# Percorso del file scaricato\nembedding_file = \"/kaggle/working/cc.de.300.vec\"\n\n# Carica gli embedding FastText pre-addestrati\nfasttext_model = KeyedVectors.load_word2vec_format(embedding_file, binary=False)\nspecial_words = ['<start>', '<end>', '<unk>']\nspecial_embeddings=[np.full(300, 1),np.full(300, 2), np.full(300, 3) ]\nfasttext_model.add_vectors(special_words,special_embeddings )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T22:10:16.172606Z","iopub.execute_input":"2024-12-15T22:10:16.173154Z","iopub.status.idle":"2024-12-15T22:17:55.985888Z","shell.execute_reply.started":"2024-12-15T22:10:16.173090Z","shell.execute_reply":"2024-12-15T22:17:55.982839Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!ls /kaggle/working","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.functional as F\nimport wandb\nfrom einops import rearrange\nimport torchvision\nfrom torch.utils.data import DataLoader\nimport os\nimport gc\nimport numpy as np\nfrom torchmetrics.text import WordErrorRate\nfrom tqdm import tqdm\nimport gzip\nimport pickle\nfrom lightning.pytorch.utilities.model_summary import summarize\nfrom lightning.fabric.utilities import measure_flops\nimport pdb","metadata":{"execution":{"iopub.status.busy":"2024-12-15T22:17:55.992156Z","iopub.execute_input":"2024-12-15T22:17:55.992878Z","iopub.status.idle":"2024-12-15T22:18:06.477353Z","shell.execute_reply.started":"2024-12-15T22:17:55.992818Z","shell.execute_reply":"2024-12-15T22:18:06.476178Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Optimize this impl:\n# https://github.com/chinhsuanwu/mobilevit-pytorch/blob/master/mobilevit.py\nimport pdb\n\n\nclass Conv2DBlock(nn.Module):\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        groups=1,\n        bias=True,\n        norm=True,\n        activation=True,\n        dropout=0.1,\n    ):\n        \"\"\"__init__ Constructor for Conv2DBlock\n\n        Parameters\n        ----------\n        in_channels : int\n            Number of input channels\n        out_channels : int\n            Number of output channels\n        kernel_size : int\n            Size of the kernel\n        stride : int\n            Stride of the convolutional layer\n        padding : int\n            Padding of the convolutional layer\n        groups : int\n            Number of groups\n        bias : bool\n            Whether to use bias\n        dropout : float\n            Dropout rate\n        \"\"\"\n\n        super(Conv2DBlock, self).__init__()\n\n        self.block = nn.Sequential()\n\n        self.block.add_module(\n            \"conv2d\",\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=padding,\n                groups=groups,\n                bias=bias,\n            ),\n        )\n\n        if norm:\n            self.block.add_module(\"norm\", nn.BatchNorm2d(out_channels))\n\n        if activation:\n            self.block.add_module(\"activation\", nn.SiLU())  # sigmoid(x) * x // SWISH\n\n        if dropout:\n            self.block.add_module(\"dropout\", nn.Dropout(dropout))\n\n        self.block = self.block\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass MobileBlockV2(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride, expand_ratio):\n        \"\"\"__init__ Constructor for MobileBlockV2\n\n\n        Parameters\n        ----------\n        in_channels : int\n            Number of input channels\n        out_channels : int\n            Number of output channels\n        stride : int\n            Stride of the convolutional layer\n        expand_ratio : int\n            Expansion ratio of the block\n        \"\"\"\n\n        super(MobileBlockV2, self).__init__()\n\n        assert stride in [1, 2], \"Stride must be either 1 or 2\"\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.stride = stride\n        self.expand_ratio = expand_ratio\n        self.hidden_dim = int(round(in_channels * expand_ratio))\n\n        self.mbv2 = nn.Sequential()\n        self.uses_inverse_residual = (\n            self.in_channels == self.out_channels and self.stride == 1\n        )\n\n        if self.expand_ratio == 1:\n            self.mbv2.add_module(\n                \"depthwise_3x3\",\n                Conv2DBlock(  # Depthwise Convolution\n                    in_channels=self.hidden_dim,\n                    out_channels=self.hidden_dim,\n                    kernel_size=3,\n                    stride=stride,\n                    padding=1,\n                    groups=self.hidden_dim,\n                    bias=False,\n                    norm=True,\n                    activation=True,\n                ),\n            )\n            self.mbv2.add_module(\n                \"pointwise-linear_1x1\",\n                Conv2DBlock(  # Pointwise-Linear Convolution\n                    in_channels=self.hidden_dim,\n                    out_channels=self.hidden_dim,\n                    kernel_size=1,\n                    stride=1,\n                    padding=0,\n                    groups=self.hidden_dim,\n                    bias=False,\n                    norm=True,\n                    activation=False,\n                ),\n            )\n        else:\n            self.mbv2.add_module(\n                \"pointwise_1x1\",\n                Conv2DBlock(  # Pointwise-Linear Convolution\n                    in_channels=in_channels,\n                    out_channels=self.hidden_dim,\n                    kernel_size=1,\n                    stride=1,\n                    padding=0,\n                    groups=1,\n                    bias=False,\n                    norm=True,\n                    activation=True,\n                ),\n            )\n            self.mbv2.add_module(\n                \"depthwise_3x3\",\n                Conv2DBlock(  # Depthwise Convolution\n                    in_channels=self.hidden_dim,\n                    out_channels=self.hidden_dim,\n                    kernel_size=3,\n                    stride=stride,\n                    padding=1,\n                    groups=self.hidden_dim,\n                    bias=False,\n                    norm=True,\n                    activation=True,\n                ),\n            )\n            self.mbv2.add_module(\n                \"pointwise-linear_1x1\",\n                Conv2DBlock(  # Pointwise-Linear Convolution\n                    in_channels=self.hidden_dim,\n                    out_channels=self.out_channels,\n                    kernel_size=1,\n                    stride=1,\n                    padding=0,\n                    groups=1,\n                    bias=False,\n                    norm=True,\n                    activation=False,\n                ),\n            )\n\n        self.mbv2 = self.mbv2\n\n    def forward(self, x):\n        if self.uses_inverse_residual:\n            return x + self.mbv2(x)\n        else:\n            return self.mbv2(x)\n\n\nclass MobileViTBlock(nn.Module):\n\n    def __init__(\n        self, hidden_dim, depth, channels, kernel_size, patch_size, mlp_dim, dropout=0.1\n    ):\n\n        super(MobileViTBlock, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.depth = depth\n        self.channels = channels\n        self.kernel_size = kernel_size\n        self.patch_size = patch_size\n        self.mlp_dim = mlp_dim\n        self.dropout = dropout\n\n        self.local_conv = nn.Sequential(\n            Conv2DBlock(\n                in_channels=channels,\n                out_channels=channels,\n                kernel_size=kernel_size,\n                stride=1,\n                padding=1,\n                norm=True,\n                activation=True,\n                bias=False,\n            ),\n            Conv2DBlock(\n                in_channels=channels,\n                out_channels=hidden_dim,\n                kernel_size=1,\n                stride=1,\n                padding=0,\n                norm=True,\n                activation=True,\n                bias=False,\n            ),\n        )\n\n        self.global_transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(\n                d_model=hidden_dim,\n                nhead=8,\n                dim_feedforward=mlp_dim,\n                dropout=dropout,\n                batch_first=True,\n                activation=\"gelu\",\n            ),\n            num_layers=depth,\n            norm=nn.LayerNorm(hidden_dim),\n        )\n\n        self.fusion_conv_preres = Conv2DBlock(\n            in_channels=hidden_dim,\n            out_channels=channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False,\n            norm=True,\n            activation=True,\n        )\n\n        self.fusion_conv_postres = Conv2DBlock(\n            in_channels=2 * channels,\n            out_channels=channels,\n            kernel_size=kernel_size,\n            stride=1,\n            padding=1,\n            bias=False,\n            norm=True,\n            activation=True,\n        )\n\n    def forward(self, x):\n\n        B, T, C, H, W = x.shape\n        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n        x_res = x.clone()\n\n        # local_repr\n        x = self.local_conv(x)\n\n        ph, pw = self.patch_size\n\n        # global_repr\n        _, _, h, w = x.shape\n        x = rearrange(  # reshape the image into patches for ViT input\n            x,\n            \"(b t) d (h ph) (w pw) -> (b h w) (t ph pw) d\",\n            ph=ph,\n            pw=pw,\n            b=B,\n            t=T,\n        )\n        x = self.global_transformer(x)\n        x = rearrange(\n            x,\n            \"(b h w) (t ph pw) d -> (b t) d (h ph) (w pw)\",\n            h=h // ph,\n            w=w // pw,\n            ph=ph,\n            pw=pw,\n            b=B,\n            t=T,\n        )\n\n        # fusion\n        x = self.fusion_conv_preres(x)\n        x = torch.cat([x, x_res], dim=1)\n        x = self.fusion_conv_postres(x)\n\n        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n\n        return x\n\n\nclass VideoDecoder(nn.Module):\n    def __init__(self, input_dim,hidden_dim, output_dim,vocab, num_layers=1, dropout=0.1):\n        super(VideoDecoder, self).__init__()\n        \n        # RNN o GRU o Transformer Decoder, scegli tu l'architettura\n        self.decoder = nn.TransformerDecoderLayer(\n            d_model=input_dim,\n            nhead=8,  # Numero di teste di attenzione, personalizzabile\n            batch_first=True,\n            dropout=dropout,\n            activation='gelu'  # Funzione di attivazione, puoi cambiarla in base alle esigenze\n        )\n        self.vocab =  vocab\n        self.embed=nn.Linear(300,320)\n        self.output_layer = nn.Linear(input_dim, output_dim)\n\n    def add_padding(self, sequences, max_len):\n        batch_size, seq_len = sequences.size()\n        if batch_size < max_len:\n            padding = max_len - batch_size\n            # Pad along the first dimension (batch dimension)\n            padded_sequences = torch.nn.functional.pad(\n                sequences, (0, 0, 0, padding), value=0  # Padding per la dimensione batch\n            )\n            # Create a 1D key_padding_mask: 1 for non-padded, 0 for padded\n            attention_mask = torch.cat([torch.ones(batch_size), torch.zeros(padding)])\n        else:\n            padded_sequences = sequences\n            attention_mask = torch.ones(batch_size)  # 1D mask for unpadded sequences\n    \n        return padded_sequences, attention_mask\n\n\n\n    def get_embeddings(self, ids_tensor):\n        # Crea un embedding speciale per OOV utilizzando l'embedding di <unk>\n        unk_embedding = torch.tensor(fasttext_model['<unk>'])\n    \n        # Decodifica gli ID nel vocabolario\n        decoded_words = self.vocab.decode_from_ids(ids_tensor.squeeze().tolist())\n    \n        # Genera gli embedding\n        embeddings = []\n        for word in decoded_words:\n            if word in fasttext_model:\n                embeddings.append(torch.tensor(fasttext_model[word]))\n            else:\n                embeddings.append(unk_embedding)\n        \n        return torch.stack(embeddings, dim=0)\n        \n    def forward(self, tgt, memory):\n        pdb.set_trace()\n        tgt = self.get_embeddings(tgt)\n        tgt = self.embed(tgt)\n        tgt, padding_mask = self.add_padding(tgt, 128)\n        mask = torch.tril(torch.ones(128,128))\n        # encoder_output avrà dimensione (batch_size, seq_length, input_dim)\n        output = self.decoder(tgt=tgt.to(dtype=torch.float32), memory=memory, tgt_mask=mask, \\\n                              tgt_key_padding_mask=~padding_mask.bool())\n        output = self.output_layer(output)\n        \n        return output\n\n\nclass MobileViT(pl.LightningModule):\n\n    def __init__(\n        self,\n        dims,\n        conv_channels,\n        num_classes,\n        vocabulary,\n        expand_ratio=4,\n        patch_size=(2, 2),\n    ):\n        super(MobileViT, self).__init__()\n\n        self.dims = dims\n        self.conv_channels = conv_channels\n        self.num_classes = num_classes\n        self.expand_ratio = expand_ratio\n        self.patch_size = patch_size\n        self.kernel_size = 3\n\n        L = [2, 4, 3]\n\n        self.in_conv = Conv2DBlock(\n            in_channels=3,\n            out_channels=conv_channels[0],\n            kernel_size=self.kernel_size,\n            stride=2,\n            padding=1,\n            norm=True,\n            activation=True,\n            bias=False,\n        )\n\n        self.mv2_blocks = nn.ModuleList([])\n\n        self.mv2_blocks.append(\n            MobileBlockV2(\n                conv_channels[0], conv_channels[1], 1, expand_ratio=expand_ratio\n            )\n        )\n        self.mv2_blocks.append(\n            MobileBlockV2(\n                conv_channels[1], conv_channels[2], 2, expand_ratio=expand_ratio\n            )\n        )\n        self.mv2_blocks.append(\n            MobileBlockV2(\n                conv_channels[2], conv_channels[3], 1, expand_ratio=expand_ratio\n            )\n        )\n        self.mv2_blocks.append(\n            MobileBlockV2(\n                conv_channels[2], conv_channels[3], 1, expand_ratio=expand_ratio\n            )\n        )\n        self.mv2_blocks.append(\n            MobileBlockV2(\n                conv_channels[3], conv_channels[4], 2, expand_ratio=expand_ratio\n            )\n        )\n        self.mv2_blocks.append(\n            MobileBlockV2(\n                conv_channels[5], conv_channels[6], 2, expand_ratio=expand_ratio\n            )\n        )\n        self.mv2_blocks.append(\n            MobileBlockV2(\n                conv_channels[7], conv_channels[8], 2, expand_ratio=expand_ratio\n            )\n        )\n\n        self.mvit_blocks = nn.ModuleList([])\n\n        self.mvit_blocks.append(\n            MobileViTBlock(\n                dims[0],\n                L[0],\n                conv_channels[5],\n                self.kernel_size,\n                patch_size,\n                int(dims[0] * 2),\n            )\n        )\n        self.mvit_blocks.append(\n            MobileViTBlock(\n                dims[1],\n                L[1],\n                conv_channels[7],\n                self.kernel_size,\n                patch_size,\n                int(dims[1] * 4),\n            )\n        )\n        self.mvit_blocks.append(\n            MobileViTBlock(\n                dims[2],\n                L[2],\n                conv_channels[9],\n                self.kernel_size,\n                patch_size,\n                int(dims[2] * 4),\n            )\n        )\n\n        self.final_pw = Conv2DBlock(\n            in_channels=conv_channels[-2],\n            out_channels=conv_channels[-1],\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            groups=1,\n            bias=False,\n            norm=True,\n            activation=False,\n        )\n\n        self.decoder = VideoDecoder(input_dim=320, hidden_dim=conv_channels[-1], output_dim=len(vocabulary), vocab = vocabulary)\n\n        \n\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.classifier = nn.Linear(conv_channels[-1], num_classes, bias=False)\n\n        ## Training-related members\n        self.criterion = nn.CrossEntropyLoss()\n        self.wer = WordErrorRate()\n\n        self.vocabulary = vocabulary\n\n        self.apply(self.init_weights)  # Initialize weights\n\n    def init_weights(self, m):\n\n        if type(m) == nn.Conv2d:\n            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif type(m) == nn.BatchNorm2d:\n            nn.init.constant_(m.weight, 1)\n            nn.init.constant_(m.bias, 0)\n        elif type(m) == nn.Linear:\n            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    \n\n    def forward(self, x, tgt):\n        \n    \n\n        B, T, C, H, W = x.shape\n        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n\n        x = self.in_conv(x)\n\n        for i in range(5):\n            x = self.mv2_blocks[i](x)\n\n        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n        x = self.mvit_blocks[0](x)\n\n        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n        x = self.mv2_blocks[5](x)\n\n        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n        x = self.mvit_blocks[1](x)\n\n        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n        x = self.mv2_blocks[6](x)\n\n        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n        x = self.mvit_blocks[2](x)\n\n        x = rearrange(x, \"b t c h w -> (b t) c h w\", b=B, t=T)\n        pdb.set_trace()\n        x = self.final_pw(x)\n        x = self.pool(x)\n        x = x.flatten(1)\n        x = self.decoder(tgt,x )\n        \n        return x\n\n    def step(self, batch, phase):\n        x, y = batch\n\n        B, T, C, H, W = x.shape\n        B, N = y.shape\n\n        # assume padding is done in the dataloader\n        input_lengths = torch.full((B,), T, dtype=torch.long)\n        target_lengths = torch.full((B,), N, dtype=torch.long)\n\n        logits = self(x, y)\n        pdb.set_trace()\n\n        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n        # sum one to y as 0 is the blank token in our representation\n\n        out_sentence = torch.argmax(log_probs, dim=-1)\n        \n        loss = self.criterion(out_sentence, y)\n\n        self.log(f\"{phase}/loss\", loss, prog_bar=True)\n\n        if phase != \"train\":\n            word_error_rate = self.calculate_wer(logits, y)\n            self.log(f\"{phase}/wer\", word_error_rate, prog_bar=True)\n\n        return loss\n\n    def calculate_wer(self, logits, y):\n        pred_strings = self.string_from_logits(logits)\n        target_strings = self.string_from_ground_truth(y)\n\n        pred_strings = [\" \".join(pred) for pred in pred_strings]\n        target_strings = [\" \".join(target) for target in target_strings]\n\n        return self.wer(pred_strings, target_strings)\n\n    def string_from_logits(self, logits):\n        decoded = self.ctc_decode(logits)\n\n        return self.string_from_ground_truth(decoded)\n\n    def ctc_decode(self, logits, blank=0):\n        logits = torch.argmax(logits, dim=-1)  # Take the most probable class\n        decoded = []\n        for seq in logits.T:  # Iterate over batch\n            result = []\n            prev_token = blank\n            for token in seq:\n                if token != prev_token and token != blank:\n                    result.append(token.item())\n                prev_token = token\n            decoded.append(result)\n        return decoded\n\n    def string_from_ground_truth(self, y):\n        target_strings = []\n        for target in y:\n            if isinstance(target, torch.Tensor):\n                target = target.tolist()\n            target_strings.append(self.vocabulary.decode_from_ids(target))\n        return target_strings\n\n    def training_step(self, batch, batch_idx):\n        return self.step(batch, \"train\")\n\n    def validation_step(self, batch, batch_idx):\n        return self.step(batch, \"val\")\n\n    def test_step(self, batch, batch_idx):\n        return self.step(batch, \"test\")\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(\n            self.parameters(), lr=2e-4, amsgrad=True, weight_decay=1e-4\n        )\n        scheduler = {\n            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n                optimizer, T_0=10, T_mult=2, eta_min=1e-6\n            ),\n            \"interval\": \"step\",\n            \"frequency\": 32,\n        }\n\n        return [optimizer], [scheduler]","metadata":{"execution":{"iopub.status.busy":"2024-12-15T23:12:38.131222Z","iopub.execute_input":"2024-12-15T23:12:38.132041Z","iopub.status.idle":"2024-12-15T23:12:38.238333Z","shell.execute_reply.started":"2024-12-15T23:12:38.131959Z","shell.execute_reply":"2024-12-15T23:12:38.234964Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# 2. Our Datamodules","metadata":{}},{"cell_type":"code","source":"from dataclasses import dataclass\nimport random\n\n\n@dataclass\nclass PhoenixFiles:\n    train: str\n    dev: str\n    test: str\n\n\n@dataclass\nclass PhoenixDataHyperparameters:\n    num_workers: int\n    batch_size: int\n    sample_random: bool\n    sample_uniform: bool\n    sample_percentage: float\n    transforms: torchvision.transforms.Compose\n\n\nclass PhoenixVocabulary:\n\n    def __init__(self, filenames: PhoenixFiles):\n        self.filenames = filenames\n        self.vocab = self.build_vocab()\n        self.vocab_inversed = {v: k for k, v in self.vocab.items()}\n\n    def build_vocab(self):\n        vocab = {}\n\n        files = [self.filenames.train, self.filenames.dev, self.filenames.test]\n\n        for file in files:\n            with gzip.open(file, \"rb\") as f:\n                annotations = pickle.load(f)\n\n            for ann in tqdm(\n                random.sample(annotations, len(annotations)),\n                desc=f\"Extracting tokens from {os.path.basename(file)}\",\n            ):\n\n                # random.sample shuffles the strings, this improves token distribution (hopefully improves training)\n                for word in random.sample(\n                    ann[\"gloss\"].split(), len(ann[\"gloss\"].split())\n                ):\n                    if word not in vocab:\n                        vocab[word] = len(vocab) + 2\n\n        vocab[\"<start>\"] = 0\n        vocab[\"<end>\"] = 1\n\n        return vocab\n\n    def __len__(self):\n        return len(self.vocab)\n\n    def __getitem__(self, idx):\n        return self.vocab[idx]\n\n    def encode_as_ids(self, sentence):\n        return [self.vocab[word] for word in sentence]\n\n    def decode_from_ids(self, ids):\n        return [self.vocab_inversed[tkn] for tkn in ids]\n\n\nclass Phoenix14TDataset(torch.utils.data.Dataset):\n\n    def __init__(self, path: str, vocabulary: PhoenixVocabulary, hyperparameters):\n        self.metadata_path = path\n        self.base_dir = os.path.dirname(path)\n        self.data = None\n        self.vocab: PhoenixVocabulary = vocabulary\n        self.data_hparams = hyperparameters\n        self.transform = hyperparameters.transforms\n\n        assert (\n            self.data_hparams.sample_random != self.data_hparams.sample_uniform\n        ), \"Both random and uniform sampling cannot be enabled at the same time\"\n\n    def setup(self, stage):\n        with gzip.open(self.metadata_path, \"rb\") as f:\n            self.data = pickle.load(f)\n\n        self.signers = [d[\"signer\"] for d in self.data]\n        self.video_names = [d[\"name\"] for d in self.data]\n        self.annotations = [d[\"gloss\"] for d in self.data]\n        self.text = [d[\"text\"] for d in self.data]\n        self.targets = [\n            self.vocab.encode_as_ids([\"<start>\"]+[token for token in ann.split()]+[\"<end>\"])\n            for ann in tqdm(\n                self.annotations,\n                desc=(\n                    f\"Encoding annotations for {stage}\"\n                    if stage\n                    else \"Encoding annotations\"\n                ),\n            )\n        ]\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n\n        video_path = os.path.join(\n            self.base_dir, \"videos_phoenix\", \"videos\", self.video_names[idx] + \".mp4\"\n        )\n\n        video = torchvision.io.read_video(\n            video_path,\n            pts_unit=\"sec\",\n            output_format=\"TCHW\",\n        )[0]\n        \n\n        if self.data_hparams.sample_random:\n            indices = sorted(random.sample(\n                range(len(video)), len(video) * self.data_hparams.sample_percentage\n            ))\n        elif self.data_hparams.sample_uniform:\n            indices = np.linspace(\n                0,\n                len(video) - 1,\n                int(len(video) * self.data_hparams.sample_percentage),\n            ).tolist()\n        else:\n            indices = list(range(len(video)))\n            \n\n        T = len(indices)\n        video_tensor = video.unsqueeze(0)\n        video_tensor = rearrange(\n                self.transform(\n                    rearrange(video_tensor[:, indices], \"b t c h w -> (b t) c h w\")\n                ), \"(b t) c h w -> b t c h w\", c=3, t=T\n        ).squeeze(0)\n\n        return video_tensor, torch.tensor(self.targets[idx], dtype=torch.long)\n\n    def get_metadata(self, idx):\n        return {\n            \"signer\": self.signers[idx],\n            \"video_name\": self.video_names[idx],\n            \"annotation\": self.annotations[idx],\n            \"text\": self.text[idx],\n        }\n\n\nclass Phoenix14TDatamodule(pl.LightningDataModule):\n\n    def __init__(\n        self,\n        files: PhoenixFiles,\n        vocabulary: PhoenixVocabulary,\n        num_workers: int,\n        data_hyperparameters: PhoenixDataHyperparameters,\n    ):\n        super(Phoenix14TDatamodule, self).__init__()\n        self.metadata_paths = files\n        self.vocabulary = vocabulary\n        self.workers = num_workers\n        self.train = None\n        self.dev = None\n        self.test = None\n        self.data_hparams = data_hyperparameters\n\n        assert (\n            self.data_hparams.sample_random != self.data_hparams.sample_uniform\n        ), \"Both random and uniform sampling cannot be enabled at the same time\"\n\n    def setup(self, stage):\n        self.train = Phoenix14TDataset(\n            self.metadata_paths.train, self.vocabulary, self.data_hparams\n        )\n        self.train.setup(\"train\")\n\n        self.dev = Phoenix14TDataset(\n            self.metadata_paths.dev, self.vocabulary, self.data_hparams\n        )\n        self.dev.setup(\"dev\")\n\n        self.test = Phoenix14TDataset(\n            self.metadata_paths.test, self.vocabulary, self.data_hparams\n        )\n        self.test.setup(\"test\")\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train,\n            shuffle=True,\n            batch_size=1,\n            pin_memory=True,\n            num_workers=self.workers,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.dev,\n            shuffle=False,\n            batch_size=1,\n            pin_memory=True,\n            num_workers=self.workers,\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            self.test,\n            shuffle=False,\n            batch_size=1,\n            pin_memory=True,\n            num_workers=self.workers,\n        )\n        \n\n\ndef animate_torch_tensor(video):\n    \n    # Adapted from https://stackoverflow.com/a/57275596\n    \n    %matplotlib inline\n    from matplotlib import pyplot as plt\n    from matplotlib import animation\n    from IPython.display import HTML\n\n    \n    video_np = video.numpy().transpose(0, 2, 3, 1)\n\n    fig = plt.figure()\n    im = plt.imshow(video_np[0,:,:,:])\n\n    plt.close() # this is required to not display the generated image\n\n    def init():\n        im.set_data(video_np[0,:,:,:])\n\n    def animate(i):\n        im.set_data(video_np[i,:,:,:])\n        return im\n\n    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video_np.shape[0],\n                                interval=1000//6)\n    return HTML(anim.to_html5_video())","metadata":{"execution":{"iopub.status.busy":"2024-12-15T22:18:06.552968Z","iopub.execute_input":"2024-12-15T22:18:06.553553Z","iopub.status.idle":"2024-12-15T22:18:06.600231Z","shell.execute_reply.started":"2024-12-15T22:18:06.553474Z","shell.execute_reply":"2024-12-15T22:18:06.598640Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def xxs_mvit(vocab):\n    return MobileViT(\n        dims=[64, 80, 96],\n        conv_channels=[16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 320],\n        num_classes=len(vocab),\n        vocabulary=vocab,\n        expand_ratio=2,\n        patch_size=(2, 2),\n    )\n\n\ndef xs_mvit(vocab):\n    return MobileViT(\n        dims=[96, 120, 144],\n        conv_channels=[16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384],\n        num_classes=len(vocab),\n        vocabulary=vocab,\n        expand_ratio=4,\n        patch_size=(4, 4),\n    )\n\n\ndef s_mvit(vocab):\n    return MobileViT(\n        dims=[144, 192, 240],\n        conv_channels=[16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 640],\n        num_classes=len(vocab),\n        vocabulary=vocab,\n        expand_ratio=4,\n        patch_size=(8, 8),\n    )","metadata":{"execution":{"iopub.status.busy":"2024-12-15T22:18:06.601889Z","iopub.execute_input":"2024-12-15T22:18:06.602380Z","iopub.status.idle":"2024-12-15T22:18:06.624060Z","shell.execute_reply.started":"2024-12-15T22:18:06.602331Z","shell.execute_reply":"2024-12-15T22:18:06.622445Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import wandb\n\n# UNCOMMENT THIS TO LOG TO WANDB\n# from kaggle_secrets import UserSecretsClient\n\n# user_secrets = UserSecretsClient()\n# secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n\n\nwandb.login(key=\"aaf831dabc88d936d4e6b439b798bb4cb42814ea\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"files = PhoenixFiles(\n    train=\"/kaggle/input/phoenix14t.pami0.train.annotations_only.gzip\",\n    dev=\"/kaggle/input/phoenix14t.pami0.dev.annotations_only.gzip\",\n    test=\"/kaggle/input/phoenix14t.pami0.test.annotations_only.gzip\",\n)\n\ntransforms = torchvision.transforms.Compose(\n    [\n        torchvision.transforms.Resize(\n            (192, 256), interpolation=torchvision.transforms.InterpolationMode.BICUBIC\n        ),  # from uint8 to float32\n        lambda x: x / 255.0,\n    ]\n)\n\ndata_hparams = PhoenixDataHyperparameters(\n    num_workers=3,\n    batch_size=1,\n    sample_random=False,\n    sample_uniform=True,\n    sample_percentage=1 / 3,\n    transforms=transforms,\n)\n\nvocab = PhoenixVocabulary(files)\n\ndm = Phoenix14TDatamodule(files, vocab, 3, data_hparams)","metadata":{"execution":{"iopub.status.busy":"2024-12-15T22:18:06.625981Z","iopub.execute_input":"2024-12-15T22:18:06.626553Z","iopub.status.idle":"2024-12-15T22:18:06.795863Z","shell.execute_reply.started":"2024-12-15T22:18:06.626473Z","shell.execute_reply":"2024-12-15T22:18:06.794466Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Extracting tokens from phoenix14t.pami0.train.annotations_only.gzip: 100%|██████████| 7096/7096 [00:00<00:00, 116722.28it/s]\nExtracting tokens from phoenix14t.pami0.dev.annotations_only.gzip: 100%|██████████| 519/519 [00:00<00:00, 82977.96it/s]\nExtracting tokens from phoenix14t.pami0.test.annotations_only.gzip: 100%|██████████| 642/642 [00:00<00:00, 105181.17it/s]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"dm.setup(\"fit\")","metadata":{"execution":{"iopub.status.busy":"2024-12-15T22:18:06.797379Z","iopub.execute_input":"2024-12-15T22:18:06.797793Z","iopub.status.idle":"2024-12-15T22:18:06.882727Z","shell.execute_reply.started":"2024-12-15T22:18:06.797755Z","shell.execute_reply":"2024-12-15T22:18:06.881316Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Encoding annotations for train: 100%|██████████| 7096/7096 [00:00<00:00, 246994.03it/s]\nEncoding annotations for dev: 100%|██████████| 519/519 [00:00<00:00, 260419.16it/s]\nEncoding annotations for test: 100%|██████████| 642/642 [00:00<00:00, 266740.28it/s]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Check out a video","metadata":{}},{"cell_type":"code","source":"animate_torch_tensor(dm.train[0][0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# flush cuda cache\ntorch.cuda.empty_cache()\n\nimport lightning.pytorch as lp\n\n#wandb.init(project=\"mobilevit\")\n\n# setup training and wandb\n\nBATCH_SIZE = 1  # we don't do padding, so batch size must be 1\n\n#wandb_logger = lp.loggers.WandbLogger()\n\nmodel = xxs_mvit(vocab)\n\n\ntrainer = pl.Trainer(\n    max_epochs=100,\n    accelerator=\"cpu\",#cuda\n    #logger=wandb_logger,\n    callbacks=[\n        lp.callbacks.ModelCheckpoint(\n            monitor=\"val/loss\",\n            filename=\"best_model\",\n            save_top_k=1,\n            mode=\"min\",\n        ),\n        lp.callbacks.EarlyStopping(\n            monitor=\"val/loss\",\n            patience=3,\n            mode=\"min\",\n        ),\n        lp.callbacks.LearningRateMonitor(logging_interval=\"step\"),\n    ],\n    limit_train_batches=0.25,\n    accumulate_grad_batches=32,\n)\n\n#wandb_logger.watch(model, log_graph=False)\n\ntrainer.fit(model, datamodule=dm)\n\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-12-15T23:12:44.753374Z","iopub.execute_input":"2024-12-15T23:12:44.755041Z"},"trusted":true},"outputs":[{"name":"stderr","text":"INFO: GPU available: False, used: False\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\n\nEncoding annotations for train: 100%|██████████| 7096/7096 [00:00<00:00, 281253.25it/s]\n\nEncoding annotations for dev: 100%|██████████| 519/519 [00:00<00:00, 156124.49it/s]\n\nEncoding annotations for test: 100%|██████████| 642/642 [00:00<00:00, 98823.52it/s]\nINFO: \n  | Name        | Type              | Params | Mode \n----------------------------------------------------------\n0 | in_conv     | Conv2DBlock       | 464    | train\n1 | mv2_blocks  | ModuleList        | 45.7 K | train\n2 | mvit_blocks | ModuleList        | 1.1 M  | train\n3 | final_pw    | Conv2DBlock       | 26.2 K | train\n4 | decoder     | VideoDecoder      | 2.6 M  | train\n5 | pool        | AdaptiveAvgPool2d | 0      | train\n6 | classifier  | Linear            | 357 K  | train\n7 | criterion   | CrossEntropyLoss  | 0      | train\n8 | wer         | WordErrorRate     | 0      | train\n----------------------------------------------------------\n4.1 M     Trainable params\n0         Non-trainable params\n4.1 M     Total params\n16.459    Total estimated model params size (MB)\n344       Modules in train mode\n0         Modules in eval mode\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"366e09a5a7e04765a7a63d0fc3dabbd2"}},"metadata":{}},{"name":"stdout","text":"> \u001b[0;32m/tmp/ipykernel_24/13045280.py\u001b[0m(548)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n\u001b[0;32m    546 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b t c h w -> (b t) c h w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    547 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 548 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_pw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  c\n"},{"name":"stdout","text":"> \u001b[0;32m/tmp/ipykernel_24/13045280.py\u001b[0m(360)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n\u001b[0;32m    358 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 360 \u001b[0;31m        \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361 \u001b[0;31m        \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362 \u001b[0;31m        \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  c\n"},{"name":"stdout","text":"> \u001b[0;32m/tmp/ipykernel_24/13045280.py\u001b[0m(568)\u001b[0;36mstep\u001b[0;34m()\u001b[0m\n\u001b[0;32m    566 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 568 \u001b[0;31m        \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569 \u001b[0;31m        \u001b[0;31m# sum one to y as 0 is the blank token in our representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  y\n"},{"name":"stdout","text":"tensor([[  0, 155,  61, 105,   1]])\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  logits\n"},{"name":"stdout","text":"tensor([[ 0.1599, -1.4473,  0.3679,  ...,  0.4613, -0.5852,  0.5374],\n        [ 0.1972,  0.6829,  0.8834,  ...,  2.6309,  0.4268,  1.4334],\n        [ 0.2453,  0.4888,  0.5236,  ...,  2.3149, -0.1181,  1.7953],\n        ...,\n        [ 0.5002,  0.6139,  0.4048,  ...,  2.6128, -0.1727,  1.3125],\n        [ 0.5002,  0.6139,  0.4048,  ...,  2.6128, -0.1727,  1.3125],\n        [ 0.5002,  0.6139,  0.4048,  ...,  2.6128, -0.1727,  1.3125]])\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  logits.shape\n"},{"name":"stdout","text":"torch.Size([128, 1117])\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  n\n"},{"name":"stdout","text":"> \u001b[0;32m/tmp/ipykernel_24/13045280.py\u001b[0m(571)\u001b[0;36mstep\u001b[0;34m()\u001b[0m\n\u001b[0;32m    569 \u001b[0;31m        \u001b[0;31m# sum one to y as 0 is the blank token in our representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 571 \u001b[0;31m        \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{phase}/loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprog_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  torch.argmax(log_probs)\n"},{"name":"stdout","text":"tensor(2231)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  torch.argmax(log_probs, dim=-1)\n"},{"name":"stdout","text":"tensor([ 332, 1114,  852,  295,  332, 1114, 1114, 1114, 1114, 1114, 1114, 1114,\n        1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114,\n        1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114,\n        1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114,\n        1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114,\n        1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114,\n        1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114,\n        1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114,\n        1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114,\n        1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114,\n        1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114])\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"xxs_kwargs = {\n    \"dims\": [64, 80, 96],\n    \"conv_channels\": [16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 320],\n    \"num_classes\": len(vocab),\n    \"vocabulary\": vocab,\n    \"expand_ratio\": 2,\n    \"patch_size\": (2, 2),\n}\nget_vector\nmodel = xxs_mvit(vocab)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n    model = model.cuda()\n    x, y = dm.train[53]\n    x = x.cuda().unsqueeze(0)\n\n    logits = model(x)\n    decoded = model.ctc_decode(logits)\n    print(torch.tensor(decoded).min())\n\n    print(\"Predicted:\", vocab.decode_from_ids(decoded[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(\" \".join(vocab.decode_from_ids(y.tolist())))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}