{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIS With MobileViTs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Our MobileViTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T22:30:11.866555Z",
     "iopub.status.busy": "2024-12-16T22:30:11.866159Z",
     "iopub.status.idle": "2024-12-16T22:30:19.974615Z",
     "shell.execute_reply": "2024-12-16T22:30:19.973848Z",
     "shell.execute_reply.started": "2024-12-16T22:30:11.866514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import wandb\n",
    "from einops import rearrange\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "from torchmetrics.text import WordErrorRate\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "import pickle\n",
    "from lightning.pytorch.utilities.model_summary import summarize\n",
    "from lightning.fabric.utilities import measure_flops\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following implementation is partially based on the work found in https://github.com/chinhsuanwu/mobilevit-pytorch/blob/master/mobilevit.py, which is a PyTorch implementation of the MobileViT model. Our impl includes temporal attention mechanisms and a decoder head for sequence prediction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T22:49:39.623798Z",
     "iopub.status.busy": "2024-12-16T22:49:39.623429Z",
     "iopub.status.idle": "2024-12-16T22:49:39.664329Z",
     "shell.execute_reply": "2024-12-16T22:49:39.663462Z",
     "shell.execute_reply.started": "2024-12-16T22:49:39.623764Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Optimize this impl:\n",
    "# https://github.com/chinhsuanwu/mobilevit-pytorch/blob/master/mobilevit.py\n",
    "\n",
    "\n",
    "class Conv2DBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        groups=1,\n",
    "        bias=True,\n",
    "        norm=True,\n",
    "        activation=True,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        \"\"\"__init__ Constructor for Conv2DBlock\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            Number of input channels\n",
    "        out_channels : int\n",
    "            Number of output channels\n",
    "        kernel_size : int\n",
    "            Size of the kernel\n",
    "        stride : int\n",
    "            Stride of the convolutional layer\n",
    "        padding : int\n",
    "            Padding of the convolutional layer\n",
    "        groups : int\n",
    "            Number of groups\n",
    "        bias : bool\n",
    "            Whether to use bias\n",
    "        dropout : float\n",
    "            Dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super(Conv2DBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential()\n",
    "\n",
    "        self.block.add_module(\n",
    "            \"conv2d\",\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                groups=groups,\n",
    "                bias=bias,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if norm:\n",
    "            self.block.add_module(\"norm\", nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        if activation:\n",
    "            self.block.add_module(\"activation\", nn.SiLU())  # sigmoid(x) * x // SWISH\n",
    "\n",
    "        if dropout:\n",
    "            self.block.add_module(\"dropout\", nn.Dropout2d(dropout))\n",
    "\n",
    "        self.block = self.block\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class MobileBlockV2(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride, expand_ratio):\n",
    "        \"\"\"__init__ Constructor for MobileBlockV2\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            Number of input channels\n",
    "        out_channels : int\n",
    "            Number of output channels\n",
    "        stride : int\n",
    "            Stride of the convolutional layer\n",
    "        expand_ratio : int\n",
    "            Expansion ratio of the block\n",
    "        \"\"\"\n",
    "\n",
    "        super(MobileBlockV2, self).__init__()\n",
    "\n",
    "        assert stride in [1, 2], \"Stride must be either 1 or 2\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.hidden_dim = int(round(in_channels * expand_ratio))\n",
    "\n",
    "        self.mbv2 = nn.Sequential()\n",
    "        self.uses_inverse_residual = (\n",
    "            self.in_channels == self.out_channels and self.stride == 1\n",
    "        )\n",
    "\n",
    "        if self.expand_ratio == 1:\n",
    "            self.mbv2.add_module(\n",
    "                \"depthwise_3x3\",\n",
    "                Conv2DBlock(  # Depthwise Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=3,\n",
    "                    stride=stride,\n",
    "                    padding=1,\n",
    "                    groups=self.hidden_dim,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                ),\n",
    "            )\n",
    "            self.mbv2.add_module(\n",
    "                \"pointwise-linear_1x1\",\n",
    "                Conv2DBlock(  # Pointwise-Linear Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    groups=self.hidden_dim,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=False,\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            self.mbv2.add_module(\n",
    "                \"pointwise_1x1\",\n",
    "                Conv2DBlock(  # Pointwise-Linear Convolution\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    groups=1,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                ),\n",
    "            )\n",
    "            self.mbv2.add_module(\n",
    "                \"depthwise_3x3\",\n",
    "                Conv2DBlock(  # Depthwise Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=3,\n",
    "                    stride=stride,\n",
    "                    padding=1,\n",
    "                    groups=self.hidden_dim,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                ),\n",
    "            )\n",
    "            self.mbv2.add_module(\n",
    "                \"pointwise-linear_1x1\",\n",
    "                Conv2DBlock(  # Pointwise-Linear Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    groups=1,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=False,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        self.mbv2 = self.mbv2\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.uses_inverse_residual:\n",
    "            return x + self.mbv2(x)\n",
    "        else:\n",
    "            return self.mbv2(x)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        dropout: float = 0.1,\n",
    "        max_len: int = 5000,\n",
    "        batch_first: bool = False,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.batch_first:\n",
    "            x = x + self.pe[:, : x.size(1)]\n",
    "        else:\n",
    "            x = x.transpose(0, 1) + self.pe[:, : x.size(0)]\n",
    "            x = x.transpose(0, 1)\n",
    "\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class MobileViTBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, hidden_dim, depth, channels, kernel_size, patch_size, mlp_dim, dropout=0.2\n",
    "    ):\n",
    "\n",
    "        super(MobileViTBlock, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.depth = depth\n",
    "        self.channels = channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.patch_size = patch_size\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.local_conv = nn.Sequential(\n",
    "            Conv2DBlock(\n",
    "                in_channels=channels,\n",
    "                out_channels=channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                norm=True,\n",
    "                activation=True,\n",
    "                bias=True,\n",
    "            ),\n",
    "            Conv2DBlock(\n",
    "                in_channels=channels,\n",
    "                out_channels=hidden_dim,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                norm=True,\n",
    "                activation=True,\n",
    "                bias=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.global_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_dim,\n",
    "                nhead=8,\n",
    "                dim_feedforward=mlp_dim,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "                activation=\"gelu\",\n",
    "            ),\n",
    "            num_layers=depth,\n",
    "            norm=nn.LayerNorm(hidden_dim),\n",
    "        )\n",
    "\n",
    "        self.fusion_conv_preres = Conv2DBlock(\n",
    "            in_channels=hidden_dim,\n",
    "            out_channels=channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "            norm=True,\n",
    "            activation=True,\n",
    "        )\n",
    "\n",
    "        self.fusion_conv_postres = Conv2DBlock(\n",
    "            in_channels=2 * channels,\n",
    "            out_channels=channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias=True,\n",
    "            norm=True,\n",
    "            activation=True,\n",
    "        )\n",
    "\n",
    "        self.temporal_positional_encoding = PositionalEncoding(\n",
    "            hidden_dim, dropout, max_len=500, batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n",
    "        x_res = x.clone()\n",
    "\n",
    "        # local_repr\n",
    "        x = self.local_conv(x)\n",
    "\n",
    "        ph, pw = self.patch_size\n",
    "\n",
    "        # global_repr\n",
    "        _, _, h, w = x.shape\n",
    "        x = rearrange(  # reshape the image into patches for ViT input\n",
    "            x,\n",
    "            \"(b t) d (h ph) (w pw) -> (b h w) (t ph pw) d\",\n",
    "            ph=ph,\n",
    "            pw=pw,\n",
    "            b=B,\n",
    "            t=T,\n",
    "        )\n",
    "        x = self.temporal_positional_encoding(x)\n",
    "        x = self.global_transformer(x)\n",
    "        x = rearrange(\n",
    "            x,\n",
    "            \"(b h w) (t ph pw) d -> (b t) d (h ph) (w pw)\",\n",
    "            h=h // ph,\n",
    "            w=w // pw,\n",
    "            ph=ph,\n",
    "            pw=pw,\n",
    "            b=B,\n",
    "            t=T,\n",
    "        )\n",
    "\n",
    "        # fusion\n",
    "        x = self.fusion_conv_preres(x)\n",
    "        x = torch.cat([x, x_res], dim=1)\n",
    "        x = self.fusion_conv_postres(x)\n",
    "\n",
    "        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MobileViT(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dims,\n",
    "        conv_channels,\n",
    "        num_classes,\n",
    "        vocabulary,\n",
    "        expand_ratio=4,\n",
    "        patch_size=(2, 2),\n",
    "    ):\n",
    "        super(MobileViT, self).__init__()\n",
    "\n",
    "        self.dims = dims\n",
    "        self.conv_channels = conv_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.patch_size = patch_size\n",
    "        self.kernel_size = 3\n",
    "        self.vocabulary = vocabulary\n",
    "        self.vocab_size = len(vocabulary)  # Include <eos> token if needed\n",
    "        self.eos_token = self.vocabulary[\"<eos>\"]\n",
    "\n",
    "        L = [2, 4, 3]\n",
    "\n",
    "        self.in_conv = Conv2DBlock(\n",
    "            in_channels=3,\n",
    "            out_channels=conv_channels[0],\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            norm=True,\n",
    "            activation=True,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.mv2_blocks = nn.ModuleList(\n",
    "            [\n",
    "                MobileBlockV2(\n",
    "                    conv_channels[0], conv_channels[1], 2, expand_ratio=expand_ratio\n",
    "                ),\n",
    "                MobileBlockV2(\n",
    "                    conv_channels[1], conv_channels[2], 2, expand_ratio=expand_ratio\n",
    "                ),\n",
    "                MobileBlockV2(\n",
    "                    conv_channels[2], conv_channels[3], 1, expand_ratio=expand_ratio\n",
    "                ),\n",
    "                MobileBlockV2(\n",
    "                    conv_channels[2], conv_channels[3], 1, expand_ratio=expand_ratio\n",
    "                ),\n",
    "                MobileBlockV2(\n",
    "                    conv_channels[3], conv_channels[4], 1, expand_ratio=expand_ratio\n",
    "                ),\n",
    "                MobileBlockV2(\n",
    "                    conv_channels[5], conv_channels[6], 1, expand_ratio=expand_ratio\n",
    "                ),\n",
    "                MobileBlockV2(\n",
    "                    conv_channels[7], conv_channels[8], 1, expand_ratio=expand_ratio\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.mvit_blocks = nn.ModuleList(\n",
    "            [\n",
    "                MobileViTBlock(\n",
    "                    dims[0],\n",
    "                    L[0],\n",
    "                    conv_channels[5],\n",
    "                    self.kernel_size,\n",
    "                    patch_size,\n",
    "                    int(dims[0] * 2),\n",
    "                ),\n",
    "                MobileViTBlock(\n",
    "                    dims[1],\n",
    "                    L[1],\n",
    "                    conv_channels[7],\n",
    "                    self.kernel_size,\n",
    "                    patch_size,\n",
    "                    int(dims[1] * 4),\n",
    "                ),\n",
    "                MobileViTBlock(\n",
    "                    dims[2],\n",
    "                    L[2],\n",
    "                    conv_channels[9],\n",
    "                    self.kernel_size,\n",
    "                    patch_size,\n",
    "                    int(dims[2] * 4),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.final_pw = Conv2DBlock(\n",
    "            in_channels=conv_channels[-2],\n",
    "            out_channels=conv_channels[-1],\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            groups=1,\n",
    "            bias=False,\n",
    "            norm=True,\n",
    "            activation=False,\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Transformer Decoder\n",
    "        self.encoder_dim = conv_channels[-1]\n",
    "        self.transformer_decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=self.encoder_dim, nhead=8, dim_feedforward=1024, dropout=0.2\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            self.transformer_decoder_layer, num_layers=3\n",
    "        )\n",
    "\n",
    "        self.temporal_positional_encoding = PositionalEncoding(self.encoder_dim, 0.1)\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.encoder_dim)\n",
    "        self.classifier = nn.Linear(self.encoder_dim, self.vocab_size)\n",
    "\n",
    "        # Loss and Metrics\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=-1, label_smoothing=0.1)\n",
    "        self.wer = WordErrorRate()\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        \"\"\"\n",
    "        x: input tensor (B, T, C, H, W)\n",
    "        targets: ground truth sequence (B, T') where each value is a token index\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n",
    "\n",
    "        x = self.in_conv(x)\n",
    "        for i in range(5):\n",
    "            x = self.mv2_blocks[i](x)\n",
    "\n",
    "        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n",
    "        x = self.mvit_blocks[0](x)\n",
    "\n",
    "        x = rearrange(x, \"b t c h w -> (b t) c h w\", b=B, t=T)\n",
    "        x = self.mv2_blocks[5](x)\n",
    "\n",
    "        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n",
    "        x = self.mvit_blocks[1](x)\n",
    "\n",
    "        x = rearrange(x, \"b t c h w -> (b t) c h w\", b=B, t=T)\n",
    "        x = self.mv2_blocks[6](x)\n",
    "\n",
    "        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n",
    "        x = self.mvit_blocks[2](x)\n",
    "\n",
    "        x = rearrange(x, \"b t c h w -> (b t) c h w\", b=B, t=T)\n",
    "        x = self.final_pw(x)\n",
    "\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.flatten(1)\n",
    "\n",
    "        x = rearrange(x, \"(b t) d -> t b d\", b=B, t=T)  # (T, B, D)\n",
    "\n",
    "        if targets is not None:\n",
    "            targets = rearrange(targets, \"b t -> t b\")\n",
    "            tgt_embeddings = self.embedding(targets[:-1])  # (T'-1, B, D)\n",
    "\n",
    "            seq_len = tgt_embeddings.size(0)\n",
    "            causal_mask = torch.triu(\n",
    "                torch.ones(seq_len, seq_len, device=x.device), diagonal=1\n",
    "            ).bool()\n",
    "\n",
    "            padding_mask = (targets[:-1] == -1).transpose(0, 1)  # (B, T'-1)\n",
    "\n",
    "            tgt_embeddings = self.temporal_positional_encoding(tgt_embeddings)\n",
    "            x = self.temporal_positional_encoding(x)\n",
    "\n",
    "            output = self.decoder(\n",
    "                tgt=tgt_embeddings,\n",
    "                memory=x,\n",
    "                tgt_mask=causal_mask,\n",
    "                tgt_key_padding_mask=padding_mask,\n",
    "            )\n",
    "\n",
    "            logits = self.classifier(output)\n",
    "            return logits\n",
    "        else:\n",
    "            outputs = []\n",
    "            tgt = torch.zeros((1, B), dtype=torch.long, device=x.device).fill_(\n",
    "                self.eos_token\n",
    "            )\n",
    "\n",
    "            for _ in range(100):\n",
    "                tgt_embeddings = self.embedding(tgt)\n",
    "                output = self.decoder(tgt=tgt_embeddings, memory=x)\n",
    "                logits = self.classifier(output[-1:])\n",
    "                next_token = logits.argmax(-1)\n",
    "                outputs.append(next_token)\n",
    "                if (next_token == self.eos_token).all():\n",
    "                    break\n",
    "                tgt = torch.cat([tgt, next_token], dim=0)\n",
    "\n",
    "            return torch.cat(outputs, dim=0)\n",
    "\n",
    "    def ablated_forward(self, x, targets=None):\n",
    "\n",
    "        B, T, C, H, W = x.shape\n",
    "\n",
    "        fake_memory = torch.zeros((T, B, self.encoder_dim), device=x.device)\n",
    "\n",
    "        if targets is not None:\n",
    "            targets = rearrange(targets, \"b t -> t b\")\n",
    "            tgt_embeddings = self.embedding(targets[:-1])\n",
    "            tgt_embeddings = self.temporal_positional_encoding(tgt_embeddings)\n",
    "            fake_memory = self.temporal_positional_encoding(fake_memory)\n",
    "\n",
    "            seq_len = tgt_embeddings.size(0)\n",
    "            causal_mask = torch.triu(\n",
    "                torch.ones(seq_len, seq_len, device=x.device), diagonal=1\n",
    "            ).bool()\n",
    "\n",
    "            padding_mask = (targets[:-1] == -1).transpose(0, 1)\n",
    "\n",
    "            output = self.decoder(\n",
    "                tgt=tgt_embeddings,\n",
    "                memory=fake_memory,\n",
    "                tgt_mask=causal_mask,\n",
    "                tgt_key_padding_mask=padding_mask,\n",
    "            )\n",
    "\n",
    "            logits = self.classifier(output)\n",
    "            return logits\n",
    "        else:\n",
    "            outputs = []\n",
    "            tgt = torch.zeros((1, B), dtype=torch.long, device=x.device).fill_(\n",
    "                self.eos_token\n",
    "            )\n",
    "\n",
    "            for _ in range(100):\n",
    "                tgt_embeddings = self.embedding(tgt)\n",
    "                output = self.decoder(tgt=tgt_embeddings, memory=fake_memory)\n",
    "                logits = self.classifier(output[-1:])\n",
    "                next_token = logits.argmax(-1)\n",
    "                outputs.append(next_token)\n",
    "                if (next_token == self.eos_token).all():\n",
    "                    break\n",
    "                tgt = torch.cat([tgt, next_token], dim=0)\n",
    "\n",
    "            return torch.cat(outputs, dim=0)\n",
    "\n",
    "    def step(self, batch, phase):\n",
    "        x, y = batch\n",
    "\n",
    "        logits = self(x, targets=y)  # (T'-1, B, V)\n",
    "        y = rearrange(y[:, 1:], \"b t -> t b\")  # Shift ground truth (T'-1, B)\n",
    "\n",
    "        loss = self.criterion(logits.view(-1, self.vocab_size), y.view(-1))\n",
    "        self.log(f\"{phase}/loss\", loss, on_step=False, on_epoch=True)\n",
    "\n",
    "        if phase != \"train\":\n",
    "            word_error_rate = self.calculate_wer(logits, y)\n",
    "            self.log(f\"{phase}/wer\", word_error_rate, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def calculate_wer(self, logits, y):\n",
    "        pred_strings = self.string_from_logits(logits)\n",
    "        target_strings = self.string_from_ground_truth(y)\n",
    "\n",
    "        pred_strings = [\" \".join(pred) for pred in pred_strings]\n",
    "        target_strings = [\" \".join(target) for target in target_strings]\n",
    "\n",
    "        return self.wer(pred_strings, target_strings)\n",
    "\n",
    "    def string_from_logits(self, logits):\n",
    "        indices = torch.argmax(logits, dim=-1)\n",
    "        predicted_strings = [\n",
    "            self.vocabulary.decode_from_ids(idx.tolist()) for idx in indices\n",
    "        ]\n",
    "\n",
    "        return predicted_strings\n",
    "\n",
    "    def string_from_ground_truth(self, y):\n",
    "        target_strings = []\n",
    "        for target in y:\n",
    "            if isinstance(target, torch.Tensor):\n",
    "                target = target.tolist()\n",
    "            target_strings.append(self.vocabulary.decode_from_ids(target))\n",
    "        return target_strings\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), lr=2e-4, amsgrad=True, weight_decay=1e-4\n",
    "        )\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \"min\", patience=3, verbose=True\n",
    "        )\n",
    "        return [optimizer], [\n",
    "            {\"scheduler\": scheduler, \"interval\": \"epoch\", \"monitor\": \"val/loss\"}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Our Datamodules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells contain the definitions of the various datamodules, datasets and various struct-like classes used to train the model in a clean and modular way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T22:49:42.634739Z",
     "iopub.status.busy": "2024-12-16T22:49:42.634405Z",
     "iopub.status.idle": "2024-12-16T22:49:42.670647Z",
     "shell.execute_reply": "2024-12-16T22:49:42.669774Z",
     "shell.execute_reply.started": "2024-12-16T22:49:42.634708Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import random\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PhoenixFiles:\n",
    "    train: str\n",
    "    dev: str\n",
    "    test: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PhoenixDataHyperparameters:\n",
    "    num_workers: int\n",
    "    batch_size: int\n",
    "    sample_random: bool\n",
    "    sample_uniform: bool\n",
    "    sample_percentage: float\n",
    "    transforms: torchvision.transforms.Compose\n",
    "\n",
    "\n",
    "class PhoenixVocabulary:\n",
    "\n",
    "    def __init__(self, filenames: PhoenixFiles):\n",
    "        self.filenames = filenames\n",
    "        self.vocab = self.build_vocab()\n",
    "        self.vocab_inversed = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def build_vocab(self):\n",
    "        vocab = {}\n",
    "\n",
    "        files = [self.filenames.train, self.filenames.dev, self.filenames.test]\n",
    "\n",
    "        for file in files:\n",
    "            with gzip.open(file, \"rb\") as f:\n",
    "                annotations = pickle.load(f)\n",
    "\n",
    "            for ann in tqdm(\n",
    "                annotations,\n",
    "                desc=f\"Extracting tokens from {os.path.basename(file)}\",\n",
    "            ):\n",
    "                for word in ann[\"gloss\"].split():\n",
    "                    if word not in vocab:\n",
    "                        vocab[word] = len(vocab)\n",
    "\n",
    "        vocab[\"<eos>\"] =  len(vocab)\n",
    "        return vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.vocab[idx]\n",
    "\n",
    "    def encode_as_ids(self, sentence):\n",
    "        return [self.vocab[word] for word in sentence]\n",
    "\n",
    "    def decode_from_ids(self, ids):\n",
    "        return [self.vocab_inversed[tkn] for tkn in ids]\n",
    "\n",
    "\n",
    "class Phoenix14TDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, path: str, vocabulary: PhoenixVocabulary, hyperparameters):\n",
    "        self.metadata_path = path\n",
    "        self.base_dir = os.path.dirname(path)\n",
    "        self.data = None\n",
    "        self.vocab: PhoenixVocabulary = vocabulary\n",
    "        self.data_hparams = hyperparameters\n",
    "        self.transform = hyperparameters.transforms\n",
    "\n",
    "        assert (\n",
    "            self.data_hparams.sample_random != self.data_hparams.sample_uniform\n",
    "        ), \"Both random and uniform sampling cannot be enabled at the same time\"\n",
    "\n",
    "    def setup(self, stage):\n",
    "        with gzip.open(self.metadata_path, \"rb\") as f:\n",
    "            self.data = pickle.load(f)\n",
    "\n",
    "        filtered_data = []\n",
    "        for d in tqdm(self.data, desc=\"Filtering sequences with length > 1\"):\n",
    "            targets = self.vocab.encode_as_ids([token for token in d[\"gloss\"].split()])\n",
    "            if len(targets) > 1:  # Discard sequences with only one token\n",
    "                filtered_data.append(d)\n",
    "\n",
    "        self.data = filtered_data\n",
    "\n",
    "        self.signers = [d[\"signer\"] for d in self.data]\n",
    "        self.video_names = [d[\"name\"] for d in self.data]\n",
    "        self.annotations = [d[\"gloss\"] for d in self.data]\n",
    "        self.text = [d[\"text\"] for d in self.data]\n",
    "        self.targets = [\n",
    "            self.vocab.encode_as_ids([token for token in ann.split()])\n",
    "            for ann in tqdm(\n",
    "                self.annotations,\n",
    "                desc=(\n",
    "                    f\"Encoding annotations for {stage}\"\n",
    "                    if stage\n",
    "                    else \"Encoding annotations\"\n",
    "                ),\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        video_path = os.path.join(\n",
    "            self.base_dir, \"videos_phoenix\", \"videos\", self.video_names[idx] + \".mp4\"\n",
    "        )\n",
    "\n",
    "        video = torchvision.io.read_video(\n",
    "            video_path,\n",
    "            pts_unit=\"sec\",\n",
    "            output_format=\"TCHW\",\n",
    "        )[0]\n",
    "        \n",
    "\n",
    "        if self.data_hparams.sample_random:\n",
    "            indices = sorted(random.sample(\n",
    "                range(len(video)), len(video) * self.data_hparams.sample_percentage\n",
    "            ))\n",
    "        elif self.data_hparams.sample_uniform:\n",
    "            indices = np.linspace(\n",
    "                0,\n",
    "                len(video) - 1,\n",
    "                int(len(video) * self.data_hparams.sample_percentage),\n",
    "            ).tolist()\n",
    "        else:\n",
    "            indices = list(range(len(video)))\n",
    "            \n",
    "\n",
    "        T = len(indices)\n",
    "        video_tensor = video.unsqueeze(0)\n",
    "        video_tensor = rearrange(\n",
    "                self.transform(\n",
    "                    rearrange(video_tensor[:, indices], \"b t c h w -> (b t) c h w\")\n",
    "                ), \"(b t) c h w -> b t c h w\", c=3, t=T\n",
    "        ).squeeze(0)\n",
    "\n",
    "        return video_tensor, torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "    def get_metadata(self, idx):\n",
    "        return {\n",
    "            \"signer\": self.signers[idx],\n",
    "            \"video_name\": self.video_names[idx],\n",
    "            \"annotation\": self.annotations[idx],\n",
    "            \"text\": self.text[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "class Phoenix14TDatamodule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        files: PhoenixFiles,\n",
    "        vocabulary: PhoenixVocabulary,\n",
    "        num_workers: int,\n",
    "        data_hyperparameters: PhoenixDataHyperparameters,\n",
    "    ):\n",
    "        super(Phoenix14TDatamodule, self).__init__()\n",
    "        self.metadata_paths = files\n",
    "        self.vocabulary = vocabulary\n",
    "        self.workers = num_workers\n",
    "        self.train = None\n",
    "        self.dev = None\n",
    "        self.test = None\n",
    "        self.data_hparams = data_hyperparameters\n",
    "\n",
    "        assert (\n",
    "            self.data_hparams.sample_random != self.data_hparams.sample_uniform\n",
    "        ), \"Both random and uniform sampling cannot be enabled at the same time\"\n",
    "\n",
    "    def setup(self, stage):\n",
    "        self.train = Phoenix14TDataset(\n",
    "            self.metadata_paths.train, self.vocabulary, self.data_hparams\n",
    "        )\n",
    "        self.train.setup(\"train\")\n",
    "\n",
    "        self.dev = Phoenix14TDataset(\n",
    "            self.metadata_paths.dev, self.vocabulary, self.data_hparams\n",
    "        )\n",
    "        self.dev.setup(\"dev\")\n",
    "\n",
    "        self.test = Phoenix14TDataset(\n",
    "            self.metadata_paths.test, self.vocabulary, self.data_hparams\n",
    "        )\n",
    "        self.test.setup(\"test\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train,\n",
    "            shuffle=True,\n",
    "            batch_size=1,\n",
    "            pin_memory=True,\n",
    "            num_workers=self.workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dev,\n",
    "            shuffle=False,\n",
    "            batch_size=1,\n",
    "            pin_memory=True,\n",
    "            num_workers=self.workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test,\n",
    "            shuffle=False,\n",
    "            batch_size=1,\n",
    "            pin_memory=True,\n",
    "            num_workers=self.workers,\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "def animate_torch_tensor(video):\n",
    "    \n",
    "    # Adapted from https://stackoverflow.com/a/57275596\n",
    "    \n",
    "    %matplotlib inline\n",
    "    from matplotlib import pyplot as plt\n",
    "    from matplotlib import animation\n",
    "    from IPython.display import HTML\n",
    "\n",
    "    \n",
    "    video_np = video.numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    im = plt.imshow(video_np[0,:,:,:])\n",
    "\n",
    "    plt.close() # this is required to not display the generated image\n",
    "\n",
    "    def init():\n",
    "        im.set_data(video_np[0,:,:,:])\n",
    "\n",
    "    def animate(i):\n",
    "        im.set_data(video_np[i,:,:,:])\n",
    "        return im\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video_np.shape[0],\n",
    "                                interval=1000//6)\n",
    "    return HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T22:49:43.842850Z",
     "iopub.status.busy": "2024-12-16T22:49:43.842032Z",
     "iopub.status.idle": "2024-12-16T22:49:43.848895Z",
     "shell.execute_reply": "2024-12-16T22:49:43.847948Z",
     "shell.execute_reply.started": "2024-12-16T22:49:43.842816Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def xxs_mvit(vocab):\n",
    "    return MobileViT(\n",
    "        dims=[64, 80, 96],\n",
    "        conv_channels=[16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 320],\n",
    "        num_classes=len(vocab),\n",
    "        vocabulary=vocab,\n",
    "        expand_ratio=2,\n",
    "        patch_size=(2, 2),\n",
    "    )\n",
    "\n",
    "\n",
    "def xs_mvit(vocab):\n",
    "    return MobileViT(\n",
    "        dims=[96, 120, 144],\n",
    "        conv_channels=[16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384],\n",
    "        num_classes=len(vocab),\n",
    "        vocabulary=vocab,\n",
    "        expand_ratio=4,\n",
    "        patch_size=(2, 2),\n",
    "    )\n",
    "\n",
    "\n",
    "def s_mvit(vocab):\n",
    "    return MobileViT(\n",
    "        dims=[144, 192, 240],\n",
    "        conv_channels=[16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 640],\n",
    "        num_classes=len(vocab),\n",
    "        vocabulary=vocab,\n",
    "        expand_ratio=4,\n",
    "        patch_size=(8, 8),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T22:30:20.081754Z",
     "iopub.status.busy": "2024-12-16T22:30:20.081402Z",
     "iopub.status.idle": "2024-12-16T22:30:21.510090Z",
     "shell.execute_reply": "2024-12-16T22:30:21.509272Z",
     "shell.execute_reply.started": "2024-12-16T22:30:20.081705Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import wandb\n",
    "#\n",
    "# # UNCOMMENT THIS TO LOG TO WANDB FROM KAGGLE\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "#\n",
    "# user_secrets = UserSecretsClient()\n",
    "# secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n",
    "#\n",
    "# wandb.login(key=secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T22:32:01.931822Z",
     "iopub.status.busy": "2024-12-16T22:32:01.931395Z",
     "iopub.status.idle": "2024-12-16T22:32:01.982070Z",
     "shell.execute_reply": "2024-12-16T22:32:01.981197Z",
     "shell.execute_reply.started": "2024-12-16T22:32:01.931779Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting tokens from phoenix14t.pami0.train.annotations_only.gzip: 100%|██████████| 7096/7096 [00:00<00:00, 624874.68it/s]\n",
      "Extracting tokens from phoenix14t.pami0.dev.annotations_only.gzip: 100%|██████████| 519/519 [00:00<00:00, 718904.81it/s]\n",
      "Extracting tokens from phoenix14t.pami0.test.annotations_only.gzip: 100%|██████████| 642/642 [00:00<00:00, 592071.94it/s]\n"
     ]
    }
   ],
   "source": [
    "files = PhoenixFiles(\n",
    "    train=\"/home/dario/repos/lis-vit/project/data/phoenixweather2014t-3rd-attempt/phoenix14t.pami0.train.annotations_only.gzip\",\n",
    "    dev=\"/home/dario/repos/lis-vit/project/data/phoenixweather2014t-3rd-attempt/phoenix14t.pami0.dev.annotations_only.gzip\",\n",
    "    test=\"/home/dario/repos/lis-vit/project/data/phoenixweather2014t-3rd-attempt/phoenix14t.pami0.test.annotations_only.gzip\",\n",
    ")\n",
    "\n",
    "transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(\n",
    "            (192, 256), interpolation=torchvision.transforms.InterpolationMode.BICUBIC\n",
    "        ),  # from uint8 to float32\n",
    "        lambda x: x / 255.0,\n",
    "        torchvision.transforms.Normalize(\n",
    "            torch.tensor([0.5337, 0.5225, 0.5162]),\n",
    "            torch.tensor([0.2873, 0.2966, 0.3266]),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_hparams = PhoenixDataHyperparameters(\n",
    "    num_workers=3,\n",
    "    batch_size=1,\n",
    "    sample_random=False,\n",
    "    sample_uniform=True,\n",
    "    sample_percentage=1 / 5,\n",
    "    transforms=transforms,\n",
    ")\n",
    "\n",
    "vocab = PhoenixVocabulary(files)\n",
    "\n",
    "dm = Phoenix14TDatamodule(files, vocab, 3, data_hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightning training loop is used to train the model\n",
    "\n",
    "Log into wandb to log the metrics (might not work without wandb log-in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T22:28:56.868789Z",
     "iopub.status.busy": "2024-12-16T22:28:56.868429Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set is_training to True to train the model (False by default to save time)\n"
     ]
    }
   ],
   "source": [
    "# flush cuda cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import lightning.pytorch as lp\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "# setup training and wandb\n",
    "is_training = False\n",
    "BATCH_SIZE = 1  # we don't do padding, so batch size must be 1\n",
    "\n",
    "if is_training:\n",
    "\n",
    "    wandb.finish()\n",
    "    wandb_logger = lp.loggers.WandbLogger(project=\"mobilevit\")\n",
    "\n",
    "    model = xxs_mvit(vocab)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=100,\n",
    "        accelerator=\"cpu\",\n",
    "        logger=wandb_logger,\n",
    "        callbacks=[\n",
    "            lp.callbacks.ModelCheckpoint(\n",
    "                monitor=\"val/loss\",\n",
    "                filename=\"best_model\",\n",
    "                save_top_k=1,\n",
    "                save_last=True,\n",
    "                mode=\"min\",\n",
    "            ),\n",
    "            lp.callbacks.EarlyStopping(\n",
    "                monitor=\"val/loss\",\n",
    "                patience=5,\n",
    "                mode=\"min\",\n",
    "            ),\n",
    "            lp.callbacks.LearningRateMonitor(logging_interval=\"step\"),\n",
    "        ],\n",
    "        limit_train_batches=0.20,\n",
    "        accumulate_grad_batches=16,\n",
    "    )\n",
    "\n",
    "    wandb_logger.watch(model, log_graph=False)\n",
    "\n",
    "    trainer.fit(model, datamodule=dm)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "else:\n",
    "    print(\"set is_training to True to train the model (False by default to save time)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Study\n",
    "\n",
    "We show that the encoder part is useless, as it can be substituted by a gaussian noise-generated memory tensor, leading to a meager $0.1$% drop in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xxs_mvit(vocab)\n",
    "model = MobileViT.load_from_checkpoint(\n",
    "    \"best_model.ckpt\",\n",
    "    **{\n",
    "        \"dims\": [64, 80, 96],\n",
    "        \"conv_channels\": [16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 320],\n",
    "        \"num_classes\": len(vocab),\n",
    "        \"vocabulary\": vocab,\n",
    "        \"expand_ratio\": 2,\n",
    "        \"patch_size\": (2, 2),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering sequences with length > 1: 100%|██████████| 7096/7096 [00:00<00:00, 368798.56it/s]\n",
      "Encoding annotations for train: 100%|██████████| 7087/7087 [00:00<00:00, 342438.51it/s]\n",
      "Filtering sequences with length > 1: 100%|██████████| 519/519 [00:00<00:00, 200686.25it/s]\n",
      "Encoding annotations for dev: 100%|██████████| 519/519 [00:00<00:00, 103211.98it/s]\n",
      "Filtering sequences with length > 1: 100%|██████████| 642/642 [00:00<00:00, 424455.10it/s]\n",
      "Encoding annotations for test: 100%|██████████| 640/640 [00:00<00:00, 1499.22it/s]\n"
     ]
    }
   ],
   "source": [
    "dm.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load validation data, compute word-error rate with and without the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [01:06<00:00,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 0.7698995471000671\n",
      "WER (Ablated): 0.7688159346580505\n",
      "Encoder impact on WER: 0.0010836124420166016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "average_error = 0.0\n",
    "average_error_ablated = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for batch in tqdm(dm.val_dataloader()):\n",
    "        video, target = batch\n",
    "        logits = model(video.to(\"cuda\"), target.to(\"cuda\"))\n",
    "        ablated_logits = model.ablated_forward(video.to(\"cuda\"), target.to(\"cuda\"))\n",
    "\n",
    "        wer = model.calculate_wer(logits, target.to(\"cuda\"))\n",
    "        ablated_wer = model.calculate_wer(ablated_logits, target.to(\"cuda\"))\n",
    "\n",
    "        average_error += wer\n",
    "        average_error_ablated += ablated_wer\n",
    "\n",
    "average_error /= len(dm.test_dataloader())\n",
    "average_error_ablated /= len(dm.test_dataloader())\n",
    "\n",
    "print(f\"WER: {average_error}\")\n",
    "print(f\"WER (Ablated): {average_error_ablated}\")\n",
    "print(f\"Encoder impact on WER: {average_error - average_error_ablated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set render_gradient_graph to True to render the gradient graph, showing that the model is actually capable of backpropagating gradients through the encoder\n",
      "WARNING: graph is huge and may take a while to render\n"
     ]
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "render_gradient_graph = False\n",
    "\n",
    "if render_gradient_graph:\n",
    "    make_dot(\n",
    "        model(video.to(\"cuda\"), target.to(\"cuda\")),\n",
    "        params=dict(model.named_parameters()),\n",
    "    ).render(\"model\", format=\"png\", cleanup=True)\n",
    "else:\n",
    "    print(\n",
    "        \"Set render_gradient_graph to True to render the gradient graph, showing that the model is actually capable of backpropagating gradients through the encoder\\nWARNING: graph is huge and may take a while to render\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1049383,
     "sourceId": 1775725,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
