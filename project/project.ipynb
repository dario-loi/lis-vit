{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIS With MobileViTs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Our MobileViTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T00:03:25.372008Z",
     "iopub.status.busy": "2024-12-14T00:03:25.371105Z",
     "iopub.status.idle": "2024-12-14T00:03:37.120701Z",
     "shell.execute_reply": "2024-12-14T00:03:37.119869Z",
     "shell.execute_reply.started": "2024-12-14T00:03:25.371950Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install lightning torchmetrics einops av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T00:03:37.123442Z",
     "iopub.status.busy": "2024-12-14T00:03:37.123154Z",
     "iopub.status.idle": "2024-12-14T00:03:44.683745Z",
     "shell.execute_reply": "2024-12-14T00:03:44.682989Z",
     "shell.execute_reply.started": "2024-12-14T00:03:37.123393Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import wandb\n",
    "from einops import rearrange\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "from torchmetrics.text import WordErrorRate\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "import pickle\n",
    "from lightning.pytorch.utilities.model_summary import summarize\n",
    "from lightning.fabric.utilities import measure_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T00:03:44.685169Z",
     "iopub.status.busy": "2024-12-14T00:03:44.684794Z",
     "iopub.status.idle": "2024-12-14T00:03:44.724218Z",
     "shell.execute_reply": "2024-12-14T00:03:44.723333Z",
     "shell.execute_reply.started": "2024-12-14T00:03:44.685145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Optimize this impl:\n",
    "# https://github.com/chinhsuanwu/mobilevit-pytorch/blob/master/mobilevit.py\n",
    "\n",
    "\n",
    "class Conv2DBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        groups=1,\n",
    "        bias=True,\n",
    "        norm=True,\n",
    "        activation=True,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        \"\"\"__init__ Constructor for Conv2DBlock\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            Number of input channels\n",
    "        out_channels : int\n",
    "            Number of output channels\n",
    "        kernel_size : int\n",
    "            Size of the kernel\n",
    "        stride : int\n",
    "            Stride of the convolutional layer\n",
    "        padding : int\n",
    "            Padding of the convolutional layer\n",
    "        groups : int\n",
    "            Number of groups\n",
    "        bias : bool\n",
    "            Whether to use bias\n",
    "        dropout : float\n",
    "            Dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super(Conv2DBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential()\n",
    "\n",
    "        self.block.add_module(\n",
    "            \"conv2d\",\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                groups=groups,\n",
    "                bias=bias,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if norm:\n",
    "            self.block.add_module(\"norm\", nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        if activation:\n",
    "            self.block.add_module(\"activation\", nn.SiLU())  # sigmoid(x) * x // SWISH\n",
    "\n",
    "        if dropout:\n",
    "            self.block.add_module(\"dropout\", nn.Dropout(dropout))\n",
    "\n",
    "        self.block = self.block\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class MobileBlockV2(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride, expand_ratio):\n",
    "        \"\"\"__init__ Constructor for MobileBlockV2\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            Number of input channels\n",
    "        out_channels : int\n",
    "            Number of output channels\n",
    "        stride : int\n",
    "            Stride of the convolutional layer\n",
    "        expand_ratio : int\n",
    "            Expansion ratio of the block\n",
    "        \"\"\"\n",
    "\n",
    "        super(MobileBlockV2, self).__init__()\n",
    "\n",
    "        assert stride in [1, 2], \"Stride must be either 1 or 2\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.hidden_dim = int(round(in_channels * expand_ratio))\n",
    "\n",
    "        self.mbv2 = nn.Sequential()\n",
    "        self.uses_inverse_residual = (\n",
    "            self.in_channels == self.out_channels and self.stride == 1\n",
    "        )\n",
    "\n",
    "        if self.expand_ratio == 1:\n",
    "            self.mbv2.add_module(\n",
    "                \"depthwise_3x3\",\n",
    "                Conv2DBlock(  # Depthwise Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=3,\n",
    "                    stride=stride,\n",
    "                    padding=1,\n",
    "                    groups=self.hidden_dim,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                ),\n",
    "            )\n",
    "            self.mbv2.add_module(\n",
    "                \"pointwise-linear_1x1\",\n",
    "                Conv2DBlock(  # Pointwise-Linear Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    groups=self.hidden_dim,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=False,\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            self.mbv2.add_module(\n",
    "                \"pointwise_1x1\",\n",
    "                Conv2DBlock(  # Pointwise-Linear Convolution\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    groups=1,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                ),\n",
    "            )\n",
    "            self.mbv2.add_module(\n",
    "                \"depthwise_3x3\",\n",
    "                Conv2DBlock(  # Depthwise Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=3,\n",
    "                    stride=stride,\n",
    "                    padding=1,\n",
    "                    groups=self.hidden_dim,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                ),\n",
    "            )\n",
    "            self.mbv2.add_module(\n",
    "                \"pointwise-linear_1x1\",\n",
    "                Conv2DBlock(  # Pointwise-Linear Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    groups=1,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=False,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        self.mbv2 = self.mbv2\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.uses_inverse_residual:\n",
    "            return x + self.mbv2(x)\n",
    "        else:\n",
    "            return self.mbv2(x)\n",
    "\n",
    "\n",
    "class MobileViTBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, hidden_dim, depth, channels, kernel_size, patch_size, mlp_dim, dropout=0.1\n",
    "    ):\n",
    "\n",
    "        super(MobileViTBlock, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.depth = depth\n",
    "        self.channels = channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.patch_size = patch_size\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.local_conv = nn.Sequential(\n",
    "            Conv2DBlock(\n",
    "                in_channels=channels,\n",
    "                out_channels=channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                norm=True,\n",
    "                activation=True,\n",
    "                bias=False,\n",
    "            ),\n",
    "            Conv2DBlock(\n",
    "                in_channels=channels,\n",
    "                out_channels=hidden_dim,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                norm=True,\n",
    "                activation=True,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.global_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_dim,\n",
    "                nhead=8,\n",
    "                dim_feedforward=mlp_dim,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "                activation=\"gelu\",\n",
    "            ),\n",
    "            num_layers=depth,\n",
    "            norm=nn.LayerNorm(hidden_dim),\n",
    "        )\n",
    "\n",
    "        self.fusion_conv_preres = Conv2DBlock(\n",
    "            in_channels=hidden_dim,\n",
    "            out_channels=channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=False,\n",
    "            norm=True,\n",
    "            activation=True,\n",
    "        )\n",
    "\n",
    "        self.fusion_conv_postres = Conv2DBlock(\n",
    "            in_channels=2 * channels,\n",
    "            out_channels=channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "            norm=True,\n",
    "            activation=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n",
    "        x_res = x.clone()\n",
    "\n",
    "        # local_repr\n",
    "        x = self.local_conv(x)\n",
    "\n",
    "        ph, pw = self.patch_size\n",
    "\n",
    "        # global_repr\n",
    "        _, _, h, w = x.shape\n",
    "        x = rearrange(  # reshape the image into patches for ViT input\n",
    "            x,\n",
    "            \"(b t) d (h ph) (w pw) -> (b h w) (t ph pw) d\",\n",
    "            ph=ph,\n",
    "            pw=pw,\n",
    "            b=B,\n",
    "            t=T,\n",
    "        )\n",
    "        x = self.global_transformer(x)\n",
    "        x = rearrange(\n",
    "            x,\n",
    "            \"(b h w) (t ph pw) d -> (b t) d (h ph) (w pw)\",\n",
    "            h=h // ph,\n",
    "            w=w // pw,\n",
    "            ph=ph,\n",
    "            pw=pw,\n",
    "            b=B,\n",
    "            t=T,\n",
    "        )\n",
    "\n",
    "        # fusion\n",
    "        x = self.fusion_conv_preres(x)\n",
    "        x = torch.cat([x, x_res], dim=1)\n",
    "        x = self.fusion_conv_postres(x)\n",
    "\n",
    "        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MobileViT(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dims,\n",
    "        conv_channels,\n",
    "        num_classes,\n",
    "        vocabulary,\n",
    "        expand_ratio=4,\n",
    "        patch_size=(2, 2),\n",
    "    ):\n",
    "        super(MobileViT, self).__init__()\n",
    "\n",
    "        self.dims = dims\n",
    "        self.conv_channels = conv_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.patch_size = patch_size\n",
    "        self.kernel_size = 3\n",
    "\n",
    "        L = [2, 4, 3]\n",
    "\n",
    "        self.in_conv = Conv2DBlock(\n",
    "            in_channels=3,\n",
    "            out_channels=conv_channels[0],\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            norm=True,\n",
    "            activation=True,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.mv2_blocks = nn.ModuleList([])\n",
    "\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[0], conv_channels[1], 1, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[1], conv_channels[2], 2, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[2], conv_channels[3], 1, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[2], conv_channels[3], 1, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[3], conv_channels[4], 2, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[5], conv_channels[6], 2, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[7], conv_channels[8], 2, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.mvit_blocks = nn.ModuleList([])\n",
    "\n",
    "        self.mvit_blocks.append(\n",
    "            MobileViTBlock(\n",
    "                dims[0],\n",
    "                L[0],\n",
    "                conv_channels[5],\n",
    "                self.kernel_size,\n",
    "                patch_size,\n",
    "                int(dims[0] * 2),\n",
    "            )\n",
    "        )\n",
    "        self.mvit_blocks.append(\n",
    "            MobileViTBlock(\n",
    "                dims[1],\n",
    "                L[1],\n",
    "                conv_channels[7],\n",
    "                self.kernel_size,\n",
    "                patch_size,\n",
    "                int(dims[1] * 4),\n",
    "            )\n",
    "        )\n",
    "        self.mvit_blocks.append(\n",
    "            MobileViTBlock(\n",
    "                dims[2],\n",
    "                L[2],\n",
    "                conv_channels[9],\n",
    "                self.kernel_size,\n",
    "                patch_size,\n",
    "                int(dims[2] * 4),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.final_pw = Conv2DBlock(\n",
    "            in_channels=conv_channels[-2],\n",
    "            out_channels=conv_channels[-1],\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            groups=1,\n",
    "            bias=False,\n",
    "            norm=True,\n",
    "            activation=False,\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(conv_channels[-1], num_classes, bias=False)\n",
    "\n",
    "        ## Training-related members\n",
    "        self.criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "        self.wer = WordErrorRate()\n",
    "\n",
    "        self.vocabulary = vocabulary\n",
    "\n",
    "        self.apply(self.init_weights)  # Initialize weights\n",
    "\n",
    "    def init_weights(self, m):\n",
    "\n",
    "        if type(m) == nn.Conv2d:\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif type(m) == nn.BatchNorm2d:\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif type(m) == nn.Linear:\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n",
    "\n",
    "        x = self.in_conv(x)\n",
    "\n",
    "        for i in range(5):\n",
    "            x = self.mv2_blocks[i](x)\n",
    "\n",
    "        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n",
    "        x = self.mvit_blocks[0](x)\n",
    "\n",
    "        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n",
    "        x = self.mv2_blocks[5](x)\n",
    "\n",
    "        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n",
    "        x = self.mvit_blocks[1](x)\n",
    "\n",
    "        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n",
    "        x = self.mv2_blocks[6](x)\n",
    "\n",
    "        x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B, t=T)\n",
    "        x = self.mvit_blocks[2](x)\n",
    "\n",
    "        x = rearrange(x, \"b t c h w -> (b t) c h w\", b=B, t=T)\n",
    "        x = self.final_pw(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.classifier(x)\n",
    "        x = rearrange(x, \"(b t) p -> t b p\", b=B, t=T)  # time major due to CTC loss\n",
    "        return x\n",
    "\n",
    "    def step(self, batch, phase):\n",
    "        x, y = batch\n",
    "\n",
    "        B, T, C, H, W = x.shape\n",
    "        B, N = y.shape\n",
    "\n",
    "        # assume padding is done in the dataloader\n",
    "        input_lengths = torch.full((B,), T, dtype=torch.long)\n",
    "        target_lengths = torch.full((B,), N, dtype=torch.long)\n",
    "\n",
    "        logits = self(x)\n",
    "\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "        # sum one to y as 0 is the blank token in our representation\n",
    "        loss = self.criterion(log_probs, y + 1, input_lengths, target_lengths)\n",
    "\n",
    "        self.log(f\"{phase}/loss\", loss, prog_bar=True)\n",
    "\n",
    "        if phase != \"train\":\n",
    "            word_error_rate = self.calculate_wer(logits, y)\n",
    "            self.log(f\"{phase}/wer\", word_error_rate, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def calculate_wer(self, logits, y):\n",
    "        pred_strings = self.string_from_logits(logits)\n",
    "        target_strings = self.string_from_ground_truth(y)\n",
    "\n",
    "        pred_strings = [\" \".join(pred) for pred in pred_strings]\n",
    "        target_strings = [\" \".join(target) for target in target_strings]\n",
    "\n",
    "        return self.wer(pred_strings, target_strings)\n",
    "\n",
    "    def string_from_logits(self, logits):\n",
    "        decoded = self.ctc_decode(logits)\n",
    "\n",
    "        return self.string_from_ground_truth(decoded)\n",
    "\n",
    "    def ctc_decode(self, logits, blank=0):\n",
    "        logits = torch.argmax(logits, dim=-1)  # Take the most probable class\n",
    "        decoded = []\n",
    "        for seq in logits.T:  # Iterate over batch\n",
    "            result = []\n",
    "            prev_token = blank\n",
    "            for token in seq:\n",
    "                if token != prev_token and token != blank:\n",
    "                    result.append(token.item())\n",
    "                prev_token = token\n",
    "            decoded.append(result)\n",
    "        return decoded\n",
    "\n",
    "    def string_from_ground_truth(self, y):\n",
    "        target_strings = []\n",
    "        for target in y:\n",
    "            if isinstance(target, torch.Tensor):\n",
    "                target = target.tolist()\n",
    "            target_strings.append(self.vocabulary.decode_from_ids(target))\n",
    "        return target_strings\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), lr=2e-4, amsgrad=True, weight_decay=1e-4\n",
    "        )\n",
    "        scheduler = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 32,\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Our Datamodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T00:09:33.020348Z",
     "iopub.status.busy": "2024-12-14T00:09:33.020071Z",
     "iopub.status.idle": "2024-12-14T00:09:33.054227Z",
     "shell.execute_reply": "2024-12-14T00:09:33.053481Z",
     "shell.execute_reply.started": "2024-12-14T00:09:33.020321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import random\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PhoenixFiles:\n",
    "    train: str\n",
    "    dev: str\n",
    "    test: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PhoenixDataHyperparameters:\n",
    "    num_workers: int\n",
    "    batch_size: int\n",
    "    sample_random: bool\n",
    "    sample_uniform: bool\n",
    "    sample_percentage: float\n",
    "    transforms: torchvision.transforms.Compose\n",
    "\n",
    "\n",
    "class PhoenixVocabulary:\n",
    "\n",
    "    def __init__(self, filenames: PhoenixFiles):\n",
    "        self.filenames = filenames\n",
    "        self.vocab = self.build_vocab()\n",
    "        self.vocab_inversed = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def build_vocab(self):\n",
    "        vocab = {}\n",
    "\n",
    "        files = [self.filenames.train, self.filenames.dev, self.filenames.test]\n",
    "\n",
    "        for file in files:\n",
    "            with gzip.open(file, \"rb\") as f:\n",
    "                annotations = pickle.load(f)\n",
    "\n",
    "            for ann in tqdm(\n",
    "                random.sample(annotations, len(annotations)),\n",
    "                desc=f\"Extracting tokens from {os.path.basename(file)}\",\n",
    "            ):\n",
    "\n",
    "                # random.sample shuffles the strings, this improves token distribution (hopefully improves training)\n",
    "                for word in random.sample(\n",
    "                    ann[\"gloss\"].split(), len(ann[\"gloss\"].split())\n",
    "                ):\n",
    "                    if word not in vocab:\n",
    "                        vocab[word] = len(vocab) + 1\n",
    "\n",
    "        vocab[\"<blank>\"] = 0\n",
    "\n",
    "        return vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.vocab[idx]\n",
    "\n",
    "    def encode_as_ids(self, sentence):\n",
    "        return [self.vocab[word] for word in sentence]\n",
    "\n",
    "    def decode_from_ids(self, ids):\n",
    "        return [self.vocab_inversed[tkn] for tkn in ids]\n",
    "\n",
    "\n",
    "class Phoenix14TDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, path: str, vocabulary: PhoenixVocabulary, hyperparameters):\n",
    "        self.metadata_path = path\n",
    "        self.base_dir = os.path.dirname(path)\n",
    "        self.data = None\n",
    "        self.vocab: PhoenixVocabulary = vocabulary\n",
    "        self.data_hparams = hyperparameters\n",
    "        self.transform = hyperparameters.transforms\n",
    "\n",
    "        assert (\n",
    "            self.data_hparams.sample_random != self.data_hparams.sample_uniform\n",
    "        ), \"Both random and uniform sampling cannot be enabled at the same time\"\n",
    "\n",
    "    def setup(self, stage):\n",
    "        with gzip.open(self.metadata_path, \"rb\") as f:\n",
    "            self.data = pickle.load(f)\n",
    "\n",
    "        self.signers = [d[\"signer\"] for d in self.data]\n",
    "        self.video_names = [d[\"name\"] for d in self.data]\n",
    "        self.annotations = [d[\"gloss\"] for d in self.data]\n",
    "        self.text = [d[\"text\"] for d in self.data]\n",
    "        self.targets = [\n",
    "            self.vocab.encode_as_ids([token for token in ann.split()])\n",
    "            for ann in tqdm(\n",
    "                self.annotations,\n",
    "                desc=(\n",
    "                    f\"Encoding annotations for {stage}\"\n",
    "                    if stage\n",
    "                    else \"Encoding annotations\"\n",
    "                ),\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        video_path = os.path.join(\n",
    "            self.base_dir, \"videos_phoenix\", \"videos\", self.video_names[idx] + \".mp4\"\n",
    "        )\n",
    "\n",
    "        video = torchvision.io.read_video(\n",
    "            video_path,\n",
    "            pts_unit=\"sec\",\n",
    "            output_format=\"TCHW\",\n",
    "        )[0]\n",
    "        \n",
    "\n",
    "        if self.data_hparams.sample_random:\n",
    "            indices = sorted(random.sample(\n",
    "                range(len(video)), len(video) * self.data_hparams.sample_percentage\n",
    "            ))\n",
    "        elif self.data_hparams.sample_uniform:\n",
    "            indices = np.linspace(\n",
    "                0,\n",
    "                len(video) - 1,\n",
    "                int(len(video) * self.data_hparams.sample_percentage),\n",
    "            ).tolist()\n",
    "        else:\n",
    "            indices = list(range(len(video)))\n",
    "            \n",
    "\n",
    "        T = len(indices)\n",
    "        video_tensor = video.unsqueeze(0)\n",
    "        video_tensor = rearrange(\n",
    "                self.transform(\n",
    "                    rearrange(video_tensor[:, indices], \"b t c h w -> (b t) c h w\")\n",
    "                ), \"(b t) c h w -> b t c h w\", c=3, t=T\n",
    "        ).squeeze(0)\n",
    "\n",
    "        return video_tensor, torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "    def get_metadata(self, idx):\n",
    "        return {\n",
    "            \"signer\": self.signers[idx],\n",
    "            \"video_name\": self.video_names[idx],\n",
    "            \"annotation\": self.annotations[idx],\n",
    "            \"text\": self.text[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "class Phoenix14TDatamodule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        files: PhoenixFiles,\n",
    "        vocabulary: PhoenixVocabulary,\n",
    "        num_workers: int,\n",
    "        data_hyperparameters: PhoenixDataHyperparameters,\n",
    "    ):\n",
    "        super(Phoenix14TDatamodule, self).__init__()\n",
    "        self.metadata_paths = files\n",
    "        self.vocabulary = vocabulary\n",
    "        self.workers = num_workers\n",
    "        self.train = None\n",
    "        self.dev = None\n",
    "        self.test = None\n",
    "        self.data_hparams = data_hyperparameters\n",
    "\n",
    "        assert (\n",
    "            self.data_hparams.sample_random != self.data_hparams.sample_uniform\n",
    "        ), \"Both random and uniform sampling cannot be enabled at the same time\"\n",
    "\n",
    "    def setup(self, stage):\n",
    "        self.train = Phoenix14TDataset(\n",
    "            self.metadata_paths.train, self.vocabulary, self.data_hparams\n",
    "        )\n",
    "        self.train.setup(\"train\")\n",
    "\n",
    "        self.dev = Phoenix14TDataset(\n",
    "            self.metadata_paths.dev, self.vocabulary, self.data_hparams\n",
    "        )\n",
    "        self.dev.setup(\"dev\")\n",
    "\n",
    "        self.test = Phoenix14TDataset(\n",
    "            self.metadata_paths.test, self.vocabulary, self.data_hparams\n",
    "        )\n",
    "        self.test.setup(\"test\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train,\n",
    "            shuffle=True,\n",
    "            batch_size=1,\n",
    "            pin_memory=True,\n",
    "            num_workers=self.workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dev,\n",
    "            shuffle=False,\n",
    "            batch_size=1,\n",
    "            pin_memory=True,\n",
    "            num_workers=self.workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test,\n",
    "            shuffle=False,\n",
    "            batch_size=1,\n",
    "            pin_memory=True,\n",
    "            num_workers=self.workers,\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "def animate_torch_tensor(video):\n",
    "    \n",
    "    # Adapted from https://stackoverflow.com/a/57275596\n",
    "    \n",
    "    %matplotlib inline\n",
    "    from matplotlib import pyplot as plt\n",
    "    from matplotlib import animation\n",
    "    from IPython.display import HTML\n",
    "\n",
    "    \n",
    "    video_np = video.numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    im = plt.imshow(video_np[0,:,:,:])\n",
    "\n",
    "    plt.close() # this is required to not display the generated image\n",
    "\n",
    "    def init():\n",
    "        im.set_data(video_np[0,:,:,:])\n",
    "\n",
    "    def animate(i):\n",
    "        im.set_data(video_np[i,:,:,:])\n",
    "        return im\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video_np.shape[0],\n",
    "                                interval=1000//6)\n",
    "    return HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T00:09:33.158307Z",
     "iopub.status.busy": "2024-12-14T00:09:33.157607Z",
     "iopub.status.idle": "2024-12-14T00:09:33.164591Z",
     "shell.execute_reply": "2024-12-14T00:09:33.163754Z",
     "shell.execute_reply.started": "2024-12-14T00:09:33.158278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def xxs_mvit(vocab):\n",
    "    return MobileViT(\n",
    "        dims=[64, 80, 96],\n",
    "        conv_channels=[16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 320],\n",
    "        num_classes=len(vocab),\n",
    "        vocabulary=vocab,\n",
    "        expand_ratio=2,\n",
    "        patch_size=(2, 2),\n",
    "    )\n",
    "\n",
    "\n",
    "def xs_mvit(vocab):\n",
    "    return MobileViT(\n",
    "        dims=[96, 120, 144],\n",
    "        conv_channels=[16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384],\n",
    "        num_classes=len(vocab),\n",
    "        vocabulary=vocab,\n",
    "        expand_ratio=4,\n",
    "        patch_size=(4, 4),\n",
    "    )\n",
    "\n",
    "\n",
    "def s_mvit(vocab):\n",
    "    return MobileViT(\n",
    "        dims=[144, 192, 240],\n",
    "        conv_channels=[16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 640],\n",
    "        num_classes=len(vocab),\n",
    "        vocabulary=vocab,\n",
    "        expand_ratio=4,\n",
    "        patch_size=(8, 8),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T00:09:33.338940Z",
     "iopub.status.busy": "2024-12-14T00:09:33.338034Z",
     "iopub.status.idle": "2024-12-14T00:09:33.565268Z",
     "shell.execute_reply": "2024-12-14T00:09:33.564364Z",
     "shell.execute_reply.started": "2024-12-14T00:09:33.338893Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# UNCOMMENT THIS TO LOG TO WANDB\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n",
    "\n",
    "\n",
    "wandb.login(key=secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T00:09:33.566946Z",
     "iopub.status.busy": "2024-12-14T00:09:33.566681Z",
     "iopub.status.idle": "2024-12-14T00:09:33.671614Z",
     "shell.execute_reply": "2024-12-14T00:09:33.670786Z",
     "shell.execute_reply.started": "2024-12-14T00:09:33.566919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "files = PhoenixFiles(\n",
    "    train=\"/kaggle/input/phoenixweather2014t-3rd-attempt/phoenix14t.pami0.train.annotations_only.gzip\",\n",
    "    dev=\"/kaggle/input/phoenixweather2014t-3rd-attempt/phoenix14t.pami0.dev.annotations_only.gzip\",\n",
    "    test=\"/kaggle/input/phoenixweather2014t-3rd-attempt/phoenix14t.pami0.test.annotations_only.gzip\",\n",
    ")\n",
    "\n",
    "transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(\n",
    "            (192, 256), interpolation=torchvision.transforms.InterpolationMode.BICUBIC\n",
    "        ),  # from uint8 to float32\n",
    "        lambda x: x / 255.0,\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_hparams = PhoenixDataHyperparameters(\n",
    "    num_workers=3,\n",
    "    batch_size=1,\n",
    "    sample_random=False,\n",
    "    sample_uniform=True,\n",
    "    sample_percentage=1 / 3,\n",
    "    transforms=transforms,\n",
    ")\n",
    "\n",
    "vocab = PhoenixVocabulary(files)\n",
    "\n",
    "dm = Phoenix14TDatamodule(files, vocab, 3, data_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T00:09:33.864173Z",
     "iopub.status.busy": "2024-12-14T00:09:33.863869Z",
     "iopub.status.idle": "2024-12-14T00:09:33.929488Z",
     "shell.execute_reply": "2024-12-14T00:09:33.928694Z",
     "shell.execute_reply.started": "2024-12-14T00:09:33.864146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dm.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T00:09:34.643047Z",
     "iopub.status.busy": "2024-12-14T00:09:34.642135Z",
     "iopub.status.idle": "2024-12-14T00:09:37.089413Z",
     "shell.execute_reply": "2024-12-14T00:09:37.088539Z",
     "shell.execute_reply.started": "2024-12-14T00:09:34.643010Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "animate_torch_tensor(dm.train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T00:16:01.952066Z",
     "iopub.status.busy": "2024-12-14T00:16:01.951677Z",
     "iopub.status.idle": "2024-12-14T00:19:16.329557Z",
     "shell.execute_reply": "2024-12-14T00:19:16.328145Z",
     "shell.execute_reply.started": "2024-12-14T00:16:01.952033Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# flush cuda cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import lightning.pytorch as lp\n",
    "\n",
    "wandb.init(project=\"mobilevit\")\n",
    "\n",
    "# setup training and wandb\n",
    "\n",
    "BATCH_SIZE = 1  # we don't do padding, so batch size must be 1\n",
    "\n",
    "wandb_logger = lp.loggers.WandbLogger()\n",
    "\n",
    "model = xxs_mvit(vocab)\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator=\"cuda\",\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[\n",
    "        lp.callbacks.ModelCheckpoint(\n",
    "            monitor=\"val/loss\",\n",
    "            filename=\"best_model\",\n",
    "            save_top_k=1,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        lp.callbacks.EarlyStopping(\n",
    "            monitor=\"val/loss\",\n",
    "            patience=3,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        lp.callbacks.LearningRateMonitor(logging_interval=\"step\"),\n",
    "    ],\n",
    "    limit_train_batches=0.25,\n",
    "    accumulate_grad_batches=32,\n",
    ")\n",
    "\n",
    "wandb_logger.watch(model, log_graph=False)\n",
    "\n",
    "trainer.fit(model, datamodule=dm)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1049383,
     "sourceId": 1775725,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
