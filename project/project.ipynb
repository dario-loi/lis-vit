{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIS With MobileViTs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Our MobileViTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import wandb\n",
    "from einops import rearrange\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize this impl:\n",
    "# https://github.com/chinhsuanwu/mobilevit-pytorch/blob/master/mobilevit.py\n",
    "\n",
    "\n",
    "class Conv2DBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        groups=1,\n",
    "        bias=True,\n",
    "        norm=True,\n",
    "        activation=True,\n",
    "    ):\n",
    "        \"\"\"__init__ Constructor for Conv2DBlock\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            Number of input channels\n",
    "        out_channels : int\n",
    "            Number of output channels\n",
    "        kernel_size : int\n",
    "            Size of the kernel\n",
    "        stride : int\n",
    "            Stride of the convolutional layer\n",
    "        padding : int\n",
    "            Padding of the convolutional layer\n",
    "        groups : int\n",
    "            Number of groups\n",
    "        bias : bool\n",
    "            Whether to use bias\n",
    "        \"\"\"\n",
    "\n",
    "        super(Conv2DBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential()\n",
    "\n",
    "        self.block.add_module(\n",
    "            \"conv2d\",\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                groups=groups,\n",
    "                bias=bias,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if norm:\n",
    "            self.block.add_module(\"norm\", nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        if activation:\n",
    "            self.block.add_module(\"activation\", nn.SiLU())\n",
    "\n",
    "        self.block = torch.compile(self.block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class MobileBlockV2(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride, expand_ratio):\n",
    "        \"\"\"__init__ Constructor for MobileBlockV2\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            Number of input channels\n",
    "        out_channels : int\n",
    "            Number of output channels\n",
    "        stride : int\n",
    "            Stride of the convolutional layer\n",
    "        expand_ratio : int\n",
    "            Expansion ratio of the block\n",
    "        \"\"\"\n",
    "\n",
    "        super(MobileBlockV2, self).__init__()\n",
    "\n",
    "        assert stride in [1, 2], \"Stride must be either 1 or 2\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.hidden_dim = int(round(in_channels * expand_ratio))\n",
    "\n",
    "        self.mbv2 = nn.Sequential()\n",
    "        self.uses_inverse_residual = (\n",
    "            self.in_channels == self.out_channels and self.stride == 1\n",
    "        )\n",
    "\n",
    "        if self.expand_ratio == 1:\n",
    "            self.mbv2.add_module(\n",
    "                \"depthwise_3x3\",\n",
    "                Conv2DBlock(  # Depthwise Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=3,\n",
    "                    stride=stride,\n",
    "                    padding=1,\n",
    "                    groups=self.hidden_dim,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                ),\n",
    "            )\n",
    "            self.mbv2.add_module(\n",
    "                \"pointwise-linear_1x1\",\n",
    "                Conv2DBlock(  # Pointwise-Linear Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    groups=self.hidden_dim,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=False,\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            self.mbv2.add_module(\n",
    "                \"pointwise_1x1\",\n",
    "                Conv2DBlock(  # Pointwise-Linear Convolution\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    groups=1,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                ),\n",
    "            )\n",
    "            self.mbv2.add_module(\n",
    "                \"depthwise_3x3\",\n",
    "                Conv2DBlock(  # Depthwise Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.hidden_dim,\n",
    "                    kernel_size=3,\n",
    "                    stride=stride,\n",
    "                    padding=1,\n",
    "                    groups=self.hidden_dim,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                ),\n",
    "            )\n",
    "            self.mbv2.add_module(\n",
    "                \"pointwise-linear_1x1\",\n",
    "                Conv2DBlock(  # Pointwise-Linear Convolution\n",
    "                    in_channels=self.hidden_dim,\n",
    "                    out_channels=self.out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    groups=1,\n",
    "                    bias=False,\n",
    "                    norm=True,\n",
    "                    activation=False,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        self.mbv2 = torch.compile(self.mbv2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.uses_inverse_residual:\n",
    "            return x + self.mbv2(x)\n",
    "        else:\n",
    "            return self.mbv2(x)\n",
    "\n",
    "\n",
    "class MobileViTBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, hidden_dim, depth, channels, kernel_size, patch_size, mlp_dim, dropout=0.0\n",
    "    ):\n",
    "\n",
    "        super(MobileViTBlock, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.depth = depth\n",
    "        self.channels = channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.patch_size = patch_size\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.local_conv = torch.compile(\n",
    "            nn.Sequential(\n",
    "                Conv2DBlock(\n",
    "                    in_channels=channels,\n",
    "                    out_channels=channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=1,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                Conv2DBlock(\n",
    "                    in_channels=channels,\n",
    "                    out_channels=hidden_dim,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    norm=True,\n",
    "                    activation=True,\n",
    "                    bias=False,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        self.global_transformer = torch.compile(\n",
    "            nn.TransformerEncoder(\n",
    "                nn.TransformerEncoderLayer(\n",
    "                    d_model=hidden_dim,\n",
    "                    nhead=8,\n",
    "                    dim_feedforward=mlp_dim,\n",
    "                    dropout=dropout,\n",
    "                    batch_first=True,\n",
    "                    activation=\"gelu\",\n",
    "                ),\n",
    "                num_layers=depth,\n",
    "                norm=nn.LayerNorm(hidden_dim),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.fusion_conv_preres = torch.compile(\n",
    "            Conv2DBlock(\n",
    "                in_channels=hidden_dim,\n",
    "                out_channels=channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=False,\n",
    "                norm=True,\n",
    "                activation=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.fusion_conv_postres = torch.compile(\n",
    "            Conv2DBlock(\n",
    "                in_channels=2 * channels,\n",
    "                out_channels=channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "                norm=True,\n",
    "                activation=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_res = x.clone()\n",
    "\n",
    "        # local_repr\n",
    "        x = self.local_conv(x)\n",
    "\n",
    "        ph, pw = self.patch_size\n",
    "\n",
    "        # global_repr\n",
    "        _, _, h, w = x.shape\n",
    "        x = rearrange(  # reshape the image into patches for ViT input\n",
    "            x,\n",
    "            \"b d (h ph) (w pw) -> (b h w) (ph pw) d\",\n",
    "            ph=ph,\n",
    "            pw=pw,\n",
    "        )\n",
    "        x = self.global_transformer(x)\n",
    "        x = rearrange(\n",
    "            x,\n",
    "            \"(b h w) (ph pw) d -> b d (h ph) (w pw)\",\n",
    "            h=h // ph,\n",
    "            w=w // pw,\n",
    "            ph=ph,\n",
    "            pw=pw,\n",
    "        )\n",
    "\n",
    "        # fusion\n",
    "        x = self.fusion_conv_preres(x)\n",
    "        x = torch.cat([x, x_res], dim=1)\n",
    "        x = self.fusion_conv_postres(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MobileViT(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dims,\n",
    "        conv_channels,\n",
    "        num_classes,\n",
    "        expand_ratio=4,\n",
    "        patch_size=(2, 2),\n",
    "    ):\n",
    "        super(MobileViT, self).__init__()\n",
    "\n",
    "        self.dims = dims\n",
    "        self.conv_channels = conv_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.patch_size = patch_size\n",
    "        self.kernel_size = 3\n",
    "\n",
    "        L = [2, 4, 3]\n",
    "\n",
    "        self.in_conv = Conv2DBlock(\n",
    "            in_channels=3,\n",
    "            out_channels=conv_channels[0],\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            norm=True,\n",
    "            activation=True,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.mv2_blocks = nn.ModuleList([])\n",
    "\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[0], conv_channels[1], 1, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[1], conv_channels[2], 2, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[2], conv_channels[3], 1, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[2], conv_channels[3], 1, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[3], conv_channels[4], 2, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[5], conv_channels[6], 2, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "        self.mv2_blocks.append(\n",
    "            MobileBlockV2(\n",
    "                conv_channels[7], conv_channels[8], 2, expand_ratio=expand_ratio\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.mvit_blocks = nn.ModuleList([])\n",
    "\n",
    "        self.mvit_blocks.append(\n",
    "            MobileViTBlock(\n",
    "                dims[0],\n",
    "                L[0],\n",
    "                conv_channels[5],\n",
    "                self.kernel_size,\n",
    "                patch_size,\n",
    "                int(dims[0] * 2),\n",
    "            )\n",
    "        )\n",
    "        self.mvit_blocks.append(\n",
    "            MobileViTBlock(\n",
    "                dims[1],\n",
    "                L[1],\n",
    "                conv_channels[7],\n",
    "                self.kernel_size,\n",
    "                patch_size,\n",
    "                int(dims[1] * 4),\n",
    "            )\n",
    "        )\n",
    "        self.mvit_blocks.append(\n",
    "            MobileViTBlock(\n",
    "                dims[2],\n",
    "                L[2],\n",
    "                conv_channels[9],\n",
    "                self.kernel_size,\n",
    "                patch_size,\n",
    "                int(dims[2] * 4),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.final_pw = Conv2DBlock(\n",
    "            in_channels=conv_channels[-2],\n",
    "            out_channels=conv_channels[-1],\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            groups=1,\n",
    "            bias=False,\n",
    "            norm=True,\n",
    "            activation=False,\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(conv_channels[-1], num_classes, bias=False)\n",
    "\n",
    "        ## Training-related members\n",
    "        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "        self.apply(self.init_weights)  # Initialize weights\n",
    "\n",
    "    def init_weights(self, m):\n",
    "\n",
    "        if type(m) == nn.Conv2d:\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif type(m) == nn.BatchNorm2d:\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif type(m) == nn.Linear:\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.in_conv(x)\n",
    "\n",
    "        for i in range(5):\n",
    "            x = self.mv2_blocks[i](x)\n",
    "\n",
    "        x = self.mvit_blocks[0](x)\n",
    "        x = self.mv2_blocks[5](x)\n",
    "        x = self.mvit_blocks[1](x)\n",
    "        x = self.mv2_blocks[6](x)\n",
    "        x = self.mvit_blocks[2](x)\n",
    "        x = self.final_pw(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def compute_metrics(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute confusion matrix and metrics for multi-class classification.\n",
    "        \"\"\"\n",
    "        y_pred_classes = torch.argmax(y_pred, dim=1)\n",
    "        conf_matrix = torch.zeros(\n",
    "            (self.num_classes, self.num_classes), dtype=torch.int64\n",
    "        )\n",
    "        for t, p in zip(y_true.view(-1), y_pred_classes.view(-1)):\n",
    "            conf_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "        TP = torch.diag(conf_matrix)\n",
    "        FP = conf_matrix.sum(dim=0) - TP\n",
    "        FN = conf_matrix.sum(dim=1) - TP\n",
    "        TN = conf_matrix.sum() - (TP + FP + FN)\n",
    "\n",
    "        EPS = 1e-10\n",
    "        accuracy = y_pred_classes.eq(y_true).sum().item() / y_true.size(0)\n",
    "        precision = TP / (TP + FP + EPS)\n",
    "        recall = TP / (TP + FN + EPS)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + EPS)\n",
    "\n",
    "        # Class mean\n",
    "        overall_precision = precision.mean().item()\n",
    "        overall_recall = recall.mean().item()\n",
    "        overall_f1 = f1.mean().item()\n",
    "\n",
    "        return {\n",
    "            \"conf_matrix\": conf_matrix,\n",
    "            \"precision\": overall_precision,\n",
    "            \"recall\": overall_recall,\n",
    "            \"f1\": overall_f1,\n",
    "            \"accuracy\": accuracy,\n",
    "        }\n",
    "\n",
    "    def log_metrics(self, phase, loss, acc, prec, rec, f1):\n",
    "        self.log(f\"{phase}/loss\", loss, prog_bar=True)\n",
    "        self.log(f\"{phase}/accuracy\", acc, prog_bar=True)\n",
    "        self.log(f\"{phase}/precision\", prec, prog_bar=True)\n",
    "        self.log(f\"{phase}/recall\", rec, prog_bar=True)\n",
    "        self.log(f\"{phase}/f1\", f1, prog_bar=True)\n",
    "\n",
    "    def step(self, batch, phase):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        metrics = self.compute_metrics(y_hat, y)\n",
    "        acc = metrics[\"accuracy\"]\n",
    "        prec = metrics[\"precision\"]\n",
    "        rec = metrics[\"recall\"]\n",
    "        f1 = metrics[\"f1\"]\n",
    "\n",
    "        self.log_metrics(phase, loss, acc, prec, rec, f1)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), lr=2e-4, amsgrad=True, weight_decay=0.01\n",
    "        )\n",
    "        scheduler = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.utilities.model_summary import summarize\n",
    "from lightning.fabric.utilities import measure_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xxs_mvit(classes: int = 10):\n",
    "    return MobileViT(\n",
    "        dims=[64, 80, 96],\n",
    "        conv_channels=[16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 320],\n",
    "        num_classes=classes,\n",
    "        expand_ratio=2,\n",
    "        patch_size=(2, 2),\n",
    "    )\n",
    "\n",
    "\n",
    "def xs_mvit(classes: int = 10):\n",
    "    return MobileViT(\n",
    "        dims=[96, 120, 144],\n",
    "        conv_channels=[16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384],\n",
    "        num_classes=classes,\n",
    "        expand_ratio=4,\n",
    "        patch_size=(4, 4),\n",
    "    )\n",
    "\n",
    "\n",
    "def s_mvit(classes: int = 10):\n",
    "    return MobileViT(\n",
    "        dims=[144, 192, 240],\n",
    "        conv_channels=[16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 640],\n",
    "        num_classes=classes,\n",
    "        expand_ratio=4,\n",
    "        patch_size=(8, 8),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 dataset\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "LFWPeople_trainval = torchvision.datasets.LFWPeople(\n",
    "    root=\"./data\", split=\"train\", download=True, transform=transform\n",
    ")\n",
    "LFWPeople_test = torchvision.datasets.LFWPeople(\n",
    "    root=\"./data\", split=\"test\", download=True, transform=transform\n",
    ")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_size = int(0.8 * len(LFWPeople_trainval))\n",
    "val_size = len(LFWPeople_trainval) - train_size\n",
    "\n",
    "\n",
    "LFWPeople_train, LFWPeople_val = torch.utils.data.random_split(\n",
    "    LFWPeople_trainval,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kaggle_secrets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkaggle_secrets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UserSecretsClient\n\u001b[1;32m      5\u001b[0m user_secrets \u001b[38;5;241m=\u001b[39m UserSecretsClient()\n\u001b[1;32m      6\u001b[0m secret_value_0 \u001b[38;5;241m=\u001b[39m user_secrets\u001b[38;5;241m.\u001b[39mget_secret(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwandb_api\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kaggle_secrets'"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n",
    "\n",
    "wandb.login(key=secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ws4252k4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁███▁▁▁</td></tr><tr><td>train/f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▅▆▅▅▅▅▆▆▆▅▄▄▄▃▆▅▆██▃▃█▅▆▄▂▁▄▅▆▇▆▄▁▄▅▃▃</td></tr><tr><td>train/precision</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val/accuracy</td><td>▁</td></tr><tr><td>val/f1</td><td>▁</td></tr><tr><td>val/loss</td><td>▁</td></tr><tr><td>val/precision</td><td>▁</td></tr><tr><td>val/recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train/accuracy</td><td>0</td></tr><tr><td>train/f1</td><td>0</td></tr><tr><td>train/loss</td><td>8.19563</td></tr><tr><td>train/precision</td><td>0</td></tr><tr><td>train/recall</td><td>0</td></tr><tr><td>trainer/global_step</td><td>1904</td></tr><tr><td>val/accuracy</td><td>0.03727</td></tr><tr><td>val/f1</td><td>0</td></tr><tr><td>val/loss</td><td>8.37582</td></tr><tr><td>val/precision</td><td>0</td></tr><tr><td>val/recall</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rural-salad-15</strong> at: <a href='https://wandb.ai/academic_projects/mobilevit/runs/ws4252k4' target=\"_blank\">https://wandb.ai/academic_projects/mobilevit/runs/ws4252k4</a><br/> View project at: <a href='https://wandb.ai/academic_projects/mobilevit' target=\"_blank\">https://wandb.ai/academic_projects/mobilevit</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241203_130427-ws4252k4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ws4252k4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dario/repos/lis-vit/project/wandb/run-20241203_161521-97vuokbj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/academic_projects/mobilevit/runs/97vuokbj' target=\"_blank\">solar-bee-18</a></strong> to <a href='https://wandb.ai/academic_projects/mobilevit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/academic_projects/mobilevit' target=\"_blank\">https://wandb.ai/academic_projects/mobilevit</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/academic_projects/mobilevit/runs/97vuokbj' target=\"_blank\">https://wandb.ai/academic_projects/mobilevit/runs/97vuokbj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/academic_projects/mobilevit/runs/97vuokbj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7aa227b276d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"mobilevit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/dario/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | in_conv     | Conv2DBlock       | 464    | train\n",
      "1 | mv2_blocks  | ModuleList        | 183 K  | train\n",
      "2 | mvit_blocks | ModuleList        | 2.2 M  | train\n",
      "3 | final_pw    | Conv2DBlock       | 37.6 K | train\n",
      "4 | pool        | AdaptiveAvgPool2d | 0      | train\n",
      "5 | classifier  | Linear            | 2.2 M  | train\n",
      "6 | criterion   | CrossEntropyLoss  | 0      | train\n",
      "----------------------------------------------------------\n",
      "4.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 M     Total params\n",
      "18.489    Total estimated model params size (MB)\n",
      "345       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1905 [00:00<?, ?it/s]                          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1203 16:17:08.996000 24231 torch/_dynamo/convert_frame.py:844] [24/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W1203 16:17:08.996000 24231 torch/_dynamo/convert_frame.py:844] [24/8]    function: 'forward' (/tmp/ipykernel_24231/2669167420.py:64)\n",
      "W1203 16:17:08.996000 24231 torch/_dynamo/convert_frame.py:844] [24/8]    last reason: 24/0: GLOBAL_STATE changed: grad_mode \n",
      "W1203 16:17:08.996000 24231 torch/_dynamo/convert_frame.py:844] [24/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1203 16:17:08.996000 24231 torch/_dynamo/convert_frame.py:844] [24/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 51/1905 [00:29<18:08,  1.70it/s, v_num=okbj, train/loss=8.670, train/accuracy=0.000, train/precision=0.000, train/recall=0.000, train/f1=0.000] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1905/1905 [3:12:36<00:00,  0.16it/s, v_num=52k4, train/loss=8.370, train/accuracy=0.000, train/precision=0.000, train/recall=0.000, train/f1=0.000, val/loss=8.380, val/accuracy=0.0373, val/precision=0.000, val/recall=0.000, val/f1=0.000]\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dario/repos/lis-vit/.venv/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2374: UserWarning: Run (ws4252k4) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stdout\", data),\n",
      "/home/dario/repos/lis-vit/.venv/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2374: UserWarning: Run (x47z66mn) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stdout\", data),\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:212\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    211\u001b[0m dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m batch, _, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_fetcher)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fetchers.py:133\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fetchers.py:60\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/utilities/combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/utilities/combined_loader.py:78\u001b[0m, in \u001b[0;36m_MaxSizeCycle.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     out[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterators[i])\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m     success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success:\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-linux-x86_64-gnu/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-linux-x86_64-gnu/lib/python3.11/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-linux-x86_64-gnu/lib/python3.11/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-linux-x86_64-gnu/lib/python3.11/multiprocessing/connection.py:948\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 948\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-linux-x86_64-gnu/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 42\u001b[0m\n\u001b[1;32m     21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     22\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     23\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     ],\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     40\u001b[0m wandb_logger\u001b[38;5;241m.\u001b[39mwatch(model, log_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 42\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/lis-vit/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "import lightning.pytorch as lp\n",
    "\n",
    "# setup training and wandb\n",
    "\n",
    "N_LABELS = len(LFWPeople_trainval.class_to_idx)\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "wandb_logger = lp.loggers.WandbLogger()\n",
    "\n",
    "model = xs_mvit(N_LABELS)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    LFWPeople_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=3\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    LFWPeople_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=3\n",
    ")\n",
    "test_loader = DataLoader(LFWPeople_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"cuda\",\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[\n",
    "        lp.callbacks.ModelCheckpoint(\n",
    "            monitor=\"val/loss\",\n",
    "            filename=\"best_model\",\n",
    "            save_top_k=1,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        lp.callbacks.EarlyStopping(\n",
    "            monitor=\"val/ loss\",\n",
    "            patience=3,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "wandb_logger.watch(model, log_graph=False)\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flush cuda cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab {'A': 0, 'AACHEN': 1, 'AB': 2, 'AB-JETZT': 3, 'AB-PLUSPLUS': 4, 'ABEND': 5, 'ABER': 6, 'ABFALLEN': 7, 'ABSCHIED': 8, 'ABSCHNITT': 9, 'ABSINKEN': 10, 'ABWECHSELN': 11, 'ACH': 12, 'ACHT': 13, 'ACHTE': 14, 'ACHTHUNDERT': 15, 'ACHTUNG': 16, 'ACHTZEHN': 17, 'ACHTZIG': 18, 'AEHNLCH': 19, 'AEHNLICH': 20, 'AENDERN': 21, 'AFRIKA': 22, 'AKTIV': 23, 'AKTUELL': 24, 'ALLE': 25, 'ALLGAEU': 26, 'ALLGEMEIN': 27, 'ALPEN': 28, 'ALPENRAND': 29, 'ALPENTAL': 30, 'ALS': 31, 'ALSO': 32, 'ALT': 33, 'AM': 34, 'AM-KUESTE': 35, 'AM-MEER': 36, 'AM-RAND': 37, 'AM-TAG': 38, 'AMERIKA': 39, 'AN': 40, 'ANDERE': 41, 'ANDERE-MOEGLICHKEIT': 42, 'ANDERS': 43, 'ANFANG': 44, 'ANGEMESSEN': 45, 'ANGENEHM': 46, 'ANGST': 47, 'ANHALT': 48, 'ANKOMMEN': 49, 'ANSAMMELN': 50, 'ANSCHAUEN': 51, 'APRIL': 52, 'ARM': 53, 'ATLANTIK': 54, 'AUCH': 55, 'AUCH-NICHT': 56, 'AUF': 57, 'AUF-JEDEN-FALL': 58, 'AUFBLUEHEN': 59, 'AUFEINANDERTREFFEN': 60, 'AUFFUELLEN': 61, 'AUFHEITERN': 62, 'AUFHOEREN': 63, 'AUFKLAREN': 64, 'AUFKOMMEN': 65, 'AUFLOCKERUNG': 66, 'AUFLOCKERUNG-PLUSPLUS': 67, 'AUFLOESEN': 68, 'AUFLOESEN-PLUSPLUS': 69, 'AUFPASSEN': 70, 'AUFTAUCHEN': 71, 'AUFZIEHEN': 72, 'AUFZIEHEN-PLUSPLUS': 73, 'AUGUST': 74, 'AUS': 75, 'AUSEINANDER': 76, 'AUSHALTEN': 77, 'AUSNAHME': 78, 'AUSRICHTEN': 79, 'AUSSEHEN': 80, 'AUSSERGEWOEHNLICH': 81, 'AUSWAEHLEN-PLUSPLUS': 82, 'AUTO': 83, 'AUTOMATISCH': 84, 'B': 85, 'BAD': 86, 'BADEN': 87, 'BADEN-WUERTTEMBERG': 88, 'BALD': 89, 'BAUER': 90, 'BAUM': 91, 'BAYERN': 92, 'BEDECKT': 93, 'BEDEUTEN': 94, 'BEDINGUNGEN': 95, 'BEGINN': 96, 'BEGRUESSEN': 97, 'BEI-UNS': 98, 'BEISEITE': 99, 'BEISPIEL': 100, 'BEKANNT': 101, 'BEKANNTGEBEN': 102, 'BEKOMMEN': 103, 'BELAESTIGUNG': 104, 'BELGIEN': 105, 'BEOBACHTEN': 106, 'BEREICH': 107, 'BERG': 108, 'BERGAB': 109, 'BERGAUF': 110, 'BERLIN': 111, 'BERUF': 112, 'BERUHIGEN': 113, 'BESONDERS': 114, 'BESPRECHEN': 115, 'BESSER': 116, 'BESSER-PLUSPLUS': 117, 'BESTIMMT': 118, 'BETROFFEN': 119, 'BETT': 120, 'BEWEGEN': 121, 'BEWOELKT': 122, 'BEWOELKT-PLUSPLUS': 123, 'BIS': 124, 'BIS-JETZT': 125, 'BIS-MITTE': 126, 'BIS-MORGEN': 127, 'BISHER': 128, 'BISSCHEN': 129, 'BISSCHEN-PLUSPLUS': 130, 'BITTE': 131, 'BLATT': 132, 'BLAU': 133, 'BLEIBEN': 134, 'BLEIBEN-GLEICH': 135, 'BLITZ': 136, 'BLOCKIEREN': 137, 'BLUETE': 138, 'BLUMEN-PLUSPLUS': 139, 'BODEN': 140, 'BODENSEE': 141, 'BOEE': 142, 'BRAND': 143, 'BRANDENBURG': 144, 'BRAUCHEN': 145, 'BRAUN': 146, 'BREMEN': 147, 'BRINGEN': 148, 'BRITANNIEN': 149, 'BROCKEN': 150, 'BRUCKBERG': 151, 'BUNT': 152, 'BURG': 153, 'C': 154, 'CHANCE': 155, 'CHANCE-PLUSPLUS': 156, 'CHAOS': 157, 'D': 158, 'DA': 159, 'DABEI': 160, 'DABEI-PLUSPLUS': 161, 'DAENEMARK': 162, 'DAFUER': 163, 'DAMEN': 164, 'DANACH': 165, 'DANEBEN': 166, 'DANN': 167, 'DARAUF': 168, 'DARUM': 169, 'DARUNTER': 170, 'DAS-IST-ES': 171, 'DAS-WAR-ES': 172, 'DASSELBE': 173, 'DAUER': 174, 'DAUERND': 175, 'DAZU': 176, 'DAZWISCHEN': 177, 'DEMNAECHST': 178, 'DENKEN': 179, 'DESHALB': 180, 'DESWEGEN': 181, 'DEUTSCH': 182, 'DEUTSCHLAND': 183, 'DEUTSCHLANDRAUM': 184, 'DEZEMBER': 185, 'DICHT': 186, 'DICK': 187, 'DIENST': 188, 'DIENSTAG': 189, 'DIESE': 190, 'DIESE-WOCHE': 191, 'DIESMAL': 192, 'DOCH': 193, 'DOCH-SONST-NOCH': 194, 'DONNER': 195, 'DONNERSTAG': 196, 'DRAUSSEN': 197, 'DREHEN': 198, 'DREI': 199, 'DREI-MONATE': 200, 'DREIDREISSIG': 201, 'DREIHUNDERT': 202, 'DREIMAL': 203, 'DREISSIG': 204, 'DREIZEHN': 205, 'DRESDEN': 206, 'DRITTE': 207, 'DRUCK': 208, 'DRUCKFLAECHE': 209, 'DU': 210, 'DUENN': 211, 'DUESSELDORF': 212, 'DUMM': 213, 'DUNST': 214, 'DURCH': 215, 'DURCHEINANDER': 216, 'DURCHGEHEND': 217, 'DURCHSCHNITT': 218, 'E': 219, 'EBEN': 220, 'ECHT': 221, 'EIFEL': 222, 'EIGENTLICH': 223, 'EIN': 224, 'EIN-BISSCHEN': 225, 'EIN-JAHR': 226, 'EIN-PAAR': 227, 'EIN-WOCHE': 228, 'EINE': 229, 'EINFACH': 230, 'EINFLUSS': 231, 'EINHUNDERT': 232, 'EINIGE': 233, 'EINIGERMASSEN': 234, 'EINLUSS': 235, 'EINS': 236, 'EINSCHRAENKEN': 237, 'EINZELN-PLUSPLUS': 238, 'EIS': 239, 'EISEN': 240, 'ELF': 241, 'ELFTE': 242, 'EMPFINDLICH': 243, 'ENDE': 244, 'ENDLICH': 245, 'ENGLAND': 246, 'ENORM': 247, 'ENTFERNT': 248, 'ENTHALTEN': 249, 'ENTSCHULDIGUNG': 250, 'ENTSPANNT': 251, 'ENTWICKELN': 252, 'ERDRUTSCH': 253, 'ERFAHREN': 254, 'ERFAHRUNG': 255, 'ERFURT': 256, 'ERHOEHEN': 257, 'ERLEICHERT': 258, 'ERSCHROCKEN': 259, 'ERST': 260, 'ERSTE': 261, 'ERSTMAL': 262, 'ERWARTEN': 263, 'ERZ': 264, 'ES-BEDEUTET': 265, 'ES-GIBT': 266, 'ETWAS': 267, 'EUCH': 268, 'EUROPA': 269, 'EWIG': 270, 'EXTREM': 271, 'F': 272, 'FACH': 273, 'FAHREN': 274, 'FALLEN': 275, 'FAST': 276, 'FEBRUAR': 277, 'FEHLT': 278, 'FEIER': 279, 'FELD': 280, 'FERTIG': 281, 'FEST': 282, 'FEUCHT': 283, 'FINNLAND': 284, 'FLACH': 285, 'FLAECHENDECKEND': 286, 'FLIESSEN': 287, 'FLOCKEN': 288, 'FLUSS': 289, 'FLUT': 290, 'FOEHN': 291, 'FOLGE': 292, 'FRAGEZEICHEN': 293, 'FRANKFURT': 294, 'FRANKREICH': 295, 'FREI': 296, 'FREITAG': 297, 'FREIZEIT': 298, 'FREUEN': 299, 'FREUNDLICH': 300, 'FRISCH': 301, 'FRONT': 302, 'FROST': 303, 'FROST-PLUSPLUS': 304, 'FRUEH': 305, 'FRUEHLING': 306, 'FUEHLEN': 307, 'FUEHLEN-WIE': 308, 'FUENF': 309, 'FUENF-TAGE': 310, 'FUENF-UHR': 311, 'FUENFHUNDERT': 312, 'FUENFTE': 313, 'FUENFZEHN': 314, 'FUENFZIG': 315, 'FUER': 316, 'FUER-ALLE': 317, 'FUER-UNS': 318, 'G': 319, 'GANZTAGS': 320, 'GEBEN': 321, 'GEBIRGE': 322, 'GEFAHR': 323, 'GEFRIEREN': 324, 'GEHEN': 325, 'GEHOERT': 326, 'GEHT-SO': 327, 'GELB': 328, 'GEMISCHT': 329, 'GEMUETLICH': 330, 'GENAU': 331, 'GENIESSEN': 332, 'GENUG': 333, 'GERADE': 334, 'GESAMT': 335, 'GESCHWINDIGKEIT': 336, 'GESTERN': 337, 'GETRENNT': 338, 'GEWESEN': 339, 'GEWITTER': 340, 'GEWITTER-PLUSPLUS': 341, 'GEWOHNT': 342, 'GIPFEL': 343, 'GLATT': 344, 'GLATTEIS': 345, 'GLAUBEN': 346, 'GLEICH': 347, 'GLEICH-BLEIBEN': 348, 'GLEICH-PLUSPLUS': 349, 'GLEICH-WIE': 350, 'GLUECK': 351, 'GOLD': 352, 'GOTT': 353, 'GRAD': 354, 'GRAD-PLUSPLUS': 355, 'GRAU': 356, 'GRAUPEL': 357, 'GRAUPEL-PLUSPLUS': 358, 'GRENZE': 359, 'GRIECHENLAND': 360, 'GROB': 361, 'GROSS': 362, 'GROSSBRITANNIEN': 363, 'GRUEN': 364, 'GRUND': 365, 'GUT': 366, 'GUT-ABEND': 367, 'H': 368, 'HAAR': 369, 'HABEN': 370, 'HABEN2': 371, 'HAELFTE': 372, 'HAFTEN': 373, 'HAGEL': 374, 'HAGEL-PLUSPLUS': 375, 'HALB': 376, 'HALLO': 377, 'HALTEN': 378, 'HAMBURG': 379, 'HANNOVER': 380, 'HARMLOS': 381, 'HART': 382, 'HAUPT': 383, 'HAUPTSAECHLICH': 384, 'HAVEN': 385, 'HEFTIG': 386, 'HEILIG': 387, 'HEILIGE': 388, 'HEISS': 389, 'HELL': 390, 'HERAB': 391, 'HERBST': 392, 'HERREN': 393, 'HERVORRAGEND': 394, 'HERZ': 395, 'HESSEN': 396, 'HEUTE': 397, 'HEUTE-ABEND': 398, 'HEUTE-MITTAG': 399, 'HEUTE-NACHT': 400, 'HIER': 401, 'HIMMEL': 402, 'HINDERNIS': 403, 'HOCH': 404, 'HOCHWASSER': 405, 'HOEHE': 406, 'HOEHER': 407, 'HOEREN': 408, 'HOFFEN': 409, 'HOLEN': 410, 'HOLLAND': 411, 'HOLSTEIN': 412, 'HUND': 413, 'HUNDERT': 414, 'HUT': 415, 'I': 416, 'ICH': 417, 'IHR': 418, 'IM': 419, 'IM-LAUFE': 420, 'IM-MOMENT': 421, 'IM-VERLAUF': 422, 'IMMER': 423, 'IN': 424, 'IN-BESTIMMT-ZEIT': 425, 'IN-DIESE-WOCHE': 426, 'IN-KOMMEND': 427, 'IN-KOMMEND-TAG': 428, 'IN-KOMMEND-ZEIT': 429, 'IN-KUERZE': 430, 'IN-PAAR-TAG': 431, 'IN-PAAR-TAGE-SPAETER': 432, 'INFORMIEREN': 433, 'INNERHALB': 434, 'INSEL': 435, 'INSEL-PLUSPLUS': 436, 'INSGESAMT': 437, 'INTERESSANT': 438, 'INTERNET': 439, 'IRGENDWANN': 440, 'IRLAND': 441, 'ISLAND': 442, 'ITALIEN': 443, 'IX': 444, 'J': 445, 'JA': 446, 'JAHR': 447, 'JANUAR': 448, 'JEDEN-TAG': 449, 'JETZT': 450, 'JULI': 451, 'JUNI': 452, 'K': 453, 'KALT': 454, 'KALT-PLUSPLUS': 455, 'KALTFRONT': 456, 'KANADA': 457, 'KANAL': 458, 'KANN': 459, 'KAPUTTGEGANGEN': 460, 'KARFREITAG': 461, 'KAUM': 462, 'KAUM-PLUSPLUS': 463, 'KEIN': 464, 'KIEL': 465, 'KILOMETER': 466, 'KLAPPEN': 467, 'KLAR': 468, 'KLAR-PLUSPLUS': 469, 'KLEIN': 470, 'KLEINIGKEIT': 471, 'KNAPP': 472, 'KOBLENZ': 473, 'KOELN': 474, 'KOENNEN': 475, 'KOENNEN-PLUSPLUS': 476, 'KOERPER': 477, 'KOERPER-PLUSPLUS': 478, 'KOMMA': 479, 'KOMMEN': 480, 'KOMMEN-PLUSPLUS': 481, 'KOMPLETT': 482, 'KONSTANT': 483, 'KORB': 484, 'KRAEFTIG': 485, 'KRATZEN': 486, 'KRISE': 487, 'KROATIEN': 488, 'KUCHEN': 489, 'KUEHL': 490, 'KUEHL-PLUSPLUS': 491, 'KUEHLER': 492, 'KUESTE': 493, 'KURVE': 494, 'KURZ': 495, 'L': 496, 'LAERM-PLUSPLUS': 497, 'LAGE': 498, 'LAHM': 499, 'LAND': 500, 'LANDSCHAFT': 501, 'LANG': 502, 'LANGSAM': 503, 'LAUFEN': 504, 'LAUSITZ': 505, 'LEBEN': 506, 'LEICHT': 507, 'LEIDER': 508, 'LEIPZIG': 509, 'LESEN': 510, 'LETZTE': 511, 'LETZTE-WOCHE': 512, 'LEUTE': 513, 'LICHT': 514, 'LIEB': 515, 'LIEGEN': 516, 'LITER': 517, 'LOCH': 518, 'LOCH-PLUSPLUS': 519, 'LOCKER': 520, 'LOS': 521, 'LUECKE': 522, 'LUFT': 523, 'M': 524, 'MACHEN': 525, 'MAERZ': 526, 'MAESSIG': 527, 'MAESSIG-PLUSPLUS': 528, 'MAI': 529, 'MAINZ': 530, 'MAL': 531, 'MAL-AB-ZU': 532, 'MAL-PLUSPLUS': 533, 'MANCHMAL': 534, 'MANCHMAL-PLUSPLUS': 535, 'MARKT': 536, 'MASCHINE': 537, 'MATSCH': 538, 'MAXIMAL': 539, 'MECKLENBURG': 540, 'MECKLENBURG-VORPOMMERN': 541, 'MEER': 542, 'MEHR': 543, 'MEHR-PLUSPLUS': 544, 'MEHR-WENIG': 545, 'MEHRMALS': 546, 'MEINEN': 547, 'MEISTENS': 548, 'MERKEN': 549, 'MERKWUERDIG': 550, 'MESSEN': 551, 'METER': 552, 'MILD': 553, 'MINDESTENS': 554, 'MINUS': 555, 'MISCHUNG': 556, 'MIT': 557, 'MITBEKOMMEN': 558, 'MITEILEN': 559, 'MITNEHMEN': 560, 'MITTAG': 561, 'MITTE': 562, 'MITTEGEBIRGE': 563, 'MITTEILEN': 564, 'MITTWOCH': 565, 'MITZIEHEN': 566, 'MM': 567, 'MOEGEN': 568, 'MOEGLICH': 569, 'MOMENT': 570, 'MOND': 571, 'MONTAG': 572, 'MORGEN': 573, 'MORGEN-FRUEH': 574, 'MORGENS': 575, 'MOSKAU': 576, 'MUENCHEN': 577, 'MUENSTER': 578, 'MUESSEN': 579, 'MUND': 580, 'N': 581, 'NACH': 582, 'NACH-HAUSE': 583, 'NACHMITTAG': 584, 'NACHT': 585, 'NAECHSTE': 586, 'NAECHSTE-WOCHE': 587, 'NAEHE': 588, 'NAH': 589, 'NASS': 590, 'NATUR': 591, 'NEBEL': 592, 'NEIN': 593, 'NEU': 594, 'NEUN': 595, 'NEUNHUNDERT': 596, 'NEUNTE': 597, 'NEUNZEHN': 598, 'NEUNZEHNTE': 599, 'NEUNZIG': 600, 'NICHT': 601, 'NICHT-ACHTZEHN': 602, 'NICHT-BEDEUTEN': 603, 'NICHT-DEUTSCH': 604, 'NICHT-DIESE-WOCHE': 605, 'NICHT-EINFLUSS': 606, 'NICHT-FROST': 607, 'NICHT-FUEHLEN': 608, 'NICHT-FUENF': 609, 'NICHT-GEMUETLICH': 610, 'NICHT-GENUG': 611, 'NICHT-GEWITTER': 612, 'NICHT-GLEICH': 613, 'NICHT-GRAD': 614, 'NICHT-HABEN': 615, 'NICHT-HART': 616, 'NICHT-HEISS': 617, 'NICHT-HOEHE': 618, 'NICHT-IMMER': 619, 'NICHT-KALT': 620, 'NICHT-KAUM': 621, 'NICHT-KEIN': 622, 'NICHT-KLAR': 623, 'NICHT-KOENNEN': 624, 'NICHT-KOMMEN': 625, 'NICHT-MEHR': 626, 'NICHT-MEISTENS': 627, 'NICHT-NACHT': 628, 'NICHT-NAJA': 629, 'NICHT-NEBEL': 630, 'NICHT-NEIN': 631, 'NICHT-NICHT-MEHR': 632, 'NICHT-NICHT-NUR': 633, 'NICHT-NICHTS': 634, 'NICHT-NOCH-NICHT': 635, 'NICHT-NORD': 636, 'NICHT-NULL': 637, 'NICHT-REGEN': 638, 'NICHT-REGEN-PLUSPLUS': 639, 'NICHT-REGION': 640, 'NICHT-RICHTIG': 641, 'NICHT-SCHEINEN': 642, 'NICHT-SCHLECHT': 643, 'NICHT-SCHLIMM': 644, 'NICHT-SCHNEE': 645, 'NICHT-SCHOEN': 646, 'NICHT-SEHEN': 647, 'NICHT-SELTEN': 648, 'NICHT-SONNE': 649, 'NICHT-SPUEREN': 650, 'NICHT-STARK': 651, 'NICHT-TEIL': 652, 'NICHT-THEMA': 653, 'NICHT-TROCKEN': 654, 'NICHT-VIEL': 655, 'NICHT-VON': 656, 'NICHT-WARM': 657, 'NICHT-WARTEN': 658, 'NICHT-ZU-WARM': 659, 'NICHT-cl-KOMMEN': 660, 'NICHTALP-AUCH': 661, 'NICHTALP-BRAUCHEN': 662, 'NICHTALP-GIBT': 663, 'NICHTALP-KEIN': 664, 'NICHTALP-KOENNEN': 665, 'NICHTALP-MUSS': 666, 'NICHTALP-NICHT': 667, 'NICHTALP-PASSEN': 668, 'NICHTALP-STIMMT': 669, 'NICHTS': 670, 'NIEDER': 671, 'NIEDERSACHSEN': 672, 'NIEDERUNG': 673, 'NIEDRIG': 674, 'NIESELREGEN': 675, 'NN': 676, 'NOCH': 677, 'NOCH-MEHR': 678, 'NOCH-NICHT': 679, 'NOCH-PLUSPLUS': 680, 'NOCHEINMAL': 681, 'NORD': 682, 'NORDOST': 683, 'NORDOSTRAUM': 684, 'NORDPOL': 685, 'NORDRAUM': 686, 'NORDRHEIN': 687, 'NORDRHEIN-WESTFALEN': 688, 'NORDSEE': 689, 'NORDWEST': 690, 'NORDWESTRAUM': 691, 'NORMAL': 692, 'NORWEGEN': 693, 'NOVEMBER': 694, 'NRW': 695, 'NULL': 696, 'NUMMER': 697, 'NUR': 698, 'O': 699, 'OB': 700, 'OBEN': 701, 'OBER': 702, 'OBWOHL': 703, 'ODER': 704, 'OESTERREICH': 705, 'OFT': 706, 'OFT-PLUSPLUS': 707, 'OHNE': 708, 'OKTOBER': 709, 'ORANGE': 710, 'ORKAN': 711, 'ORT': 712, 'ORT-PLUSPLUS': 713, 'OST': 714, 'OSTERN': 715, 'OSTRAUM': 716, 'P': 717, 'PAAR': 718, 'PAAR-TAG': 719, 'PASSEN': 720, 'PASSIEREN': 721, 'PAUSE': 722, 'PFALZ': 723, 'PFEIL': 724, 'PFINGSTEN': 725, 'PFLANZE': 726, 'PLOETZLICH': 727, 'PLUS': 728, 'POLEN': 729, 'POMMERN': 730, 'PORTUGAL': 731, 'POSITIV': 732, 'PRO': 733, 'PROBLEM': 734, 'PROZENT': 735, 'PUENKTLICH': 736, 'PULLOVER': 737, 'PUNKT': 738, 'QUADRAT': 739, 'QUADRATMETER': 740, 'QUELL': 741, 'R': 742, 'RAND': 743, 'RAUM': 744, 'RAUSFALLEN': 745, 'RECHNEN': 746, 'REDUZIEREN': 747, 'REGEN': 748, 'REGEN-AUF-ALPEN': 749, 'REGEN-PLUSPLUS': 750, 'REGION': 751, 'REGION-PLUSPLUS': 752, 'REIF': 753, 'REIN': 754, 'REKORD': 755, 'REST': 756, 'RHEIN': 757, 'RHEINDLAND': 758, 'RHEINLAND': 759, 'RHEINLAND-PFALZ': 760, 'RICHTIG': 761, 'RICHTUNG': 762, 'RISIKO': 763, 'RODELN': 764, 'ROSTOCK': 765, 'ROT': 766, 'RUECKEN': 767, 'RUEGEN': 768, 'RUHIG': 769, 'RUHRGEBIET': 770, 'RUMAENIEN': 771, 'RUND-UM-DIE-UHR': 772, 'RUSSLAND': 773, 'S': 774, 'S0NNE': 775, 'SAARLAND': 776, 'SACHSEN': 777, 'SAGE': 778, 'SAGEN': 779, 'SAMSTAG': 780, 'SAND': 781, 'SAUER': 782, 'SCH': 783, 'SCHADEN': 784, 'SCHAETZEN': 785, 'SCHAFFEN': 786, 'SCHAU-MAL': 787, 'SCHAUEN': 788, 'SCHAUER': 789, 'SCHAUER-PLUSPLUS': 790, 'SCHEINEN': 791, 'SCHEINEN-PLUSPLUS': 792, 'SCHIRM': 793, 'SCHLAF': 794, 'SCHLAGSAHNE': 795, 'SCHLECHT': 796, 'SCHLECHTER': 797, 'SCHLESWIG': 798, 'SCHLIMM': 799, 'SCHLIMMER': 800, 'SCHLUSS': 801, 'SCHMELZEN': 802, 'SCHNEE': 803, 'SCHNEE-PLUSPLUS': 804, 'SCHNEIEN': 805, 'SCHNEIEN-PLUSPLUS': 806, 'SCHNELL': 807, 'SCHOEN': 808, 'SCHON': 809, 'SCHON-WIEDER': 810, 'SCHOTTLAND': 811, 'SCHRANK': 812, 'SCHUETZEN': 813, 'SCHULD': 814, 'SCHWACH': 815, 'SCHWACH-PLUSPLUS': 816, 'SCHWARZ': 817, 'SCHWEDEN': 818, 'SCHWER': 819, 'SCHWIERIG': 820, 'SCHWIERIG-PLUSPLUS': 821, 'SCHWITZEN': 822, 'SCHWUEL': 823, 'SECHS': 824, 'SECHSHUNDERT': 825, 'SECHSTE': 826, 'SECHSZEHN': 827, 'SEE': 828, 'SEHEN': 829, 'SEHR': 830, 'SEI-DANK': 831, 'SEIT': 832, 'SEITE': 833, 'SELBE': 834, 'SELTEN': 835, 'SEPTEMBER': 836, 'SICHER': 837, 'SIE': 838, 'SIEBEN': 839, 'SIEBEN-WOCHE': 840, 'SIEBENHUNDERT': 841, 'SIEBTE': 842, 'SIEBZEHN': 843, 'SIEBZIG': 844, 'SINKEN': 845, 'SITUATION': 846, 'SITZ': 847, 'SKANDINAVIEN': 848, 'SKI': 849, 'SO': 850, 'SO-BLEIBEN': 851, 'SOLL': 852, 'SOMMER': 853, 'SONNE': 854, 'SONNE-PLUSPLUS': 855, 'SONNENUNTERGANG': 856, 'SONNTAG': 857, 'SONST': 858, 'SOWIESO': 859, 'SPAET': 860, 'SPAETER': 861, 'SPAETESTEN': 862, 'SPANIEN': 863, 'SPAZIEREN': 864, 'SPEZIELL': 865, 'SPITZE': 866, 'SPORT': 867, 'SPRIESSEN': 868, 'SPUEREN': 869, 'STABIL': 870, 'STADT': 871, 'STAMM': 872, 'STARK': 873, 'START': 874, 'STAU': 875, 'STEHEN': 876, 'STEIGEN': 877, 'STEIGEN-OBEN': 878, 'STEIGEN-RUNTER': 879, 'STEIGEN-RUNTER-PLUSPLUS': 880, 'STEIN': 881, 'STELLENWEISE': 882, 'STERN': 883, 'STERN-PLUSPLUS': 884, 'STIMMT': 885, 'STOCKEN': 886, 'STOERUNG': 887, 'STRAHLEN': 888, 'STRASSE': 889, 'STRENG': 890, 'STROEMEN': 891, 'STROM': 892, 'STUNDE': 893, 'STURM': 894, 'STURM-PLUSPLUS': 895, 'STUTTGART': 896, 'SUCHEN': 897, 'SUED': 898, 'SUED-PLUSPLUS': 899, 'SUEDOST': 900, 'SUEDOSTRAUM': 901, 'SUEDRAUM': 902, 'SUEDWEST': 903, 'SUEDWESTRAUM': 904, 'SUPER': 905, 'SYLT': 906, 'T': 907, 'T-SHIRT': 908, 'TAG': 909, 'TAGE': 910, 'TAGSUEBER': 911, 'TAL': 912, 'TANKEN': 913, 'TATSAECHLICH': 914, 'TAU': 915, 'TAUEN': 916, 'TAUGEN': 917, 'TAUSEND': 918, 'TEIL': 919, 'TEILWEISE': 920, 'TEMPERATUR': 921, 'TEMPERATUR-PLUSPLUS': 922, 'TEXT': 923, 'THEMA': 924, 'THUERINGEN': 925, 'TIEF': 926, 'TJA': 927, 'TOLL': 928, 'TRAUM': 929, 'TRENNEN': 930, 'TROCKEN': 931, 'TROPFEN': 932, 'TROPFEN-PLUSPLUS': 933, 'TROPISCH': 934, 'TROTZDEM': 935, 'TRUEB': 936, 'TRUEB-PLUSPLUS': 937, 'TSCHECHIEN': 938, 'TSCHUESS': 939, 'TUERKEI': 940, 'TUERKEI-PLUSPLUS': 941, 'TUN': 942, 'TYPISCH': 943, 'U': 944, 'UEBER': 945, 'UEBER-UNTER': 946, 'UEBERALL': 947, 'UEBERFLUTUNG': 948, 'UEBERMORGEN': 949, 'UEBERSCHWEMMUNG': 950, 'UEBERWIEGEND': 951, 'UHR': 952, 'UM': 953, 'UM-PLUSPLUS': 954, 'UMKEHREN': 955, 'UMSTAENDLICH': 956, 'UMSTELLEN': 957, 'UMWANDELN': 958, 'UND': 959, 'UND-DANN': 960, 'UND-SO-WEITER': 961, 'UNGARN': 962, 'UNGEFAEHR': 963, 'UNGEMUETLICH': 964, 'UNSER': 965, 'UNSICHER': 966, 'UNTEN': 967, 'UNTER': 968, 'UNTERNEHMEN': 969, 'UNTERSCHIED': 970, 'UNTERSCHIED-PLUSPLUS': 971, 'UNWAHRSCHEINLICH': 972, 'UNWETTER': 973, 'URLAUB': 974, 'V': 975, 'VERAENDERN': 976, 'VERANTWORTLICH': 977, 'VERBINDEN': 978, 'VERBREITEN': 979, 'VERDICHTEN': 980, 'VEREINZELT': 981, 'VERGLEICH': 982, 'VERKEHR': 983, 'VERLAUFEN': 984, 'VERMEIDEN': 985, 'VERRINGERN': 986, 'VERSCHIEBEN': 987, 'VERSCHIEDEN': 988, 'VERSCHWINDEN': 989, 'VERSCHWINDEN-PLUSPLUS': 990, 'VERSPAETET': 991, 'VERSUCHEN': 992, 'VERTEILEN': 993, 'VERTREIBEN': 994, 'VERWOEHNT': 995, 'VIDEO': 996, 'VIEL': 997, 'VIELLEICHT': 998, 'VIER': 999, 'VIERHUNDERT': 1000, 'VIERTE': 1001, 'VIERZEHN': 1002, 'VIERZIG': 1003, 'VOGEL': 1004, 'VOLL': 1005, 'VON': 1006, 'VOR': 1007, 'VOR-ALLEM': 1008, 'VOR-LETZTEN-TAGEN': 1009, 'VORAUS': 1010, 'VORAUSSAGE': 1011, 'VORBEI': 1012, 'VORBEREITEN': 1013, 'VORDERSCHEIBE': 1014, 'VORHER': 1015, 'VORMITTAG': 1016, 'VORPOMMERN': 1017, 'VORSICHT': 1018, 'VORSTELLEN': 1019, 'VORTEIL': 1020, 'VORUEBERGEHEND': 1021, 'W': 1022, 'WACHSEN': 1023, 'WAHR': 1024, 'WAHRSCHEINLICH': 1025, 'WALD': 1026, 'WANN': 1027, 'WAR': 1028, 'WARM': 1029, 'WARNUNG': 1030, 'WARSCHEINLICH': 1031, 'WARTEN': 1032, 'WARUM': 1033, 'WAS': 1034, 'WASCH': 1035, 'WASSER': 1036, 'WASSER-STEIGEN': 1037, 'WECHSEL': 1038, 'WECHSELHAFT': 1039, 'WECHSELHAFT-PLUSPLUS': 1040, 'WEG': 1041, 'WEHEN': 1042, 'WEHEN-PLUSPLUS': 1043, 'WEIBER': 1044, 'WEIHNACHT': 1045, 'WEIHNACHTEN': 1046, 'WEIL': 1047, 'WEIN': 1048, 'WEIT': 1049, 'WEIT-SEHEN': 1050, 'WEITER': 1051, 'WENIG': 1052, 'WENIGER': 1053, 'WENN': 1054, 'WER': 1055, 'WERDEN': 1056, 'WERT': 1057, 'WESER': 1058, 'WEST': 1059, 'WESTRAUM': 1060, 'WETTER': 1061, 'WICHTIG': 1062, 'WIE': 1063, 'WIE-AUSSEHEN': 1064, 'WIE-GEBLIEBEN': 1065, 'WIE-IMMER': 1066, 'WIE-IMMER-PLUSPLUS': 1067, 'WIE-LANG': 1068, 'WIEDER': 1069, 'WIEDER-ZURUECK': 1070, 'WIESE': 1071, 'WIEVIEL': 1072, 'WIND': 1073, 'WIND-PLUSPLUS': 1074, 'WINTER': 1075, 'WIR': 1076, 'WIRBEL': 1077, 'WIRBELSTURM': 1078, 'WIRKEN': 1079, 'WIRKLICH': 1080, 'WIRTSCHAFT': 1081, 'WISSEN': 1082, 'WO': 1083, 'WOCHE': 1084, 'WOCHENENDE': 1085, 'WOHER': 1086, 'WOHNEN': 1087, 'WOLKE': 1088, 'WOLKE-PLUSPLUS': 1089, 'WUENSCHEN': 1090, 'WUERTTEMBERG': 1091, 'WUERZ': 1092, 'WUNDERBAR': 1093, 'WUNDERSCHOEN': 1094, 'X': 1095, 'Y': 1096, 'Z': 1097, 'ZAHL': 1098, 'ZEHN': 1099, 'ZEHNTE': 1100, 'ZEIGEN': 1101, 'ZEIGEN-BILDSCHIRM': 1102, 'ZEIT': 1103, 'ZEITSKALA': 1104, 'ZENTIMETER': 1105, 'ZENTRUM': 1106, 'ZIEHEN': 1107, 'ZONE': 1108, 'ZOOM': 1109, 'ZU': 1110, 'ZU-ENDE': 1111, 'ZU-HAUSE': 1112, 'ZU-TUN': 1113, 'ZUERST': 1114, 'ZUFRIEDEN': 1115, 'ZUG': 1116, 'ZUM-BEISPIEL': 1117, 'ZUM-GLUECK': 1118, 'ZURUECK': 1119, 'ZUSAMMEN': 1120, 'ZUSAMMENHANG': 1121, 'ZUSAMMENSTOSS': 1122, 'ZUSAMMENTREFFEN': 1123, 'ZUSCHAUER': 1124, 'ZWANZIG': 1125, 'ZWEI': 1126, 'ZWEI-TAG': 1127, 'ZWEIDREISSIG': 1128, 'ZWEIFEL': 1129, 'ZWEITE': 1130, 'ZWEIZWANZIG': 1131, 'ZWISCHEN': 1132, 'ZWISCHEN-NULL': 1133, 'ZWOELF': 1134, 'ZWOELFTE': 1135, '__EMOTION__': 1136, '__LEFTHAND__': 1137, '__OFF__': 1138, '__ON__': 1139, '__PU__': 1140, 'cl-KOMMEN': 1141, 'cl-KOMMEN-PLUSPLUS': 1142, 'cl-NULL': 1143, 'lh-HOCH': 1144, 'loc-ALPEN': 1145, 'loc-AUFLOESEN': 1146, 'loc-AUFLOESEN-PLUSPLUS': 1147, 'loc-AUFTEILEN': 1148, 'loc-AUFZIEHEN': 1149, 'loc-AUFZIEHEN-PLUSPLUS': 1150, 'loc-AUSWAEHLEN': 1151, 'loc-BEREICH': 1152, 'loc-BERG': 1153, 'loc-DRUCKFLAECHE': 1154, 'loc-FLAECHENDECKEND': 1155, 'loc-FLUSS': 1156, 'loc-GLITZERN': 1157, 'loc-GRAD': 1158, 'loc-GRENZE': 1159, 'loc-HABEN': 1160, 'loc-HABEN2-PLUSPLUS': 1161, 'loc-INSEL-PLUSPLUS': 1162, 'loc-IRGENDWO': 1163, 'loc-IX': 1164, 'loc-KARTE': 1165, 'loc-KOMMEN': 1166, 'loc-KUESTE': 1167, 'loc-LAND': 1168, 'loc-LAND-PLUSPLUS': 1169, 'loc-LOCH-PLUSPLUS': 1170, 'loc-MEER': 1171, 'loc-MINUS': 1172, 'loc-MINUS-PLUSPLUS': 1173, 'loc-MITTE': 1174, 'loc-NEBEL': 1175, 'loc-NEBEN': 1176, 'loc-NEUN': 1177, 'loc-NORD': 1178, 'loc-NORDOST': 1179, 'loc-NORDWEST': 1180, 'loc-NULL': 1181, 'loc-OBEN': 1182, 'loc-ORT': 1183, 'loc-ORT-PLUSPLUS': 1184, 'loc-OST': 1185, 'loc-OSTBAYERN': 1186, 'loc-POSITION': 1187, 'loc-POSITION-PLUSPLUS': 1188, 'loc-RAUM': 1189, 'loc-REGEN': 1190, 'loc-REGEN-PLUSPLUS': 1191, 'loc-REGION': 1192, 'loc-REGION-PLUSPLUS': 1193, 'loc-SCHWACH-PLUSPLUS': 1194, 'loc-SECHS': 1195, 'loc-SEE': 1196, 'loc-SIEBEN': 1197, 'loc-SIEBENZEHN': 1198, 'loc-SONNE-SCHEINEN': 1199, 'loc-STELLENWEISE-PLUSPLUS': 1200, 'loc-STREIFEN': 1201, 'loc-STROEMEN': 1202, 'loc-STURM': 1203, 'loc-SUED': 1204, 'loc-SUEDOST': 1205, 'loc-SUEDRAUM': 1206, 'loc-SUEDWEST': 1207, 'loc-TAL': 1208, 'loc-TIEF': 1209, 'loc-TROCKEN': 1210, 'loc-UEBERALL': 1211, 'loc-UNTEN': 1212, 'loc-VERSCHIEBEN': 1213, 'loc-VIER': 1214, 'loc-WEHEN': 1215, 'loc-WEST': 1216, 'loc-WIND': 1217, 'loc-WOLKE': 1218, 'loc-ZEIGEN': 1219, 'loc-ZONE': 1220, 'loc-ZWEI': 1221, 'loc-ZWISCHEN': 1222, 'loc-ZWOELF': 1223, 'poss-BEI-UNS': 1224, 'poss-EUCH': 1225, 'poss-MEIN': 1226, 'poss-SEIN': 1227, 'qu-DU': 1228, 'qu-MOEGEN': 1229, 'qu-STEIGEN': 1230}\n",
      "dict_keys(['video', 'text'])\n",
      "4\n",
      "torch.Size([82, 3, 260, 210])\n",
      "torch.Size([14])\n"
     ]
    }
   ],
   "source": [
    "from phoenix_datasets import PhoenixVideoTextDataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "\n",
    "curr_dir = os.getcwd()\n",
    "datapath = \"data/phoenix-2014.v3/phoenix2014-release/phoenix-2014-multisigner\"\n",
    "path = os.path.join(curr_dir, datapath)\n",
    "\n",
    "dtrain = PhoenixVideoTextDataset(\n",
    "    # your path to this folder, download it from official website first.\n",
    "    root=path,\n",
    "    split=\"train\",\n",
    "    p_drop=0.5,\n",
    "    random_drop=True,\n",
    ")\n",
    "\n",
    "vocab = dtrain.vocab\n",
    "\n",
    "print(\"Vocab\", vocab)\n",
    "\n",
    "dl = DataLoader(dtrain, collate_fn=dtrain.collate_fn, shuffle=True, batch_size=4)\n",
    "\n",
    "for batch in dl:\n",
    "    print(batch.keys())\n",
    "    video = batch[\"video\"]\n",
    "    label = batch[\"text\"]\n",
    "\n",
    "    assert len(video) == len(label)\n",
    "\n",
    "    print(len(video))\n",
    "    print(video[0].shape)\n",
    "    print(label[0].shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1418"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([95, 3, 260, 210])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([66, 3, 260, 210])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51, 3, 260, 210])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([78, 3, 260, 210])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video[3].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
